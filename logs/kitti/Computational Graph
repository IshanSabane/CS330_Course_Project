digraph {
	graph [size="765.9,765.9"]
	node [align=left fontname=monospace fontsize=10 height=0.2 ranksep=0.1 shape=box style=filled]
	140377787203072 [label="
 (6, 100, 92)" fillcolor=darkolivegreen1]
	140377787018016 [label="ViewBackward0
---------------------
self_sizes: (600, 92)"]
	140377787016288 -> 140377787018016
	140377787016288 -> 140377786858208 [dir=none]
	140377786858208 [label="mat1
 (600, 256)" fillcolor=orange]
	140377787016288 -> 140377786858048 [dir=none]
	140377786858048 [label="mat2
 (256, 92)" fillcolor=orange]
	140377787016288 [label="AddmmBackward0
----------------------------
alpha       :              1
beta        :              1
mat1        : [saved tensor]
mat1_sizes  :     (600, 256)
mat1_strides:       (256, 1)
mat2        : [saved tensor]
mat2_sizes  :      (256, 92)
mat2_strides:       (1, 256)"]
	140377787015904 -> 140377787016288
	140377785299728 [label="class_labels_classifier.bias
 (92)" fillcolor=lightblue]
	140377785299728 -> 140377787015904
	140377787015904 [label=AccumulateGrad]
	140377787016720 -> 140377787016288
	140377787016720 [label="ViewBackward0
-------------------------
self_sizes: (6, 100, 256)"]
	140377787015280 -> 140377787016720
	140377787015280 -> 140377785299568 [dir=none]
	140377785299568 [label="bias
 (256)" fillcolor=orange]
	140377787015280 -> 140377786898288 [dir=none]
	140377786898288 [label="input
 (6, 100, 256)" fillcolor=orange]
	140377787015280 -> 140377786858448 [dir=none]
	140377786858448 [label="result1
 (6, 100, 1)" fillcolor=orange]
	140377787015280 -> 140377786858128 [dir=none]
	140377786858128 [label="result2
 (6, 100, 1)" fillcolor=orange]
	140377787015280 -> 140377785299488 [dir=none]
	140377785299488 [label="weight
 (256)" fillcolor=orange]
	140377787015280 [label="NativeLayerNormBackward0
--------------------------------
bias            : [saved tensor]
input           : [saved tensor]
normalized_shape:         (256,)
result1         : [saved tensor]
result2         : [saved tensor]
weight          : [saved tensor]"]
	140377787017344 -> 140377787015280
	140377787017344 -> 140377785299408 [dir=none]
	140377785299408 [label="bias
 (256)" fillcolor=orange]
	140377787017344 -> 140377786896768 [dir=none]
	140377786896768 [label="input
 (6, 100, 256)" fillcolor=orange]
	140377787017344 -> 140377786858608 [dir=none]
	140377786858608 [label="result1
 (6, 100, 1)" fillcolor=orange]
	140377787017344 -> 140377786858368 [dir=none]
	140377786858368 [label="result2
 (6, 100, 1)" fillcolor=orange]
	140377787017344 -> 140377785299328 [dir=none]
	140377785299328 [label="weight
 (256)" fillcolor=orange]
	140377787017344 [label="NativeLayerNormBackward0
--------------------------------
bias            : [saved tensor]
input           : [saved tensor]
normalized_shape:         (256,)
result1         : [saved tensor]
result2         : [saved tensor]
weight          : [saved tensor]"]
	140377787018928 -> 140377787017344
	140377787018928 [label="AddBackward0
------------
alpha: 1"]
	140377787018448 -> 140377787018928
	140377787018448 -> 140377785184144 [dir=none]
	140377785184144 [label="bias
 (256)" fillcolor=orange]
	140377787018448 -> 140377786898208 [dir=none]
	140377786898208 [label="input
 (6, 100, 256)" fillcolor=orange]
	140377787018448 -> 140377786858768 [dir=none]
	140377786858768 [label="result1
 (6, 100, 1)" fillcolor=orange]
	140377787018448 -> 140377786858528 [dir=none]
	140377786858528 [label="result2
 (6, 100, 1)" fillcolor=orange]
	140377787018448 -> 140377785184064 [dir=none]
	140377785184064 [label="weight
 (256)" fillcolor=orange]
	140377787018448 [label="NativeLayerNormBackward0
--------------------------------
bias            : [saved tensor]
input           : [saved tensor]
normalized_shape:         (256,)
result1         : [saved tensor]
result2         : [saved tensor]
weight          : [saved tensor]"]
	140377787018736 -> 140377787018448
	140377787018736 [label="AddBackward0
------------
alpha: 1"]
	140377787044192 -> 140377787018736
	140377787044192 -> 140377785183344 [dir=none]
	140377785183344 [label="bias
 (256)" fillcolor=orange]
	140377787044192 -> 140377786897248 [dir=none]
	140377786897248 [label="input
 (6, 100, 256)" fillcolor=orange]
	140377787044192 -> 140377786858928 [dir=none]
	140377786858928 [label="result1
 (6, 100, 1)" fillcolor=orange]
	140377787044192 -> 140377786858688 [dir=none]
	140377786858688 [label="result2
 (6, 100, 1)" fillcolor=orange]
	140377787044192 -> 140377785183264 [dir=none]
	140377785183264 [label="weight
 (256)" fillcolor=orange]
	140377787044192 [label="NativeLayerNormBackward0
--------------------------------
bias            : [saved tensor]
input           : [saved tensor]
normalized_shape:         (256,)
result1         : [saved tensor]
result2         : [saved tensor]
weight          : [saved tensor]"]
	140377787047456 -> 140377787044192
	140377787047456 [label="AddBackward0
------------
alpha: 1"]
	140377787045968 -> 140377787047456
	140377787045968 -> 140377785182544 [dir=none]
	140377785182544 [label="bias
 (256)" fillcolor=orange]
	140377787045968 -> 140377786915152 [dir=none]
	140377786915152 [label="input
 (6, 100, 256)" fillcolor=orange]
	140377787045968 -> 140377786859008 [dir=none]
	140377786859008 [label="result1
 (6, 100, 1)" fillcolor=orange]
	140377787045968 -> 140377786858848 [dir=none]
	140377786858848 [label="result2
 (6, 100, 1)" fillcolor=orange]
	140377787045968 -> 140377785182464 [dir=none]
	140377785182464 [label="weight
 (256)" fillcolor=orange]
	140377787045968 [label="NativeLayerNormBackward0
--------------------------------
bias            : [saved tensor]
input           : [saved tensor]
normalized_shape:         (256,)
result1         : [saved tensor]
result2         : [saved tensor]
weight          : [saved tensor]"]
	140377787044240 -> 140377787045968
	140377787044240 [label="AddBackward0
------------
alpha: 1"]
	140377787044624 -> 140377787044240
	140377787044624 -> 140377785182064 [dir=none]
	140377785182064 [label="bias
 (256)" fillcolor=orange]
	140377787044624 -> 140377786916592 [dir=none]
	140377786916592 [label="input
 (6, 100, 256)" fillcolor=orange]
	140377787044624 -> 140377786859168 [dir=none]
	140377786859168 [label="result1
 (6, 100, 1)" fillcolor=orange]
	140377787044624 -> 140377786858288 [dir=none]
	140377786858288 [label="result2
 (6, 100, 1)" fillcolor=orange]
	140377787044624 -> 140377785181984 [dir=none]
	140377785181984 [label="weight
 (256)" fillcolor=orange]
	140377787044624 [label="NativeLayerNormBackward0
--------------------------------
bias            : [saved tensor]
input           : [saved tensor]
normalized_shape:         (256,)
result1         : [saved tensor]
result2         : [saved tensor]
weight          : [saved tensor]"]
	140377787044816 -> 140377787044624
	140377787044816 [label="AddBackward0
------------
alpha: 1"]
	140377787044912 -> 140377787044816
	140377787044912 -> 140377785181264 [dir=none]
	140377785181264 [label="bias
 (256)" fillcolor=orange]
	140377787044912 -> 140377786915632 [dir=none]
	140377786915632 [label="input
 (6, 100, 256)" fillcolor=orange]
	140377787044912 -> 140377786859328 [dir=none]
	140377786859328 [label="result1
 (6, 100, 1)" fillcolor=orange]
	140377787044912 -> 140377786859088 [dir=none]
	140377786859088 [label="result2
 (6, 100, 1)" fillcolor=orange]
	140377787044912 -> 140377785181184 [dir=none]
	140377785181184 [label="weight
 (256)" fillcolor=orange]
	140377787044912 [label="NativeLayerNormBackward0
--------------------------------
bias            : [saved tensor]
input           : [saved tensor]
normalized_shape:         (256,)
result1         : [saved tensor]
result2         : [saved tensor]
weight          : [saved tensor]"]
	140377787045632 -> 140377787044912
	140377787045632 [label="AddBackward0
------------
alpha: 1"]
	140377787045680 -> 140377787045632
	140377787045680 -> 140377785180464 [dir=none]
	140377785180464 [label="bias
 (256)" fillcolor=orange]
	140377787045680 -> 140377786913152 [dir=none]
	140377786913152 [label="input
 (6, 100, 256)" fillcolor=orange]
	140377787045680 -> 140377786859248 [dir=none]
	140377786859248 [label="result1
 (6, 100, 1)" fillcolor=orange]
	140377787045680 -> 140377786859408 [dir=none]
	140377786859408 [label="result2
 (6, 100, 1)" fillcolor=orange]
	140377787045680 -> 140377785180384 [dir=none]
	140377785180384 [label="weight
 (256)" fillcolor=orange]
	140377787045680 [label="NativeLayerNormBackward0
--------------------------------
bias            : [saved tensor]
input           : [saved tensor]
normalized_shape:         (256,)
result1         : [saved tensor]
result2         : [saved tensor]
weight          : [saved tensor]"]
	140377787046352 -> 140377787045680
	140377787046352 [label="AddBackward0
------------
alpha: 1"]
	140377787047696 -> 140377787046352
	140377787047696 -> 140377785589488 [dir=none]
	140377785589488 [label="bias
 (256)" fillcolor=orange]
	140377787047696 -> 140377786914592 [dir=none]
	140377786914592 [label="input
 (6, 100, 256)" fillcolor=orange]
	140377787047696 -> 140377786945760 [dir=none]
	140377786945760 [label="result1
 (6, 100, 1)" fillcolor=orange]
	140377787047696 -> 140377786945600 [dir=none]
	140377786945600 [label="result2
 (6, 100, 1)" fillcolor=orange]
	140377787047696 -> 140377785589408 [dir=none]
	140377785589408 [label="weight
 (256)" fillcolor=orange]
	140377787047696 [label="NativeLayerNormBackward0
--------------------------------
bias            : [saved tensor]
input           : [saved tensor]
normalized_shape:         (256,)
result1         : [saved tensor]
result2         : [saved tensor]
weight          : [saved tensor]"]
	140377787047888 -> 140377787047696
	140377787047888 [label="AddBackward0
------------
alpha: 1"]
	140377787047840 -> 140377787047888
	140377787047840 -> 140377785588688 [dir=none]
	140377785588688 [label="bias
 (256)" fillcolor=orange]
	140377787047840 -> 140377786913632 [dir=none]
	140377786913632 [label="input
 (6, 100, 256)" fillcolor=orange]
	140377787047840 -> 140377786945920 [dir=none]
	140377786945920 [label="result1
 (6, 100, 1)" fillcolor=orange]
	140377787047840 -> 140377786945680 [dir=none]
	140377786945680 [label="result2
 (6, 100, 1)" fillcolor=orange]
	140377787047840 -> 140377785588608 [dir=none]
	140377785588608 [label="weight
 (256)" fillcolor=orange]
	140377787047840 [label="NativeLayerNormBackward0
--------------------------------
bias            : [saved tensor]
input           : [saved tensor]
normalized_shape:         (256,)
result1         : [saved tensor]
result2         : [saved tensor]
weight          : [saved tensor]"]
	140377787069056 -> 140377787047840
	140377787069056 [label="AddBackward0
------------
alpha: 1"]
	140377787070976 -> 140377787069056
	140377787070976 -> 140377785587888 [dir=none]
	140377785587888 [label="bias
 (256)" fillcolor=orange]
	140377787070976 -> 140377786820944 [dir=none]
	140377786820944 [label="input
 (6, 100, 256)" fillcolor=orange]
	140377787070976 -> 140377786946080 [dir=none]
	140377786946080 [label="result1
 (6, 100, 1)" fillcolor=orange]
	140377787070976 -> 140377786945840 [dir=none]
	140377786945840 [label="result2
 (6, 100, 1)" fillcolor=orange]
	140377787070976 -> 140377785587808 [dir=none]
	140377785587808 [label="weight
 (256)" fillcolor=orange]
	140377787070976 [label="NativeLayerNormBackward0
--------------------------------
bias            : [saved tensor]
input           : [saved tensor]
normalized_shape:         (256,)
result1         : [saved tensor]
result2         : [saved tensor]
weight          : [saved tensor]"]
	140377787068576 -> 140377787070976
	140377787068576 [label="AddBackward0
------------
alpha: 1"]
	140377787069440 -> 140377787068576
	140377787069440 -> 140377785587408 [dir=none]
	140377785587408 [label="bias
 (256)" fillcolor=orange]
	140377787069440 -> 140377786822384 [dir=none]
	140377786822384 [label="input
 (6, 100, 256)" fillcolor=orange]
	140377787069440 -> 140377786946240 [dir=none]
	140377786946240 [label="result1
 (6, 100, 1)" fillcolor=orange]
	140377787069440 -> 140377786946000 [dir=none]
	140377786946000 [label="result2
 (6, 100, 1)" fillcolor=orange]
	140377787069440 -> 140377785587328 [dir=none]
	140377785587328 [label="weight
 (256)" fillcolor=orange]
	140377787069440 [label="NativeLayerNormBackward0
--------------------------------
bias            : [saved tensor]
input           : [saved tensor]
normalized_shape:         (256,)
result1         : [saved tensor]
result2         : [saved tensor]
weight          : [saved tensor]"]
	140377787070304 -> 140377787069440
	140377787070304 [label="AddBackward0
------------
alpha: 1"]
	140377787070544 -> 140377787070304
	140377787070544 -> 140377785586608 [dir=none]
	140377785586608 [label="bias
 (256)" fillcolor=orange]
	140377787070544 -> 140377786821424 [dir=none]
	140377786821424 [label="input
 (6, 100, 256)" fillcolor=orange]
	140377787070544 -> 140377786946400 [dir=none]
	140377786946400 [label="result1
 (6, 100, 1)" fillcolor=orange]
	140377787070544 -> 140377786946160 [dir=none]
	140377786946160 [label="result2
 (6, 100, 1)" fillcolor=orange]
	140377787070544 -> 140377785586528 [dir=none]
	140377785586528 [label="weight
 (256)" fillcolor=orange]
	140377787070544 [label="NativeLayerNormBackward0
--------------------------------
bias            : [saved tensor]
input           : [saved tensor]
normalized_shape:         (256,)
result1         : [saved tensor]
result2         : [saved tensor]
weight          : [saved tensor]"]
	140377787072224 -> 140377787070544
	140377787072224 [label="AddBackward0
------------
alpha: 1"]
	140377787071168 -> 140377787072224
	140377787071168 -> 140377785585808 [dir=none]
	140377785585808 [label="bias
 (256)" fillcolor=orange]
	140377787071168 -> 140377786818944 [dir=none]
	140377786818944 [label="input
 (6, 100, 256)" fillcolor=orange]
	140377787071168 -> 140377786946560 [dir=none]
	140377786946560 [label="result1
 (6, 100, 1)" fillcolor=orange]
	140377787071168 -> 140377786946320 [dir=none]
	140377786946320 [label="result2
 (6, 100, 1)" fillcolor=orange]
	140377787071168 -> 140377785585728 [dir=none]
	140377785585728 [label="weight
 (256)" fillcolor=orange]
	140377787071168 [label="NativeLayerNormBackward0
--------------------------------
bias            : [saved tensor]
input           : [saved tensor]
normalized_shape:         (256,)
result1         : [saved tensor]
result2         : [saved tensor]
weight          : [saved tensor]"]
	140377787070832 -> 140377787071168
	140377787070832 [label="AddBackward0
------------
alpha: 1"]
	140377787071456 -> 140377787070832
	140377787071456 -> 140377787314400 [dir=none]
	140377787314400 [label="bias
 (256)" fillcolor=orange]
	140377787071456 -> 140377786820384 [dir=none]
	140377786820384 [label="input
 (6, 100, 256)" fillcolor=orange]
	140377787071456 -> 140377786946720 [dir=none]
	140377786946720 [label="result1
 (6, 100, 1)" fillcolor=orange]
	140377787071456 -> 140377786946480 [dir=none]
	140377786946480 [label="result2
 (6, 100, 1)" fillcolor=orange]
	140377787071456 -> 140377787314640 [dir=none]
	140377787314640 [label="weight
 (256)" fillcolor=orange]
	140377787071456 [label="NativeLayerNormBackward0
--------------------------------
bias            : [saved tensor]
input           : [saved tensor]
normalized_shape:         (256,)
result1         : [saved tensor]
result2         : [saved tensor]
weight          : [saved tensor]"]
	140377787071696 -> 140377787071456
	140377787071696 [label="AddBackward0
------------
alpha: 1"]
	140377787072080 -> 140377787071696
	140377787072080 -> 140377785685152 [dir=none]
	140377785685152 [label="bias
 (256)" fillcolor=orange]
	140377787072080 -> 140377786819424 [dir=none]
	140377786819424 [label="input
 (6, 100, 256)" fillcolor=orange]
	140377787072080 -> 140377786946880 [dir=none]
	140377786946880 [label="result1
 (6, 100, 1)" fillcolor=orange]
	140377787072080 -> 140377786946640 [dir=none]
	140377786946640 [label="result2
 (6, 100, 1)" fillcolor=orange]
	140377787072080 -> 140377785685232 [dir=none]
	140377785685232 [label="weight
 (256)" fillcolor=orange]
	140377787072080 [label="NativeLayerNormBackward0
--------------------------------
bias            : [saved tensor]
input           : [saved tensor]
normalized_shape:         (256,)
result1         : [saved tensor]
result2         : [saved tensor]
weight          : [saved tensor]"]
	140377787097200 -> 140377787072080
	140377787097200 [label="AddBackward0
------------
alpha: 1"]
	140377787097392 -> 140377787097200
	140377787097392 -> 140377785685952 [dir=none]
	140377785685952 [label="bias
 (256)" fillcolor=orange]
	140377787097392 -> 140377787005264 [dir=none]
	140377787005264 [label="input
 (6, 100, 256)" fillcolor=orange]
	140377787097392 -> 140377786947040 [dir=none]
	140377786947040 [label="result1
 (6, 100, 1)" fillcolor=orange]
	140377787097392 -> 140377786946800 [dir=none]
	140377786946800 [label="result2
 (6, 100, 1)" fillcolor=orange]
	140377787097392 -> 140377785686032 [dir=none]
	140377785686032 [label="weight
 (256)" fillcolor=orange]
	140377787097392 [label="NativeLayerNormBackward0
--------------------------------
bias            : [saved tensor]
input           : [saved tensor]
normalized_shape:         (256,)
result1         : [saved tensor]
result2         : [saved tensor]
weight          : [saved tensor]"]
	140377787100080 -> 140377787097392
	140377787100080 [label="AddBackward0
------------
alpha: 1"]
	140377787097728 -> 140377787100080
	140377787097728 -> 140377785686432 [dir=none]
	140377785686432 [label="bias
 (256)" fillcolor=orange]
	140377787097728 -> 140377787006704 [dir=none]
	140377787006704 [label="input
 (6, 100, 256)" fillcolor=orange]
	140377787097728 -> 140377786947200 [dir=none]
	140377786947200 [label="result1
 (6, 100, 1)" fillcolor=orange]
	140377787097728 -> 140377786946960 [dir=none]
	140377786946960 [label="result2
 (6, 100, 1)" fillcolor=orange]
	140377787097728 -> 140377785686512 [dir=none]
	140377785686512 [label="weight
 (256)" fillcolor=orange]
	140377787097728 [label="NativeLayerNormBackward0
--------------------------------
bias            : [saved tensor]
input           : [saved tensor]
normalized_shape:         (256,)
result1         : [saved tensor]
result2         : [saved tensor]
weight          : [saved tensor]"]
	140377787099168 -> 140377787097728
	140377787099168 [label="AddBackward0
------------
alpha: 1"]
	140377787098352 -> 140377787099168
	140377787098352 -> 140377785687232 [dir=none]
	140377785687232 [label="bias
 (256)" fillcolor=orange]
	140377787098352 -> 140377787005744 [dir=none]
	140377787005744 [label="input
 (6, 100, 256)" fillcolor=orange]
	140377787098352 -> 140377786947360 [dir=none]
	140377786947360 [label="result1
 (6, 100, 1)" fillcolor=orange]
	140377787098352 -> 140377786947120 [dir=none]
	140377786947120 [label="result2
 (6, 100, 1)" fillcolor=orange]
	140377787098352 -> 140377785687312 [dir=none]
	140377785687312 [label="weight
 (256)" fillcolor=orange]
	140377787098352 [label="NativeLayerNormBackward0
--------------------------------
bias            : [saved tensor]
input           : [saved tensor]
normalized_shape:         (256,)
result1         : [saved tensor]
result2         : [saved tensor]
weight          : [saved tensor]"]
	140377787098688 -> 140377787098352
	140377787098688 [label="AddBackward0
------------
alpha: 1"]
	140377787099360 -> 140377787098688
	140377787099360 -> 140377786947520 [dir=none]
	140377786947520 [label="other
 (6, 100, 256)" fillcolor=orange]
	140377787099360 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140377787100560 -> 140377787099360
	140377787100560 [label="ViewBackward0
----------------------
self_sizes: (600, 256)"]
	140377787099408 -> 140377787100560
	140377787099408 -> 140377786947680 [dir=none]
	140377786947680 [label="mat1
 (600, 256)" fillcolor=orange]
	140377787099408 -> 140377786947440 [dir=none]
	140377786947440 [label="mat2
 (256, 256)" fillcolor=orange]
	140377787099408 [label="AddmmBackward0
----------------------------
alpha       :              1
beta        :              1
mat1        : [saved tensor]
mat1_sizes  :     (600, 256)
mat1_strides:       (256, 1)
mat2        : [saved tensor]
mat2_sizes  :     (256, 256)
mat2_strides:       (1, 256)"]
	140377787099552 -> 140377787099408
	140377785687392 [label="model.decoder.layers.0.self_attn.out_proj.bias
 (256)" fillcolor=lightblue]
	140377785687392 -> 140377787099552
	140377787099552 [label=AccumulateGrad]
	140377787099648 -> 140377787099408
	140377787099648 [label="ViewBackward0
-------------------------
self_sizes: (6, 100, 256)"]
	140377787099840 -> 140377787099648
	140377787099840 [label="UnsafeViewBackward0
---------------------------
self_sizes: (6, 100, 8, 32)"]
	140377787100464 -> 140377787099840
	140377787100464 [label=CloneBackward0]
	140377787100752 -> 140377787100464
	140377787100752 [label="TransposeBackward0
------------------
dim0: 1
dim1: 2"]
	140377787101088 -> 140377787100752
	140377787101088 [label="ViewBackward0
-------------------------
self_sizes: (48, 100, 32)"]
	140377787126784 -> 140377787101088
	140377787126784 -> 140377787005424 [dir=none]
	140377787005424 [label="mat2
 (48, 100, 32)" fillcolor=orange]
	140377787126784 -> 140377787005584 [dir=none]
	140377787005584 [label="self
 (48, 100, 100)" fillcolor=orange]
	140377787126784 [label="BmmBackward0
--------------------
mat2: [saved tensor]
self: [saved tensor]"]
	140377787127264 -> 140377787126784
	140377787127264 -> 140377786947280 [dir=none]
	140377786947280 [label="result
 (48, 100, 100)" fillcolor=orange]
	140377787127264 [label="SoftmaxBackward0
----------------------------
dim   : 18446744073709551615
result:       [saved tensor]"]
	140377787127456 -> 140377787127264
	140377787127456 -> 140377787005024 [dir=none]
	140377787005024 [label="mat2
 (48, 32, 100)" fillcolor=orange]
	140377787127456 -> 140377787005344 [dir=none]
	140377787005344 [label="self
 (48, 100, 32)" fillcolor=orange]
	140377787127456 [label="BmmBackward0
--------------------
mat2: [saved tensor]
self: [saved tensor]"]
	140377787126832 -> 140377787127456
	140377787126832 [label="ViewBackward0
---------------------------
self_sizes: (6, 8, 100, 32)"]
	140377787126208 -> 140377787126832
	140377787126208 [label=CloneBackward0]
	140377787127072 -> 140377787126208
	140377787127072 [label="TransposeBackward0
------------------
dim0: 1
dim1: 2"]
	140377787126640 -> 140377787127072
	140377787126640 [label="ViewBackward0
-------------------------
self_sizes: (6, 100, 256)"]
	140377787157328 -> 140377787126640
	140377787157328 -> 140377786947920 [dir=none]
	140377786947920 [label="other
 ()" fillcolor=orange]
	140377787157328 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140377787157712 -> 140377787157328
	140377787157712 [label="ViewBackward0
----------------------
self_sizes: (600, 256)"]
	140377787157088 -> 140377787157712
	140377787157088 -> 140377786948160 [dir=none]
	140377786948160 [label="mat1
 (600, 256)" fillcolor=orange]
	140377787157088 -> 140377786947840 [dir=none]
	140377786947840 [label="mat2
 (256, 256)" fillcolor=orange]
	140377787157088 [label="AddmmBackward0
----------------------------
alpha       :              1
beta        :              1
mat1        : [saved tensor]
mat1_sizes  :     (600, 256)
mat1_strides:       (256, 1)
mat2        : [saved tensor]
mat2_sizes  :     (256, 256)
mat2_strides:       (1, 256)"]
	140377787158336 -> 140377787157088
	140377785687552 [label="model.decoder.layers.0.self_attn.q_proj.bias
 (256)" fillcolor=lightblue]
	140377785687552 -> 140377787158336
	140377787158336 [label=AccumulateGrad]
	140377787157280 -> 140377787157088
	140377787157280 [label="ViewBackward0
-------------------------
self_sizes: (6, 100, 256)"]
	140377787156560 -> 140377787157280
	140377787156560 [label="AddBackward0
------------
alpha: 1"]
	140377796616496 -> 140377787156560
	140377796616496 [label="RepeatBackward0
-------------------------
repeats   :     (6, 1, 1)
self_sizes: (1, 100, 256)"]
	140377786737856 -> 140377796616496
	140377786737856 [label="UnsqueezeBackward0
------------------
dim: 0"]
	140377787183984 -> 140377786737856
	140377786100928 [label="model.query_position_embeddings.weight
 (100, 256)" fillcolor=lightblue]
	140377786100928 -> 140377787183984
	140377787183984 [label=AccumulateGrad]
	140377787158096 -> 140377787157088
	140377787158096 [label=TBackward0]
	140377796618032 -> 140377787158096
	140377785687632 [label="model.decoder.layers.0.self_attn.q_proj.weight
 (256, 256)" fillcolor=lightblue]
	140377785687632 -> 140377796618032
	140377796618032 [label=AccumulateGrad]
	140377787126064 -> 140377787127456
	140377787126064 [label="TransposeBackward0
------------------
dim0: 1
dim1: 2"]
	140377796616448 -> 140377787126064
	140377796616448 [label="ViewBackward0
---------------------------
self_sizes: (6, 8, 100, 32)"]
	140377787126256 -> 140377796616448
	140377787126256 [label=CloneBackward0]
	140377787157136 -> 140377787126256
	140377787157136 [label="TransposeBackward0
------------------
dim0: 1
dim1: 2"]
	140377787155168 -> 140377787157136
	140377787155168 [label="ViewBackward0
-------------------------
self_sizes: (6, 100, 256)"]
	140377787158384 -> 140377787155168
	140377787158384 [label="ViewBackward0
----------------------
self_sizes: (600, 256)"]
	140377787183168 -> 140377787158384
	140377787183168 -> 140377786948080 [dir=none]
	140377786948080 [label="mat1
 (600, 256)" fillcolor=orange]
	140377787183168 -> 140377786948000 [dir=none]
	140377786948000 [label="mat2
 (256, 256)" fillcolor=orange]
	140377787183168 [label="AddmmBackward0
----------------------------
alpha       :              1
beta        :              1
mat1        : [saved tensor]
mat1_sizes  :     (600, 256)
mat1_strides:       (256, 1)
mat2        : [saved tensor]
mat2_sizes  :     (256, 256)
mat2_strides:       (1, 256)"]
	140377787185088 -> 140377787183168
	140377785687872 [label="model.decoder.layers.0.self_attn.k_proj.bias
 (256)" fillcolor=lightblue]
	140377785687872 -> 140377787185088
	140377787185088 [label=AccumulateGrad]
	140377787183696 -> 140377787183168
	140377787183696 [label="ViewBackward0
-------------------------
self_sizes: (6, 100, 256)"]
	140377787156560 -> 140377787183696
	140377787183648 -> 140377787183168
	140377787183648 [label=TBackward0]
	140377787184416 -> 140377787183648
	140377785687952 [label="model.decoder.layers.0.self_attn.k_proj.weight
 (256, 256)" fillcolor=lightblue]
	140377785687952 -> 140377787184416
	140377787184416 [label=AccumulateGrad]
	140377787125968 -> 140377787126784
	140377787125968 [label="ViewBackward0
---------------------------
self_sizes: (6, 8, 100, 32)"]
	140377787127120 -> 140377787125968
	140377787127120 [label=CloneBackward0]
	140377787126592 -> 140377787127120
	140377787126592 [label="TransposeBackward0
------------------
dim0: 1
dim1: 2"]
	140377787157472 -> 140377787126592
	140377787157472 [label="ViewBackward0
-------------------------
self_sizes: (6, 100, 256)"]
	140377787183840 -> 140377787157472
	140377787183840 [label="ViewBackward0
----------------------
self_sizes: (600, 256)"]
	140377787183936 -> 140377787183840
	140377787183936 -> 140377786947600 [dir=none]
	140377786947600 [label="mat1
 (600, 256)" fillcolor=orange]
	140377787183936 [label="AddmmBackward0
----------------------------
alpha       :              1
beta        :              1
mat1        : [saved tensor]
mat1_sizes  :     (600, 256)
mat1_strides:             ()
mat2        :           None
mat2_sizes  :     (256, 256)
mat2_strides:       (1, 256)"]
	140377787514592 -> 140377787183936
	140377785687712 [label="model.decoder.layers.0.self_attn.v_proj.bias
 (256)" fillcolor=lightblue]
	140377785687712 -> 140377787514592
	140377787514592 [label=AccumulateGrad]
	140377787514400 -> 140377787183936
	140377787514400 [label=TBackward0]
	140377787514640 -> 140377787514400
	140377785687792 [label="model.decoder.layers.0.self_attn.v_proj.weight
 (256, 256)" fillcolor=lightblue]
	140377785687792 -> 140377787514640
	140377787514640 [label=AccumulateGrad]
	140377787099888 -> 140377787099408
	140377787099888 [label=TBackward0]
	140377787100704 -> 140377787099888
	140377785687472 [label="model.decoder.layers.0.self_attn.out_proj.weight
 (256, 256)" fillcolor=lightblue]
	140377785687472 -> 140377787100704
	140377787100704 [label=AccumulateGrad]
	140377787098784 -> 140377787098352
	140377785687312 [label="model.decoder.layers.0.self_attn_layer_norm.weight
 (256)" fillcolor=lightblue]
	140377785687312 -> 140377787098784
	140377787098784 [label=AccumulateGrad]
	140377787098544 -> 140377787098352
	140377785687232 [label="model.decoder.layers.0.self_attn_layer_norm.bias
 (256)" fillcolor=lightblue]
	140377785687232 -> 140377787098544
	140377787098544 [label=AccumulateGrad]
	140377787098304 -> 140377787099168
	140377787098304 -> 140377786948480 [dir=none]
	140377786948480 [label="other
 (6, 100, 256)" fillcolor=orange]
	140377787098304 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140377787099024 -> 140377787098304
	140377787099024 [label="ViewBackward0
----------------------
self_sizes: (600, 256)"]
	140377787099600 -> 140377787099024
	140377787099600 -> 140377786948240 [dir=none]
	140377786948240 [label="mat1
 (600, 256)" fillcolor=orange]
	140377787099600 -> 140377786948400 [dir=none]
	140377786948400 [label="mat2
 (256, 256)" fillcolor=orange]
	140377787099600 [label="AddmmBackward0
----------------------------
alpha       :              1
beta        :              1
mat1        : [saved tensor]
mat1_sizes  :     (600, 256)
mat1_strides:       (256, 1)
mat2        : [saved tensor]
mat2_sizes  :     (256, 256)
mat2_strides:       (1, 256)"]
	140377787100224 -> 140377787099600
	140377785686592 [label="model.decoder.layers.0.encoder_attn.out_proj.bias
 (256)" fillcolor=lightblue]
	140377785686592 -> 140377787100224
	140377787100224 [label=AccumulateGrad]
	140377787100272 -> 140377787099600
	140377787100272 [label="ViewBackward0
-------------------------
self_sizes: (6, 100, 256)"]
	140377787126448 -> 140377787100272
	140377787126448 [label="UnsafeViewBackward0
---------------------------
self_sizes: (6, 100, 8, 32)"]
	140377787157952 -> 140377787126448
	140377787157952 [label=CloneBackward0]
	140377787183216 -> 140377787157952
	140377787183216 [label="TransposeBackward0
------------------
dim0: 1
dim1: 2"]
	140377787514688 -> 140377787183216
	140377787514688 [label="ViewBackward0
-------------------------
self_sizes: (48, 100, 32)"]
	140377787514016 -> 140377787514688
	140377787514016 -> 140377787006384 [dir=none]
	140377787006384 [label="mat2
 (48, 782, 32)" fillcolor=orange]
	140377787514016 -> 140377787006544 [dir=none]
	140377787006544 [label="self
 (48, 100, 782)" fillcolor=orange]
	140377787514016 [label="BmmBackward0
--------------------
mat2: [saved tensor]
self: [saved tensor]"]
	140377787514208 -> 140377787514016
	140377787514208 -> 140377786948560 [dir=none]
	140377786948560 [label="result
 (48, 100, 782)" fillcolor=orange]
	140377787514208 [label="SoftmaxBackward0
----------------------------
dim   : 18446744073709551615
result:       [saved tensor]"]
	140377787513728 -> 140377787514208
	140377787513728 [label="ViewBackward0
----------------------------
self_sizes: (6, 8, 100, 782)"]
	140377787513536 -> 140377787513728
	140377787513536 [label="AddBackward0
------------
alpha: 1"]
	140377787513344 -> 140377787513536
	140377787513344 [label="ViewBackward0
--------------------------
self_sizes: (48, 100, 782)"]
	140377787513152 -> 140377787513344
	140377787513152 -> 140377787005984 [dir=none]
	140377787005984 [label="mat2
 (48, 32, 782)" fillcolor=orange]
	140377787513152 -> 140377787006304 [dir=none]
	140377787006304 [label="self
 (48, 100, 32)" fillcolor=orange]
	140377787513152 [label="BmmBackward0
--------------------
mat2: [saved tensor]
self: [saved tensor]"]
	140377787512960 -> 140377787513152
	140377787512960 [label="ViewBackward0
---------------------------
self_sizes: (6, 8, 100, 32)"]
	140377787512720 -> 140377787512960
	140377787512720 [label=CloneBackward0]
	140377787512528 -> 140377787512720
	140377787512528 [label="TransposeBackward0
------------------
dim0: 1
dim1: 2"]
	140377787512336 -> 140377787512528
	140377787512336 [label="ViewBackward0
-------------------------
self_sizes: (6, 100, 256)"]
	140377787512144 -> 140377787512336
	140377787512144 -> 140377786948880 [dir=none]
	140377786948880 [label="other
 ()" fillcolor=orange]
	140377787512144 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140377787512048 -> 140377787512144
	140377787512048 [label="ViewBackward0
----------------------
self_sizes: (600, 256)"]
	140377787511856 -> 140377787512048
	140377787511856 -> 140377786948960 [dir=none]
	140377786948960 [label="mat1
 (600, 256)" fillcolor=orange]
	140377787511856 -> 140377786948800 [dir=none]
	140377786948800 [label="mat2
 (256, 256)" fillcolor=orange]
	140377787511856 [label="AddmmBackward0
----------------------------
alpha       :              1
beta        :              1
mat1        : [saved tensor]
mat1_sizes  :     (600, 256)
mat1_strides:       (256, 1)
mat2        : [saved tensor]
mat2_sizes  :     (256, 256)
mat2_strides:       (1, 256)"]
	140377787511664 -> 140377787511856
	140377785686752 [label="model.decoder.layers.0.encoder_attn.q_proj.bias
 (256)" fillcolor=lightblue]
	140377785686752 -> 140377787511664
	140377787511664 [label=AccumulateGrad]
	140377787511808 -> 140377787511856
	140377787511808 [label="ViewBackward0
-------------------------
self_sizes: (6, 100, 256)"]
	140377787511472 -> 140377787511808
	140377787511472 [label="AddBackward0
------------
alpha: 1"]
	140377787098352 -> 140377787511472
	140377796616496 -> 140377787511472
	140377787512912 -> 140377787511856
	140377787512912 [label=TBackward0]
	140377787511232 -> 140377787512912
	140377785686832 [label="model.decoder.layers.0.encoder_attn.q_proj.weight
 (256, 256)" fillcolor=lightblue]
	140377785686832 -> 140377787511232
	140377787511232 [label=AccumulateGrad]
	140377787513008 -> 140377787513152
	140377787513008 [label="TransposeBackward0
------------------
dim0: 1
dim1: 2"]
	140377787512480 -> 140377787513008
	140377787512480 [label="ViewBackward0
---------------------------
self_sizes: (6, 8, 782, 32)"]
	140377787512096 -> 140377787512480
	140377787512096 [label=CloneBackward0]
	140377787511616 -> 140377787512096
	140377787511616 [label="TransposeBackward0
------------------
dim0: 1
dim1: 2"]
	140377787511136 -> 140377787511616
	140377787511136 [label="ViewBackward0
-------------------------
self_sizes: (6, 782, 256)"]
	140377787511424 -> 140377787511136
	140377787511424 [label="ViewBackward0
-----------------------
self_sizes: (4692, 256)"]
	140377787511040 -> 140377787511424
	140377787511040 -> 140377786948320 [dir=none]
	140377786948320 [label="mat1
 (4692, 256)" fillcolor=orange]
	140377787511040 -> 140377786947760 [dir=none]
	140377786947760 [label="mat2
 (256, 256)" fillcolor=orange]
	140377787511040 [label="AddmmBackward0
----------------------------
alpha       :              1
beta        :              1
mat1        : [saved tensor]
mat1_sizes  :    (4692, 256)
mat1_strides:       (256, 1)
mat2        : [saved tensor]
mat2_sizes  :     (256, 256)
mat2_strides:       (1, 256)"]
	140377787510896 -> 140377787511040
	140377785687072 [label="model.decoder.layers.0.encoder_attn.k_proj.bias
 (256)" fillcolor=lightblue]
	140377785687072 -> 140377787510896
	140377787510896 [label=AccumulateGrad]
	140377787510848 -> 140377787511040
	140377787510848 [label="ViewBackward0
-------------------------
self_sizes: (6, 782, 256)"]
	140377787486016 -> 140377787510848
	140377787486016 [label="AddBackward0
------------
alpha: 1"]
	140377787485632 -> 140377787486016
	140377787485632 -> 140377785798880 [dir=none]
	140377785798880 [label="bias
 (256)" fillcolor=orange]
	140377787485632 -> 140377787004064 [dir=none]
	140377787004064 [label="input
 (6, 782, 256)" fillcolor=orange]
	140377787485632 -> 140377786949200 [dir=none]
	140377786949200 [label="result1
 (6, 782, 1)" fillcolor=orange]
	140377787485632 -> 140377786948640 [dir=none]
	140377786948640 [label="result2
 (6, 782, 1)" fillcolor=orange]
	140377787485632 -> 140377785798800 [dir=none]
	140377785798800 [label="weight
 (256)" fillcolor=orange]
	140377787485632 [label="NativeLayerNormBackward0
--------------------------------
bias            : [saved tensor]
input           : [saved tensor]
normalized_shape:         (256,)
result1         : [saved tensor]
result2         : [saved tensor]
weight          : [saved tensor]"]
	140377787485536 -> 140377787485632
	140377787485536 [label="AddBackward0
------------
alpha: 1"]
	140377787485152 -> 140377787485536
	140377787485152 -> 140377785799360 [dir=none]
	140377785799360 [label="bias
 (256)" fillcolor=orange]
	140377787485152 -> 140377787004304 [dir=none]
	140377787004304 [label="input
 (6, 782, 256)" fillcolor=orange]
	140377787485152 -> 140377786949360 [dir=none]
	140377786949360 [label="result1
 (6, 782, 1)" fillcolor=orange]
	140377787485152 -> 140377786949120 [dir=none]
	140377786949120 [label="result2
 (6, 782, 1)" fillcolor=orange]
	140377787485152 -> 140377785799280 [dir=none]
	140377785799280 [label="weight
 (256)" fillcolor=orange]
	140377787485152 [label="NativeLayerNormBackward0
--------------------------------
bias            : [saved tensor]
input           : [saved tensor]
normalized_shape:         (256,)
result1         : [saved tensor]
result2         : [saved tensor]
weight          : [saved tensor]"]
	140377787484816 -> 140377787485152
	140377787484816 [label="AddBackward0
------------
alpha: 1"]
	140377787484528 -> 140377787484816
	140377787484528 -> 140377785800160 [dir=none]
	140377785800160 [label="bias
 (256)" fillcolor=orange]
	140377787484528 -> 140377787002944 [dir=none]
	140377787002944 [label="input
 (6, 782, 256)" fillcolor=orange]
	140377787484528 -> 140377786949040 [dir=none]
	140377786949040 [label="result1
 (6, 782, 1)" fillcolor=orange]
	140377787484528 -> 140377786949280 [dir=none]
	140377786949280 [label="result2
 (6, 782, 1)" fillcolor=orange]
	140377787484528 -> 140377785800080 [dir=none]
	140377785800080 [label="weight
 (256)" fillcolor=orange]
	140377787484528 [label="NativeLayerNormBackward0
--------------------------------
bias            : [saved tensor]
input           : [saved tensor]
normalized_shape:         (256,)
result1         : [saved tensor]
result2         : [saved tensor]
weight          : [saved tensor]"]
	140377787484288 -> 140377787484528
	140377787484288 [label="AddBackward0
------------
alpha: 1"]
	140377787483904 -> 140377787484288
	140377787483904 -> 140377785800640 [dir=none]
	140377785800640 [label="bias
 (256)" fillcolor=orange]
	140377787483904 -> 140377787003184 [dir=none]
	140377787003184 [label="input
 (6, 782, 256)" fillcolor=orange]
	140377787483904 -> 140377786948720 [dir=none]
	140377786948720 [label="result1
 (6, 782, 1)" fillcolor=orange]
	140377787483904 -> 140377786949440 [dir=none]
	140377786949440 [label="result2
 (6, 782, 1)" fillcolor=orange]
	140377787483904 -> 140377785800560 [dir=none]
	140377785800560 [label="weight
 (256)" fillcolor=orange]
	140377787483904 [label="NativeLayerNormBackward0
--------------------------------
bias            : [saved tensor]
input           : [saved tensor]
normalized_shape:         (256,)
result1         : [saved tensor]
result2         : [saved tensor]
weight          : [saved tensor]"]
	140377787483664 -> 140377787483904
	140377787483664 [label="AddBackward0
------------
alpha: 1"]
	140377787483280 -> 140377787483664
	140377787483280 -> 140377787084528 [dir=none]
	140377787084528 [label="bias
 (256)" fillcolor=orange]
	140377787483280 -> 140377787038592 [dir=none]
	140377787038592 [label="input
 (6, 782, 256)" fillcolor=orange]
	140377787483280 -> 140377786949520 [dir=none]
	140377786949520 [label="result1
 (6, 782, 1)" fillcolor=orange]
	140377787483280 -> 140377601888400 [dir=none]
	140377601888400 [label="result2
 (6, 782, 1)" fillcolor=orange]
	140377787483280 -> 140377787084608 [dir=none]
	140377787084608 [label="weight
 (256)" fillcolor=orange]
	140377787483280 [label="NativeLayerNormBackward0
--------------------------------
bias            : [saved tensor]
input           : [saved tensor]
normalized_shape:         (256,)
result1         : [saved tensor]
result2         : [saved tensor]
weight          : [saved tensor]"]
	140377787483040 -> 140377787483280
	140377787483040 [label="AddBackward0
------------
alpha: 1"]
	140377787482752 -> 140377787483040
	140377787482752 -> 140377787083648 [dir=none]
	140377787083648 [label="bias
 (256)" fillcolor=orange]
	140377787482752 -> 140377787038832 [dir=none]
	140377787038832 [label="input
 (6, 782, 256)" fillcolor=orange]
	140377787482752 -> 140377601888560 [dir=none]
	140377601888560 [label="result1
 (6, 782, 1)" fillcolor=orange]
	140377787482752 -> 140377601888320 [dir=none]
	140377601888320 [label="result2
 (6, 782, 1)" fillcolor=orange]
	140377787482752 -> 140377786556304 [dir=none]
	140377786556304 [label="weight
 (256)" fillcolor=orange]
	140377787482752 [label="NativeLayerNormBackward0
--------------------------------
bias            : [saved tensor]
input           : [saved tensor]
normalized_shape:         (256,)
result1         : [saved tensor]
result2         : [saved tensor]
weight          : [saved tensor]"]
	140377787482416 -> 140377787482752
	140377787482416 [label="AddBackward0
------------
alpha: 1"]
	140377787482224 -> 140377787482416
	140377787482224 -> 140377786661040 [dir=none]
	140377786661040 [label="bias
 (256)" fillcolor=orange]
	140377787482224 -> 140377787037472 [dir=none]
	140377787037472 [label="input
 (6, 782, 256)" fillcolor=orange]
	140377787482224 -> 140377601888720 [dir=none]
	140377601888720 [label="result1
 (6, 782, 1)" fillcolor=orange]
	140377787482224 -> 140377601888480 [dir=none]
	140377601888480 [label="result2
 (6, 782, 1)" fillcolor=orange]
	140377787482224 -> 140377786662720 [dir=none]
	140377786662720 [label="weight
 (256)" fillcolor=orange]
	140377787482224 [label="NativeLayerNormBackward0
--------------------------------
bias            : [saved tensor]
input           : [saved tensor]
normalized_shape:         (256,)
result1         : [saved tensor]
result2         : [saved tensor]
weight          : [saved tensor]"]
	140377787465440 -> 140377787482224
	140377787465440 [label="AddBackward0
------------
alpha: 1"]
	140377787465056 -> 140377787465440
	140377787465056 -> 140377786232480 [dir=none]
	140377786232480 [label="bias
 (256)" fillcolor=orange]
	140377787465056 -> 140377787037712 [dir=none]
	140377787037712 [label="input
 (6, 782, 256)" fillcolor=orange]
	140377787465056 -> 140377601888880 [dir=none]
	140377601888880 [label="result1
 (6, 782, 1)" fillcolor=orange]
	140377787465056 -> 140377601888640 [dir=none]
	140377601888640 [label="result2
 (6, 782, 1)" fillcolor=orange]
	140377787465056 -> 140377786232400 [dir=none]
	140377786232400 [label="weight
 (256)" fillcolor=orange]
	140377787465056 [label="NativeLayerNormBackward0
--------------------------------
bias            : [saved tensor]
input           : [saved tensor]
normalized_shape:         (256,)
result1         : [saved tensor]
result2         : [saved tensor]
weight          : [saved tensor]"]
	140377787464816 -> 140377787465056
	140377787464816 [label="AddBackward0
------------
alpha: 1"]
	140377787464432 -> 140377787464816
	140377787464432 -> 140377786324848 [dir=none]
	140377786324848 [label="bias
 (256)" fillcolor=orange]
	140377787464432 -> 140377787036352 [dir=none]
	140377787036352 [label="input
 (6, 782, 256)" fillcolor=orange]
	140377787464432 -> 140377601889040 [dir=none]
	140377601889040 [label="result1
 (6, 782, 1)" fillcolor=orange]
	140377787464432 -> 140377601888800 [dir=none]
	140377601888800 [label="result2
 (6, 782, 1)" fillcolor=orange]
	140377787464432 -> 140377786326848 [dir=none]
	140377786326848 [label="weight
 (256)" fillcolor=orange]
	140377787464432 [label="NativeLayerNormBackward0
--------------------------------
bias            : [saved tensor]
input           : [saved tensor]
normalized_shape:         (256,)
result1         : [saved tensor]
result2         : [saved tensor]
weight          : [saved tensor]"]
	140377787464192 -> 140377787464432
	140377787464192 [label="AddBackward0
------------
alpha: 1"]
	140377787463904 -> 140377787464192
	140377787463904 -> 140377786427168 [dir=none]
	140377786427168 [label="bias
 (256)" fillcolor=orange]
	140377787463904 -> 140377787036592 [dir=none]
	140377787036592 [label="input
 (6, 782, 256)" fillcolor=orange]
	140377787463904 -> 140377601889200 [dir=none]
	140377601889200 [label="result1
 (6, 782, 1)" fillcolor=orange]
	140377787463904 -> 140377601888960 [dir=none]
	140377601888960 [label="result2
 (6, 782, 1)" fillcolor=orange]
	140377787463904 -> 140377785988656 [dir=none]
	140377785988656 [label="weight
 (256)" fillcolor=orange]
	140377787463904 [label="NativeLayerNormBackward0
--------------------------------
bias            : [saved tensor]
input           : [saved tensor]
normalized_shape:         (256,)
result1         : [saved tensor]
result2         : [saved tensor]
weight          : [saved tensor]"]
	140377787463568 -> 140377787463904
	140377787463568 [label="AddBackward0
------------
alpha: 1"]
	140377787463184 -> 140377787463568
	140377787463184 -> 140377786098128 [dir=none]
	140377786098128 [label="bias
 (256)" fillcolor=orange]
	140377787463184 -> 140377787067904 [dir=none]
	140377787067904 [label="input
 (6, 782, 256)" fillcolor=orange]
	140377787463184 -> 140377601889360 [dir=none]
	140377601889360 [label="result1
 (6, 782, 1)" fillcolor=orange]
	140377787463184 -> 140377601889120 [dir=none]
	140377601889120 [label="result2
 (6, 782, 1)" fillcolor=orange]
	140377787463184 -> 140377786099648 [dir=none]
	140377786099648 [label="weight
 (256)" fillcolor=orange]
	140377787463184 [label="NativeLayerNormBackward0
--------------------------------
bias            : [saved tensor]
input           : [saved tensor]
normalized_shape:         (256,)
result1         : [saved tensor]
result2         : [saved tensor]
weight          : [saved tensor]"]
	140377787463040 -> 140377787463184
	140377787463040 [label="AddBackward0
------------
alpha: 1"]
	140377787462656 -> 140377787463040
	140377787462656 -> 140377786101168 [dir=none]
	140377786101168 [label="bias
 (256)" fillcolor=orange]
	140377787462656 -> 140377787068144 [dir=none]
	140377787068144 [label="input
 (6, 782, 256)" fillcolor=orange]
	140377787462656 -> 140377601889520 [dir=none]
	140377601889520 [label="result1
 (6, 782, 1)" fillcolor=orange]
	140377787462656 -> 140377601889280 [dir=none]
	140377601889280 [label="result2
 (6, 782, 1)" fillcolor=orange]
	140377787462656 -> 140377786101488 [dir=none]
	140377786101488 [label="weight
 (256)" fillcolor=orange]
	140377787462656 [label="NativeLayerNormBackward0
--------------------------------
bias            : [saved tensor]
input           : [saved tensor]
normalized_shape:         (256,)
result1         : [saved tensor]
result2         : [saved tensor]
weight          : [saved tensor]"]
	140377787462320 -> 140377787462656
	140377787462320 [label="AddBackward0
------------
alpha: 1"]
	140377787462032 -> 140377787462320
	140377787462032 -> 140377601889680 [dir=none]
	140377601889680 [label="other
 (6, 782, 256)" fillcolor=orange]
	140377787462032 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140377787461792 -> 140377787462032
	140377787461792 [label="PermuteBackward0
----------------
dims: (0, 2, 1)"]
	140377787461984 -> 140377787461792
	140377787461984 [label="ReshapeAliasBackward0
----------------------------
self_sizes: (6, 256, 23, 34)"]
	140377787440864 -> 140377787461984
	140377787440864 -> 140377787065824 [dir=none]
	140377787065824 [label="input
 (6, 2048, 23, 34)" fillcolor=orange]
	140377787440864 -> 140377786100528 [dir=none]
	140377786100528 [label="weight
 (256, 2048, 1, 1)" fillcolor=orange]
	140377787440864 [label="ConvolutionBackward0
------------------------------
bias_sizes_opt:         (256,)
dilation      :         (1, 1)
groups        :              1
input         : [saved tensor]
output_padding:         (0, 0)
padding       :         (0, 0)
stride        :         (1, 1)
transposed    :          False
weight        : [saved tensor]"]
	140377787440768 -> 140377787440864
	140377787440768 -> 140377601889840 [dir=none]
	140377601889840 [label="result
 (6, 2048, 23, 34)" fillcolor=orange]
	140377787440768 [label="ReluBackward0
----------------------
result: [saved tensor]"]
	140377787440384 -> 140377787440768
	140377787440384 [label="AddBackward0
------------
alpha: 1"]
	140377787440192 -> 140377787440384
	140377787440192 [label="AddBackward0
------------
alpha: 1"]
	140377787439952 -> 140377787440192
	140377787439952 -> 140377787065744 [dir=none]
	140377787065744 [label="other
 (1, 2048, 1, 1)" fillcolor=orange]
	140377787439952 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140377787439760 -> 140377787439952
	140377787439760 -> 140377787065664 [dir=none]
	140377787065664 [label="input
 (6, 512, 23, 34)" fillcolor=orange]
	140377787439760 -> 140377786101248 [dir=none]
	140377786101248 [label="weight
 (2048, 512, 1, 1)" fillcolor=orange]
	140377787439760 [label="ConvolutionBackward0
------------------------------
bias_sizes_opt:           (0,)
dilation      :         (1, 1)
groups        :              1
input         : [saved tensor]
output_padding:         (0, 0)
padding       :         (0, 0)
stride        :         (1, 1)
transposed    :          False
weight        : [saved tensor]"]
	140377787439568 -> 140377787439760
	140377787439568 -> 140377601889920 [dir=none]
	140377601889920 [label="result
 (6, 512, 23, 34)" fillcolor=orange]
	140377787439568 [label="ReluBackward0
----------------------
result: [saved tensor]"]
	140377787439328 -> 140377787439568
	140377787439328 [label="AddBackward0
------------
alpha: 1"]
	140377787439136 -> 140377787439328
	140377787439136 -> 140377787065584 [dir=none]
	140377787065584 [label="other
 (1, 512, 1, 1)" fillcolor=orange]
	140377787439136 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140377787439040 -> 140377787439136
	140377787439040 -> 140377787065504 [dir=none]
	140377787065504 [label="input
 (6, 512, 23, 34)" fillcolor=orange]
	140377787439040 -> 140377786100768 [dir=none]
	140377786100768 [label="weight
 (512, 512, 3, 3)" fillcolor=orange]
	140377787439040 [label="ConvolutionBackward0
------------------------------
bias_sizes_opt:           (0,)
dilation      :         (1, 1)
groups        :              1
input         : [saved tensor]
output_padding:         (0, 0)
padding       :         (1, 1)
stride        :         (1, 1)
transposed    :          False
weight        : [saved tensor]"]
	140377787438848 -> 140377787439040
	140377787438848 -> 140377601889760 [dir=none]
	140377601889760 [label="result
 (6, 512, 23, 34)" fillcolor=orange]
	140377787438848 [label="ReluBackward0
----------------------
result: [saved tensor]"]
	140377787438512 -> 140377787438848
	140377787438512 [label="AddBackward0
------------
alpha: 1"]
	140377787438320 -> 140377787438512
	140377787438320 -> 140377787065424 [dir=none]
	140377787065424 [label="other
 (1, 512, 1, 1)" fillcolor=orange]
	140377787438320 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140377787438224 -> 140377787438320
	140377787438224 -> 140377787065344 [dir=none]
	140377787065344 [label="input
 (6, 2048, 23, 34)" fillcolor=orange]
	140377787438224 -> 140377786100128 [dir=none]
	140377786100128 [label="weight
 (512, 2048, 1, 1)" fillcolor=orange]
	140377787438224 [label="ConvolutionBackward0
------------------------------
bias_sizes_opt:           (0,)
dilation      :         (1, 1)
groups        :              1
input         : [saved tensor]
output_padding:         (0, 0)
padding       :         (0, 0)
stride        :         (1, 1)
transposed    :          False
weight        : [saved tensor]"]
	140377787440240 -> 140377787438224
	140377787440240 -> 140377601890080 [dir=none]
	140377601890080 [label="result
 (6, 2048, 23, 34)" fillcolor=orange]
	140377787440240 [label="ReluBackward0
----------------------
result: [saved tensor]"]
	140377787437936 -> 140377787440240
	140377787437936 [label="AddBackward0
------------
alpha: 1"]
	140377787437744 -> 140377787437936
	140377787437744 [label="AddBackward0
------------
alpha: 1"]
	140377787437504 -> 140377787437744
	140377787437504 -> 140377787065264 [dir=none]
	140377787065264 [label="other
 (1, 2048, 1, 1)" fillcolor=orange]
	140377787437504 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140377787437312 -> 140377787437504
	140377787437312 -> 140377787065184 [dir=none]
	140377787065184 [label="input
 (6, 512, 23, 34)" fillcolor=orange]
	140377787437312 -> 140377786099568 [dir=none]
	140377786099568 [label="weight
 (2048, 512, 1, 1)" fillcolor=orange]
	140377787437312 [label="ConvolutionBackward0
------------------------------
bias_sizes_opt:           (0,)
dilation      :         (1, 1)
groups        :              1
input         : [saved tensor]
output_padding:         (0, 0)
padding       :         (0, 0)
stride        :         (1, 1)
transposed    :          False
weight        : [saved tensor]"]
	140377787437216 -> 140377787437312
	140377787437216 -> 140377601890320 [dir=none]
	140377601890320 [label="result
 (6, 512, 23, 34)" fillcolor=orange]
	140377787437216 [label="ReluBackward0
----------------------
result: [saved tensor]"]
	140377787412240 -> 140377787437216
	140377787412240 [label="AddBackward0
------------
alpha: 1"]
	140377787412048 -> 140377787412240
	140377787412048 -> 140377787065104 [dir=none]
	140377787065104 [label="other
 (1, 512, 1, 1)" fillcolor=orange]
	140377787412048 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140377787411856 -> 140377787412048
	140377787411856 -> 140377787064864 [dir=none]
	140377787064864 [label="input
 (6, 512, 23, 34)" fillcolor=orange]
	140377787411856 -> 140377786099088 [dir=none]
	140377786099088 [label="weight
 (512, 512, 3, 3)" fillcolor=orange]
	140377787411856 [label="ConvolutionBackward0
------------------------------
bias_sizes_opt:           (0,)
dilation      :         (1, 1)
groups        :              1
input         : [saved tensor]
output_padding:         (0, 0)
padding       :         (1, 1)
stride        :         (1, 1)
transposed    :          False
weight        : [saved tensor]"]
	140377787411760 -> 140377787411856
	140377787411760 -> 140377601890400 [dir=none]
	140377601890400 [label="result
 (6, 512, 23, 34)" fillcolor=orange]
	140377787411760 [label="ReluBackward0
----------------------
result: [saved tensor]"]
	140377787411520 -> 140377787411760
	140377787411520 [label="AddBackward0
------------
alpha: 1"]
	140377787411328 -> 140377787411520
	140377787411328 -> 140377787065024 [dir=none]
	140377787065024 [label="other
 (1, 512, 1, 1)" fillcolor=orange]
	140377787411328 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140377787411136 -> 140377787411328
	140377787411136 -> 140377787064784 [dir=none]
	140377787064784 [label="input
 (6, 2048, 23, 34)" fillcolor=orange]
	140377787411136 -> 140377786098448 [dir=none]
	140377786098448 [label="weight
 (512, 2048, 1, 1)" fillcolor=orange]
	140377787411136 [label="ConvolutionBackward0
------------------------------
bias_sizes_opt:           (0,)
dilation      :         (1, 1)
groups        :              1
input         : [saved tensor]
output_padding:         (0, 0)
padding       :         (0, 0)
stride        :         (1, 1)
transposed    :          False
weight        : [saved tensor]"]
	140377787437888 -> 140377787411136
	140377787437888 -> 140377601889600 [dir=none]
	140377601889600 [label="result
 (6, 2048, 23, 34)" fillcolor=orange]
	140377787437888 [label="ReluBackward0
----------------------
result: [saved tensor]"]
	140377787410848 -> 140377787437888
	140377787410848 [label="AddBackward0
------------
alpha: 1"]
	140377787410656 -> 140377787410848
	140377787410656 [label="AddBackward0
------------
alpha: 1"]
	140377787410320 -> 140377787410656
	140377787410320 -> 140377787064704 [dir=none]
	140377787064704 [label="other
 (1, 2048, 1, 1)" fillcolor=orange]
	140377787410320 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140377787410128 -> 140377787410320
	140377787410128 -> 140377787064624 [dir=none]
	140377787064624 [label="input
 (6, 512, 23, 34)" fillcolor=orange]
	140377787410128 -> 140377786097888 [dir=none]
	140377786097888 [label="weight
 (2048, 512, 1, 1)" fillcolor=orange]
	140377787410128 [label="ConvolutionBackward0
------------------------------
bias_sizes_opt:           (0,)
dilation      :         (1, 1)
groups        :              1
input         : [saved tensor]
output_padding:         (0, 0)
padding       :         (0, 0)
stride        :         (1, 1)
transposed    :          False
weight        : [saved tensor]"]
	140377787410032 -> 140377787410128
	140377787410032 -> 140377601890640 [dir=none]
	140377601890640 [label="result
 (6, 512, 23, 34)" fillcolor=orange]
	140377787410032 [label="ReluBackward0
----------------------
result: [saved tensor]"]
	140377787409792 -> 140377787410032
	140377787409792 [label="AddBackward0
------------
alpha: 1"]
	140377787409600 -> 140377787409792
	140377787409600 -> 140377787064544 [dir=none]
	140377787064544 [label="other
 (1, 512, 1, 1)" fillcolor=orange]
	140377787409600 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140377787409408 -> 140377787409600
	140377787409408 -> 140377787064464 [dir=none]
	140377787064464 [label="input
 (6, 512, 45, 68)" fillcolor=orange]
	140377787409408 -> 140377785990816 [dir=none]
	140377785990816 [label="weight
 (512, 512, 3, 3)" fillcolor=orange]
	140377787409408 [label="ConvolutionBackward0
------------------------------
bias_sizes_opt:           (0,)
dilation      :         (1, 1)
groups        :              1
input         : [saved tensor]
output_padding:         (0, 0)
padding       :         (1, 1)
stride        :         (2, 2)
transposed    :          False
weight        : [saved tensor]"]
	140377787409216 -> 140377787409408
	140377787409216 -> 140377601890160 [dir=none]
	140377601890160 [label="result
 (6, 512, 45, 68)" fillcolor=orange]
	140377787409216 [label="ReluBackward0
----------------------
result: [saved tensor]"]
	140377787408976 -> 140377787409216
	140377787408976 [label="AddBackward0
------------
alpha: 1"]
	140377787408784 -> 140377787408976
	140377787408784 -> 140377787064384 [dir=none]
	140377787064384 [label="other
 (1, 512, 1, 1)" fillcolor=orange]
	140377787408784 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140377787408592 -> 140377787408784
	140377787408592 -> 140377787092880 [dir=none]
	140377787092880 [label="input
 (6, 1024, 45, 68)" fillcolor=orange]
	140377787408592 -> 140377785990176 [dir=none]
	140377785990176 [label="weight
 (512, 1024, 1, 1)" fillcolor=orange]
	140377787408592 [label="ConvolutionBackward0
------------------------------
bias_sizes_opt:           (0,)
dilation      :         (1, 1)
groups        :              1
input         : [saved tensor]
output_padding:         (0, 0)
padding       :         (0, 0)
stride        :         (1, 1)
transposed    :          False
weight        : [saved tensor]"]
	140377787408544 -> 140377787408592
	140377787408544 -> 140377601890240 [dir=none]
	140377601890240 [label="result
 (6, 1024, 45, 68)" fillcolor=orange]
	140377787408544 [label="ReluBackward0
----------------------
result: [saved tensor]"]
	140377787387712 -> 140377787408544
	140377787387712 [label="AddBackward0
------------
alpha: 1"]
	140377787387520 -> 140377787387712
	140377787387520 [label="AddBackward0
------------
alpha: 1"]
	140377787387184 -> 140377787387520
	140377787387184 -> 140377787092800 [dir=none]
	140377787092800 [label="other
 (1, 1024, 1, 1)" fillcolor=orange]
	140377787387184 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140377787386992 -> 140377787387184
	140377787386992 -> 140377787092720 [dir=none]
	140377787092720 [label="input
 (6, 256, 45, 68)" fillcolor=orange]
	140377787386992 -> 140377785989056 [dir=none]
	140377785989056 [label="weight
 (1024, 256, 1, 1)" fillcolor=orange]
	140377787386992 [label="ConvolutionBackward0
------------------------------
bias_sizes_opt:           (0,)
dilation      :         (1, 1)
groups        :              1
input         : [saved tensor]
output_padding:         (0, 0)
padding       :         (0, 0)
stride        :         (1, 1)
transposed    :          False
weight        : [saved tensor]"]
	140377787386896 -> 140377787386992
	140377787386896 -> 140377601890880 [dir=none]
	140377601890880 [label="result
 (6, 256, 45, 68)" fillcolor=orange]
	140377787386896 [label="ReluBackward0
----------------------
result: [saved tensor]"]
	140377787386656 -> 140377787386896
	140377787386656 [label="AddBackward0
------------
alpha: 1"]
	140377787386464 -> 140377787386656
	140377787386464 -> 140377787092640 [dir=none]
	140377787092640 [label="other
 (1, 256, 1, 1)" fillcolor=orange]
	140377787386464 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140377787386272 -> 140377787386464
	140377787386272 -> 140377787092560 [dir=none]
	140377787092560 [label="input
 (6, 256, 45, 68)" fillcolor=orange]
	140377787386272 -> 140377785988576 [dir=none]
	140377785988576 [label="weight
 (256, 256, 3, 3)" fillcolor=orange]
	140377787386272 [label="ConvolutionBackward0
------------------------------
bias_sizes_opt:           (0,)
dilation      :         (1, 1)
groups        :              1
input         : [saved tensor]
output_padding:         (0, 0)
padding       :         (1, 1)
stride        :         (1, 1)
transposed    :          False
weight        : [saved tensor]"]
	140377787386080 -> 140377787386272
	140377787386080 -> 140377601890560 [dir=none]
	140377601890560 [label="result
 (6, 256, 45, 68)" fillcolor=orange]
	140377787386080 [label="ReluBackward0
----------------------
result: [saved tensor]"]
	140377787385840 -> 140377787386080
	140377787385840 [label="AddBackward0
------------
alpha: 1"]
	140377787385648 -> 140377787385840
	140377787385648 -> 140377787092480 [dir=none]
	140377787092480 [label="other
 (1, 256, 1, 1)" fillcolor=orange]
	140377787385648 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140377787385456 -> 140377787385648
	140377787385456 -> 140377787092400 [dir=none]
	140377787092400 [label="input
 (6, 1024, 45, 68)" fillcolor=orange]
	140377787385456 -> 140377785987936 [dir=none]
	140377785987936 [label="weight
 (256, 1024, 1, 1)" fillcolor=orange]
	140377787385456 [label="ConvolutionBackward0
------------------------------
bias_sizes_opt:           (0,)
dilation      :         (1, 1)
groups        :              1
input         : [saved tensor]
output_padding:         (0, 0)
padding       :         (0, 0)
stride        :         (1, 1)
transposed    :          False
weight        : [saved tensor]"]
	140377787387568 -> 140377787385456
	140377787387568 -> 140377601890000 [dir=none]
	140377601890000 [label="result
 (6, 1024, 45, 68)" fillcolor=orange]
	140377787387568 [label="ReluBackward0
----------------------
result: [saved tensor]"]
	140377787385120 -> 140377787387568
	140377787385120 [label="AddBackward0
------------
alpha: 1"]
	140377787384544 -> 140377787385120
	140377787384544 [label="AddBackward0
------------
alpha: 1"]
	140377787384016 -> 140377787384544
	140377787384016 -> 140377787092320 [dir=none]
	140377787092320 [label="other
 (1, 1024, 1, 1)" fillcolor=orange]
	140377787384016 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140377787384592 -> 140377787384016
	140377787384592 -> 140377787092240 [dir=none]
	140377787092240 [label="input
 (6, 256, 45, 68)" fillcolor=orange]
	140377787384592 -> 140377785987376 [dir=none]
	140377785987376 [label="weight
 (1024, 256, 1, 1)" fillcolor=orange]
	140377787384592 [label="ConvolutionBackward0
------------------------------
bias_sizes_opt:           (0,)
dilation      :         (1, 1)
groups        :              1
input         : [saved tensor]
output_padding:         (0, 0)
padding       :         (0, 0)
stride        :         (1, 1)
transposed    :          False
weight        : [saved tensor]"]
	140377787378944 -> 140377787384592
	140377787378944 -> 140377601891120 [dir=none]
	140377601891120 [label="result
 (6, 256, 45, 68)" fillcolor=orange]
	140377787378944 [label="ReluBackward0
----------------------
result: [saved tensor]"]
	140377787377984 -> 140377787378944
	140377787377984 [label="AddBackward0
------------
alpha: 1"]
	140377787377552 -> 140377787377984
	140377787377552 -> 140377787092160 [dir=none]
	140377787092160 [label="other
 (1, 256, 1, 1)" fillcolor=orange]
	140377787377552 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140377787377072 -> 140377787377552
	140377787377072 -> 140377787092080 [dir=none]
	140377787092080 [label="input
 (6, 256, 45, 68)" fillcolor=orange]
	140377787377072 -> 140377786429168 [dir=none]
	140377786429168 [label="weight
 (256, 256, 3, 3)" fillcolor=orange]
	140377787377072 [label="ConvolutionBackward0
------------------------------
bias_sizes_opt:           (0,)
dilation      :         (1, 1)
groups        :              1
input         : [saved tensor]
output_padding:         (0, 0)
padding       :         (1, 1)
stride        :         (1, 1)
transposed    :          False
weight        : [saved tensor]"]
	140377787376880 -> 140377787377072
	140377787376880 -> 140377601890800 [dir=none]
	140377601890800 [label="result
 (6, 256, 45, 68)" fillcolor=orange]
	140377787376880 [label="ReluBackward0
----------------------
result: [saved tensor]"]
	140377787376784 -> 140377787376880
	140377787376784 [label="AddBackward0
------------
alpha: 1"]
	140377787376688 -> 140377787376784
	140377787376688 -> 140377787092000 [dir=none]
	140377787092000 [label="other
 (1, 256, 1, 1)" fillcolor=orange]
	140377787376688 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140377787376640 -> 140377787376688
	140377787376640 -> 140377787091920 [dir=none]
	140377787091920 [label="input
 (6, 1024, 45, 68)" fillcolor=orange]
	140377787376640 -> 140377786428528 [dir=none]
	140377786428528 [label="weight
 (256, 1024, 1, 1)" fillcolor=orange]
	140377787376640 [label="ConvolutionBackward0
------------------------------
bias_sizes_opt:           (0,)
dilation      :         (1, 1)
groups        :              1
input         : [saved tensor]
output_padding:         (0, 0)
padding       :         (0, 0)
stride        :         (1, 1)
transposed    :          False
weight        : [saved tensor]"]
	140377787385168 -> 140377787376640
	140377787385168 -> 140377601890480 [dir=none]
	140377601890480 [label="result
 (6, 1024, 45, 68)" fillcolor=orange]
	140377787385168 [label="ReluBackward0
----------------------
result: [saved tensor]"]
	140377787376016 -> 140377787385168
	140377787376016 [label="AddBackward0
------------
alpha: 1"]
	140377787375920 -> 140377787376016
	140377787375920 [label="AddBackward0
------------
alpha: 1"]
	140377787375680 -> 140377787375920
	140377787375680 -> 140377787091840 [dir=none]
	140377787091840 [label="other
 (1, 1024, 1, 1)" fillcolor=orange]
	140377787375680 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140377796031440 -> 140377787375680
	140377796031440 -> 140377787091760 [dir=none]
	140377787091760 [label="input
 (6, 256, 45, 68)" fillcolor=orange]
	140377796031440 -> 140377786427968 [dir=none]
	140377786427968 [label="weight
 (1024, 256, 1, 1)" fillcolor=orange]
	140377796031440 [label="ConvolutionBackward0
------------------------------
bias_sizes_opt:           (0,)
dilation      :         (1, 1)
groups        :              1
input         : [saved tensor]
output_padding:         (0, 0)
padding       :         (0, 0)
stride        :         (1, 1)
transposed    :          False
weight        : [saved tensor]"]
	140377796030720 -> 140377796031440
	140377796030720 -> 140377601891360 [dir=none]
	140377601891360 [label="result
 (6, 256, 45, 68)" fillcolor=orange]
	140377796030720 [label="ReluBackward0
----------------------
result: [saved tensor]"]
	140377796031536 -> 140377796030720
	140377796031536 [label="AddBackward0
------------
alpha: 1"]
	140377796030768 -> 140377796031536
	140377796030768 -> 140377787091680 [dir=none]
	140377787091680 [label="other
 (1, 256, 1, 1)" fillcolor=orange]
	140377796030768 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140377796030672 -> 140377796030768
	140377796030672 -> 140377787091600 [dir=none]
	140377787091600 [label="input
 (6, 256, 45, 68)" fillcolor=orange]
	140377796030672 -> 140377786427488 [dir=none]
	140377786427488 [label="weight
 (256, 256, 3, 3)" fillcolor=orange]
	140377796030672 [label="ConvolutionBackward0
------------------------------
bias_sizes_opt:           (0,)
dilation      :         (1, 1)
groups        :              1
input         : [saved tensor]
output_padding:         (0, 0)
padding       :         (1, 1)
stride        :         (1, 1)
transposed    :          False
weight        : [saved tensor]"]
	140377796034416 -> 140377796030672
	140377796034416 -> 140377601891040 [dir=none]
	140377601891040 [label="result
 (6, 256, 45, 68)" fillcolor=orange]
	140377796034416 [label="ReluBackward0
----------------------
result: [saved tensor]"]
	140377796034272 -> 140377796034416
	140377796034272 [label="AddBackward0
------------
alpha: 1"]
	140377796034224 -> 140377796034272
	140377796034224 -> 140377787091520 [dir=none]
	140377787091520 [label="other
 (1, 256, 1, 1)" fillcolor=orange]
	140377796034224 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140377796033936 -> 140377796034224
	140377796033936 -> 140377787091440 [dir=none]
	140377787091440 [label="input
 (6, 1024, 45, 68)" fillcolor=orange]
	140377796033936 -> 140377786426848 [dir=none]
	140377786426848 [label="weight
 (256, 1024, 1, 1)" fillcolor=orange]
	140377796033936 [label="ConvolutionBackward0
------------------------------
bias_sizes_opt:           (0,)
dilation      :         (1, 1)
groups        :              1
input         : [saved tensor]
output_padding:         (0, 0)
padding       :         (0, 0)
stride        :         (1, 1)
transposed    :          False
weight        : [saved tensor]"]
	140377787375824 -> 140377796033936
	140377787375824 -> 140377601890720 [dir=none]
	140377601890720 [label="result
 (6, 1024, 45, 68)" fillcolor=orange]
	140377787375824 [label="ReluBackward0
----------------------
result: [saved tensor]"]
	140377796032592 -> 140377787375824
	140377796032592 [label="AddBackward0
------------
alpha: 1"]
	140377796030864 -> 140377796032592
	140377796030864 [label="AddBackward0
------------
alpha: 1"]
	140377796582512 -> 140377796030864
	140377796582512 -> 140377787091360 [dir=none]
	140377787091360 [label="other
 (1, 1024, 1, 1)" fillcolor=orange]
	140377796582512 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140377796579728 -> 140377796582512
	140377796579728 -> 140377787091280 [dir=none]
	140377787091280 [label="input
 (6, 256, 45, 68)" fillcolor=orange]
	140377796579728 -> 140377786426288 [dir=none]
	140377786426288 [label="weight
 (1024, 256, 1, 1)" fillcolor=orange]
	140377796579728 [label="ConvolutionBackward0
------------------------------
bias_sizes_opt:           (0,)
dilation      :         (1, 1)
groups        :              1
input         : [saved tensor]
output_padding:         (0, 0)
padding       :         (0, 0)
stride        :         (1, 1)
transposed    :          False
weight        : [saved tensor]"]
	140377796579632 -> 140377796579728
	140377796579632 -> 140377601891600 [dir=none]
	140377601891600 [label="result
 (6, 256, 45, 68)" fillcolor=orange]
	140377796579632 [label="ReluBackward0
----------------------
result: [saved tensor]"]
	140377795914864 -> 140377796579632
	140377795914864 [label="AddBackward0
------------
alpha: 1"]
	140377795911792 -> 140377795914864
	140377795911792 -> 140377787091200 [dir=none]
	140377787091200 [label="other
 (1, 256, 1, 1)" fillcolor=orange]
	140377795911792 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140377795914528 -> 140377795911792
	140377795914528 -> 140377787091120 [dir=none]
	140377787091120 [label="input
 (6, 256, 45, 68)" fillcolor=orange]
	140377795914528 -> 140377786425808 [dir=none]
	140377786425808 [label="weight
 (256, 256, 3, 3)" fillcolor=orange]
	140377795914528 [label="ConvolutionBackward0
------------------------------
bias_sizes_opt:           (0,)
dilation      :         (1, 1)
groups        :              1
input         : [saved tensor]
output_padding:         (0, 0)
padding       :         (1, 1)
stride        :         (1, 1)
transposed    :          False
weight        : [saved tensor]"]
	140377787391040 -> 140377795914528
	140377787391040 -> 140377601891280 [dir=none]
	140377601891280 [label="result
 (6, 256, 45, 68)" fillcolor=orange]
	140377787391040 [label="ReluBackward0
----------------------
result: [saved tensor]"]
	140377787390608 -> 140377787391040
	140377787390608 [label="AddBackward0
------------
alpha: 1"]
	140377787390224 -> 140377787390608
	140377787390224 -> 140377787091040 [dir=none]
	140377787091040 [label="other
 (1, 256, 1, 1)" fillcolor=orange]
	140377787390224 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140377787389600 -> 140377787390224
	140377787389600 -> 140377787090960 [dir=none]
	140377787090960 [label="input
 (6, 1024, 45, 68)" fillcolor=orange]
	140377787389600 -> 140377786326768 [dir=none]
	140377786326768 [label="weight
 (256, 1024, 1, 1)" fillcolor=orange]
	140377787389600 [label="ConvolutionBackward0
------------------------------
bias_sizes_opt:           (0,)
dilation      :         (1, 1)
groups        :              1
input         : [saved tensor]
output_padding:         (0, 0)
padding       :         (0, 0)
stride        :         (1, 1)
transposed    :          False
weight        : [saved tensor]"]
	140377796034368 -> 140377787389600
	140377796034368 -> 140377601890960 [dir=none]
	140377601890960 [label="result
 (6, 1024, 45, 68)" fillcolor=orange]
	140377796034368 [label="ReluBackward0
----------------------
result: [saved tensor]"]
	140377787388640 -> 140377796034368
	140377787388640 [label="AddBackward0
------------
alpha: 1"]
	140377787388160 -> 140377787388640
	140377787388160 [label="AddBackward0
------------
alpha: 1"]
	140377787390032 -> 140377787388160
	140377787390032 -> 140377787090880 [dir=none]
	140377787090880 [label="other
 (1, 1024, 1, 1)" fillcolor=orange]
	140377787390032 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140377786882416 -> 140377787390032
	140377786882416 -> 140377787090800 [dir=none]
	140377787090800 [label="input
 (6, 256, 45, 68)" fillcolor=orange]
	140377786882416 -> 140377786326208 [dir=none]
	140377786326208 [label="weight
 (1024, 256, 1, 1)" fillcolor=orange]
	140377786882416 [label="ConvolutionBackward0
------------------------------
bias_sizes_opt:           (0,)
dilation      :         (1, 1)
groups        :              1
input         : [saved tensor]
output_padding:         (0, 0)
padding       :         (0, 0)
stride        :         (1, 1)
transposed    :          False
weight        : [saved tensor]"]
	140377786882224 -> 140377786882416
	140377786882224 -> 140377601891840 [dir=none]
	140377601891840 [label="result
 (6, 256, 45, 68)" fillcolor=orange]
	140377786882224 [label="ReluBackward0
----------------------
result: [saved tensor]"]
	140377786882080 -> 140377786882224
	140377786882080 [label="AddBackward0
------------
alpha: 1"]
	140377786881888 -> 140377786882080
	140377786881888 -> 140377787090720 [dir=none]
	140377787090720 [label="other
 (1, 256, 1, 1)" fillcolor=orange]
	140377786881888 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140377786881792 -> 140377786881888
	140377786881792 -> 140377787090480 [dir=none]
	140377787090480 [label="input
 (6, 256, 45, 68)" fillcolor=orange]
	140377786881792 -> 140377786325728 [dir=none]
	140377786325728 [label="weight
 (256, 256, 3, 3)" fillcolor=orange]
	140377786881792 [label="ConvolutionBackward0
------------------------------
bias_sizes_opt:           (0,)
dilation      :         (1, 1)
groups        :              1
input         : [saved tensor]
output_padding:         (0, 0)
padding       :         (1, 1)
stride        :         (1, 1)
transposed    :          False
weight        : [saved tensor]"]
	140377786881600 -> 140377786881792
	140377786881600 -> 140377601891520 [dir=none]
	140377601891520 [label="result
 (6, 256, 45, 68)" fillcolor=orange]
	140377786881600 [label="ReluBackward0
----------------------
result: [saved tensor]"]
	140377786881360 -> 140377786881600
	140377786881360 [label="AddBackward0
------------
alpha: 1"]
	140377786881168 -> 140377786881360
	140377786881168 -> 140377787090640 [dir=none]
	140377787090640 [label="other
 (1, 256, 1, 1)" fillcolor=orange]
	140377786881168 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140377786880976 -> 140377786881168
	140377786880976 -> 140377787090400 [dir=none]
	140377787090400 [label="input
 (6, 1024, 45, 68)" fillcolor=orange]
	140377786880976 -> 140377786325088 [dir=none]
	140377786325088 [label="weight
 (256, 1024, 1, 1)" fillcolor=orange]
	140377786880976 [label="ConvolutionBackward0
------------------------------
bias_sizes_opt:           (0,)
dilation      :         (1, 1)
groups        :              1
input         : [saved tensor]
output_padding:         (0, 0)
padding       :         (0, 0)
stride        :         (1, 1)
transposed    :          False
weight        : [saved tensor]"]
	140377787388112 -> 140377786880976
	140377787388112 -> 140377601891200 [dir=none]
	140377601891200 [label="result
 (6, 1024, 45, 68)" fillcolor=orange]
	140377787388112 [label="ReluBackward0
----------------------
result: [saved tensor]"]
	140377786880592 -> 140377787388112
	140377786880592 [label="AddBackward0
------------
alpha: 1"]
	140377786880496 -> 140377786880592
	140377786880496 [label="AddBackward0
------------
alpha: 1"]
	140377786880256 -> 140377786880496
	140377786880256 -> 140377787090320 [dir=none]
	140377787090320 [label="other
 (1, 1024, 1, 1)" fillcolor=orange]
	140377786880256 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140377786880160 -> 140377786880256
	140377786880160 -> 140377787090240 [dir=none]
	140377787090240 [label="input
 (6, 256, 45, 68)" fillcolor=orange]
	140377786880160 -> 140377786324528 [dir=none]
	140377786324528 [label="weight
 (1024, 256, 1, 1)" fillcolor=orange]
	140377786880160 [label="ConvolutionBackward0
------------------------------
bias_sizes_opt:           (0,)
dilation      :         (1, 1)
groups        :              1
input         : [saved tensor]
output_padding:         (0, 0)
padding       :         (0, 0)
stride        :         (1, 1)
transposed    :          False
weight        : [saved tensor]"]
	140377786882656 -> 140377786880160
	140377786882656 -> 140377601892080 [dir=none]
	140377601892080 [label="result
 (6, 256, 45, 68)" fillcolor=orange]
	140377786882656 [label="ReluBackward0
----------------------
result: [saved tensor]"]
	140377786882848 -> 140377786882656
	140377786882848 [label="AddBackward0
------------
alpha: 1"]
	140377786882944 -> 140377786882848
	140377786882944 -> 140377787090160 [dir=none]
	140377787090160 [label="other
 (1, 256, 1, 1)" fillcolor=orange]
	140377786882944 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140377786883040 -> 140377786882944
	140377786883040 -> 140377787090080 [dir=none]
	140377787090080 [label="input
 (6, 256, 90, 135)" fillcolor=orange]
	140377786883040 -> 140377786324048 [dir=none]
	140377786324048 [label="weight
 (256, 256, 3, 3)" fillcolor=orange]
	140377786883040 [label="ConvolutionBackward0
------------------------------
bias_sizes_opt:           (0,)
dilation      :         (1, 1)
groups        :              1
input         : [saved tensor]
output_padding:         (0, 0)
padding       :         (1, 1)
stride        :         (2, 2)
transposed    :          False
weight        : [saved tensor]"]
	140377786883136 -> 140377786883040
	140377786883136 -> 140377601891760 [dir=none]
	140377601891760 [label="result
 (6, 256, 90, 135)" fillcolor=orange]
	140377786883136 [label="ReluBackward0
----------------------
result: [saved tensor]"]
	140377786883280 -> 140377786883136
	140377786883280 [label="AddBackward0
------------
alpha: 1"]
	140377786883376 -> 140377786883280
	140377786883376 -> 140377787090000 [dir=none]
	140377787090000 [label="other
 (1, 256, 1, 1)" fillcolor=orange]
	140377786883376 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140377786883472 -> 140377786883376
	140377786883472 -> 140377787089920 [dir=none]
	140377787089920 [label="input
 (6, 512, 90, 135)" fillcolor=orange]
	140377786883472 -> 140377786323408 [dir=none]
	140377786323408 [label="weight
 (256, 512, 1, 1)" fillcolor=orange]
	140377786883472 [label="ConvolutionBackward0
------------------------------
bias_sizes_opt:           (0,)
dilation      :         (1, 1)
groups        :              1
input         : [saved tensor]
output_padding:         (0, 0)
padding       :         (0, 0)
stride        :         (1, 1)
transposed    :          False
weight        : [saved tensor]"]
	140377786883568 -> 140377786883472
	140377786883568 -> 140377601892240 [dir=none]
	140377601892240 [label="result
 (6, 512, 90, 135)" fillcolor=orange]
	140377786883568 [label="ReluBackward0
----------------------
result: [saved tensor]"]
	140377786883712 -> 140377786883568
	140377786883712 [label="AddBackward0
------------
alpha: 1"]
	140377786883808 -> 140377786883712
	140377786883808 [label="AddBackward0
------------
alpha: 1"]
	140377786883952 -> 140377786883808
	140377786883952 -> 140377787089840 [dir=none]
	140377787089840 [label="other
 (1, 512, 1, 1)" fillcolor=orange]
	140377786883952 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140377786884048 -> 140377786883952
	140377786884048 -> 140377787089760 [dir=none]
	140377787089760 [label="input
 (6, 128, 90, 135)" fillcolor=orange]
	140377786884048 -> 140377786232080 [dir=none]
	140377786232080 [label="weight
 (512, 128, 1, 1)" fillcolor=orange]
	140377786884048 [label="ConvolutionBackward0
------------------------------
bias_sizes_opt:           (0,)
dilation      :         (1, 1)
groups        :              1
input         : [saved tensor]
output_padding:         (0, 0)
padding       :         (0, 0)
stride        :         (1, 1)
transposed    :          False
weight        : [saved tensor]"]
	140377786883856 -> 140377786884048
	140377786883856 -> 140377601892000 [dir=none]
	140377601892000 [label="result
 (6, 128, 90, 135)" fillcolor=orange]
	140377786883856 [label="ReluBackward0
----------------------
result: [saved tensor]"]
	140377601945856 -> 140377786883856
	140377601945856 [label="AddBackward0
------------
alpha: 1"]
	140377601945952 -> 140377601945856
	140377601945952 -> 140377787089680 [dir=none]
	140377787089680 [label="other
 (1, 128, 1, 1)" fillcolor=orange]
	140377601945952 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140377601946048 -> 140377601945952
	140377601946048 -> 140377787089600 [dir=none]
	140377787089600 [label="input
 (6, 128, 90, 135)" fillcolor=orange]
	140377601946048 -> 140377786231600 [dir=none]
	140377786231600 [label="weight
 (128, 128, 3, 3)" fillcolor=orange]
	140377601946048 [label="ConvolutionBackward0
------------------------------
bias_sizes_opt:           (0,)
dilation      :         (1, 1)
groups        :              1
input         : [saved tensor]
output_padding:         (0, 0)
padding       :         (1, 1)
stride        :         (1, 1)
transposed    :          False
weight        : [saved tensor]"]
	140377601946144 -> 140377601946048
	140377601946144 -> 140377601892160 [dir=none]
	140377601892160 [label="result
 (6, 128, 90, 135)" fillcolor=orange]
	140377601946144 [label="ReluBackward0
----------------------
result: [saved tensor]"]
	140377601946288 -> 140377601946144
	140377601946288 [label="AddBackward0
------------
alpha: 1"]
	140377601946384 -> 140377601946288
	140377601946384 -> 140377787089520 [dir=none]
	140377787089520 [label="other
 (1, 128, 1, 1)" fillcolor=orange]
	140377601946384 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140377601946480 -> 140377601946384
	140377601946480 -> 140377787089440 [dir=none]
	140377787089440 [label="input
 (6, 512, 90, 135)" fillcolor=orange]
	140377601946480 -> 140377786231120 [dir=none]
	140377786231120 [label="weight
 (128, 512, 1, 1)" fillcolor=orange]
	140377601946480 [label="ConvolutionBackward0
------------------------------
bias_sizes_opt:           (0,)
dilation      :         (1, 1)
groups        :              1
input         : [saved tensor]
output_padding:         (0, 0)
padding       :         (0, 0)
stride        :         (1, 1)
transposed    :          False
weight        : [saved tensor]"]
	140377786883760 -> 140377601946480
	140377786883760 -> 140377601891920 [dir=none]
	140377601891920 [label="result
 (6, 512, 90, 135)" fillcolor=orange]
	140377786883760 [label="ReluBackward0
----------------------
result: [saved tensor]"]
	140377601946672 -> 140377786883760
	140377601946672 [label="AddBackward0
------------
alpha: 1"]
	140377601946768 -> 140377601946672
	140377601946768 [label="AddBackward0
------------
alpha: 1"]
	140377601946912 -> 140377601946768
	140377601946912 -> 140377787089360 [dir=none]
	140377787089360 [label="other
 (1, 512, 1, 1)" fillcolor=orange]
	140377601946912 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140377601947008 -> 140377601946912
	140377601947008 -> 140377787089280 [dir=none]
	140377787089280 [label="input
 (6, 128, 90, 135)" fillcolor=orange]
	140377601947008 -> 140377786230560 [dir=none]
	140377786230560 [label="weight
 (512, 128, 1, 1)" fillcolor=orange]
	140377601947008 [label="ConvolutionBackward0
------------------------------
bias_sizes_opt:           (0,)
dilation      :         (1, 1)
groups        :              1
input         : [saved tensor]
output_padding:         (0, 0)
padding       :         (0, 0)
stride        :         (1, 1)
transposed    :          False
weight        : [saved tensor]"]
	140377601947104 -> 140377601947008
	140377601947104 -> 140377601889440 [dir=none]
	140377601889440 [label="result
 (6, 128, 90, 135)" fillcolor=orange]
	140377601947104 [label="ReluBackward0
----------------------
result: [saved tensor]"]
	140377601947248 -> 140377601947104
	140377601947248 [label="AddBackward0
------------
alpha: 1"]
	140377601947344 -> 140377601947248
	140377601947344 -> 140377787089200 [dir=none]
	140377787089200 [label="other
 (1, 128, 1, 1)" fillcolor=orange]
	140377601947344 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140377601947440 -> 140377601947344
	140377601947440 -> 140377787089120 [dir=none]
	140377787089120 [label="input
 (6, 128, 90, 135)" fillcolor=orange]
	140377601947440 -> 140377786230080 [dir=none]
	140377786230080 [label="weight
 (128, 128, 3, 3)" fillcolor=orange]
	140377601947440 [label="ConvolutionBackward0
------------------------------
bias_sizes_opt:           (0,)
dilation      :         (1, 1)
groups        :              1
input         : [saved tensor]
output_padding:         (0, 0)
padding       :         (1, 1)
stride        :         (1, 1)
transposed    :          False
weight        : [saved tensor]"]
	140377601947536 -> 140377601947440
	140377601947536 -> 140377601891680 [dir=none]
	140377601891680 [label="result
 (6, 128, 90, 135)" fillcolor=orange]
	140377601947536 [label="ReluBackward0
----------------------
result: [saved tensor]"]
	140377601947680 -> 140377601947536
	140377601947680 [label="AddBackward0
------------
alpha: 1"]
	140377601947776 -> 140377601947680
	140377601947776 -> 140377787089040 [dir=none]
	140377787089040 [label="other
 (1, 128, 1, 1)" fillcolor=orange]
	140377601947776 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140377601947872 -> 140377601947776
	140377601947872 -> 140377787088960 [dir=none]
	140377787088960 [label="input
 (6, 512, 90, 135)" fillcolor=orange]
	140377601947872 -> 140377786229440 [dir=none]
	140377786229440 [label="weight
 (128, 512, 1, 1)" fillcolor=orange]
	140377601947872 [label="ConvolutionBackward0
------------------------------
bias_sizes_opt:           (0,)
dilation      :         (1, 1)
groups        :              1
input         : [saved tensor]
output_padding:         (0, 0)
padding       :         (0, 0)
stride        :         (1, 1)
transposed    :          False
weight        : [saved tensor]"]
	140377601946720 -> 140377601947872
	140377601946720 -> 140377601891440 [dir=none]
	140377601891440 [label="result
 (6, 512, 90, 135)" fillcolor=orange]
	140377601946720 [label="ReluBackward0
----------------------
result: [saved tensor]"]
	140377601948064 -> 140377601946720
	140377601948064 [label="AddBackward0
------------
alpha: 1"]
	140377601948160 -> 140377601948064
	140377601948160 [label="AddBackward0
------------
alpha: 1"]
	140377601948304 -> 140377601948160
	140377601948304 -> 140377787141312 [dir=none]
	140377787141312 [label="other
 (1, 512, 1, 1)" fillcolor=orange]
	140377601948304 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140377601948400 -> 140377601948304
	140377601948400 -> 140377787141952 [dir=none]
	140377787141952 [label="input
 (6, 128, 90, 135)" fillcolor=orange]
	140377601948400 -> 140377786228880 [dir=none]
	140377786228880 [label="weight
 (512, 128, 1, 1)" fillcolor=orange]
	140377601948400 [label="ConvolutionBackward0
------------------------------
bias_sizes_opt:           (0,)
dilation      :         (1, 1)
groups        :              1
input         : [saved tensor]
output_padding:         (0, 0)
padding       :         (0, 0)
stride        :         (1, 1)
transposed    :          False
weight        : [saved tensor]"]
	140377601948496 -> 140377601948400
	140377601948496 -> 140377601958112 [dir=none]
	140377601958112 [label="result
 (6, 128, 90, 135)" fillcolor=orange]
	140377601948496 [label="ReluBackward0
----------------------
result: [saved tensor]"]
	140377601948640 -> 140377601948496
	140377601948640 [label="AddBackward0
------------
alpha: 1"]
	140377601948736 -> 140377601948640
	140377601948736 -> 140377787139472 [dir=none]
	140377787139472 [label="other
 (1, 128, 1, 1)" fillcolor=orange]
	140377601948736 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140377601948832 -> 140377601948736
	140377601948832 -> 140377787141472 [dir=none]
	140377787141472 [label="input
 (6, 128, 90, 135)" fillcolor=orange]
	140377601948832 -> 140377786662480 [dir=none]
	140377786662480 [label="weight
 (128, 128, 3, 3)" fillcolor=orange]
	140377601948832 [label="ConvolutionBackward0
------------------------------
bias_sizes_opt:           (0,)
dilation      :         (1, 1)
groups        :              1
input         : [saved tensor]
output_padding:         (0, 0)
padding       :         (1, 1)
stride        :         (1, 1)
transposed    :          False
weight        : [saved tensor]"]
	140377601948928 -> 140377601948832
	140377601948928 -> 140377601957952 [dir=none]
	140377601957952 [label="result
 (6, 128, 90, 135)" fillcolor=orange]
	140377601948928 [label="ReluBackward0
----------------------
result: [saved tensor]"]
	140377601949072 -> 140377601948928
	140377601949072 [label="AddBackward0
------------
alpha: 1"]
	140377601949168 -> 140377601949072
	140377601949168 -> 140377787141872 [dir=none]
	140377787141872 [label="other
 (1, 128, 1, 1)" fillcolor=orange]
	140377601949168 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140377601949264 -> 140377601949168
	140377601949264 -> 140377787141552 [dir=none]
	140377787141552 [label="input
 (6, 512, 90, 135)" fillcolor=orange]
	140377601949264 -> 140377786661840 [dir=none]
	140377786661840 [label="weight
 (128, 512, 1, 1)" fillcolor=orange]
	140377601949264 [label="ConvolutionBackward0
------------------------------
bias_sizes_opt:           (0,)
dilation      :         (1, 1)
groups        :              1
input         : [saved tensor]
output_padding:         (0, 0)
padding       :         (0, 0)
stride        :         (1, 1)
transposed    :          False
weight        : [saved tensor]"]
	140377601948112 -> 140377601949264
	140377601948112 -> 140377601958352 [dir=none]
	140377601958352 [label="result
 (6, 512, 90, 135)" fillcolor=orange]
	140377601948112 [label="ReluBackward0
----------------------
result: [saved tensor]"]
	140377601949456 -> 140377601948112
	140377601949456 [label="AddBackward0
------------
alpha: 1"]
	140377601949552 -> 140377601949456
	140377601949552 [label="AddBackward0
------------
alpha: 1"]
	140377601949648 -> 140377601949552
	140377601949648 -> 140377787141152 [dir=none]
	140377787141152 [label="other
 (1, 512, 1, 1)" fillcolor=orange]
	140377601949648 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140377601921184 -> 140377601949648
	140377601921184 -> 140377787141392 [dir=none]
	140377787141392 [label="input
 (6, 128, 90, 135)" fillcolor=orange]
	140377601921184 -> 140377786661280 [dir=none]
	140377786661280 [label="weight
 (512, 128, 1, 1)" fillcolor=orange]
	140377601921184 [label="ConvolutionBackward0
------------------------------
bias_sizes_opt:           (0,)
dilation      :         (1, 1)
groups        :              1
input         : [saved tensor]
output_padding:         (0, 0)
padding       :         (0, 0)
stride        :         (1, 1)
transposed    :          False
weight        : [saved tensor]"]
	140377601921280 -> 140377601921184
	140377601921280 -> 140377601958512 [dir=none]
	140377601958512 [label="result
 (6, 128, 90, 135)" fillcolor=orange]
	140377601921280 [label="ReluBackward0
----------------------
result: [saved tensor]"]
	140377601921424 -> 140377601921280
	140377601921424 [label="AddBackward0
------------
alpha: 1"]
	140377601921520 -> 140377601921424
	140377601921520 -> 140377787140992 [dir=none]
	140377787140992 [label="other
 (1, 128, 1, 1)" fillcolor=orange]
	140377601921520 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140377601921616 -> 140377601921520
	140377601921616 -> 140377787141232 [dir=none]
	140377787141232 [label="input
 (6, 128, 180, 270)" fillcolor=orange]
	140377601921616 -> 140377786660800 [dir=none]
	140377786660800 [label="weight
 (128, 128, 3, 3)" fillcolor=orange]
	140377601921616 [label="ConvolutionBackward0
------------------------------
bias_sizes_opt:           (0,)
dilation      :         (1, 1)
groups        :              1
input         : [saved tensor]
output_padding:         (0, 0)
padding       :         (1, 1)
stride        :         (2, 2)
transposed    :          False
weight        : [saved tensor]"]
	140377601921712 -> 140377601921616
	140377601921712 -> 140377601958832 [dir=none]
	140377601958832 [label="result
 (6, 128, 180, 270)" fillcolor=orange]
	140377601921712 [label="ReluBackward0
----------------------
result: [saved tensor]"]
	140377601921856 -> 140377601921712
	140377601921856 [label="AddBackward0
------------
alpha: 1"]
	140377601921952 -> 140377601921856
	140377601921952 -> 140377787140832 [dir=none]
	140377787140832 [label="other
 (1, 128, 1, 1)" fillcolor=orange]
	140377601921952 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140377601922048 -> 140377601921952
	140377601922048 -> 140377787141072 [dir=none]
	140377787141072 [label="input
 (6, 256, 180, 270)" fillcolor=orange]
	140377601922048 -> 140377786660160 [dir=none]
	140377786660160 [label="weight
 (128, 256, 1, 1)" fillcolor=orange]
	140377601922048 [label="ConvolutionBackward0
------------------------------
bias_sizes_opt:           (0,)
dilation      :         (1, 1)
groups        :              1
input         : [saved tensor]
output_padding:         (0, 0)
padding       :         (0, 0)
stride        :         (1, 1)
transposed    :          False
weight        : [saved tensor]"]
	140377601922144 -> 140377601922048
	140377601922144 -> 140377601958912 [dir=none]
	140377601958912 [label="result
 (6, 256, 180, 270)" fillcolor=orange]
	140377601922144 [label="ReluBackward0
----------------------
result: [saved tensor]"]
	140377601922288 -> 140377601922144
	140377601922288 [label="AddBackward0
------------
alpha: 1"]
	140377601922384 -> 140377601922288
	140377601922384 [label="AddBackward0
------------
alpha: 1"]
	140377601922528 -> 140377601922384
	140377601922528 -> 140377787140672 [dir=none]
	140377787140672 [label="other
 (1, 256, 1, 1)" fillcolor=orange]
	140377601922528 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140377601922624 -> 140377601922528
	140377601922624 -> 140377787140912 [dir=none]
	140377787140912 [label="input
 (6, 64, 180, 270)" fillcolor=orange]
	140377601922624 -> 140377786659040 [dir=none]
	140377786659040 [label="weight
 (256, 64, 1, 1)" fillcolor=orange]
	140377601922624 [label="ConvolutionBackward0
------------------------------
bias_sizes_opt:           (0,)
dilation      :         (1, 1)
groups        :              1
input         : [saved tensor]
output_padding:         (0, 0)
padding       :         (0, 0)
stride        :         (1, 1)
transposed    :          False
weight        : [saved tensor]"]
	140377601922720 -> 140377601922624
	140377601922720 -> 140377601958272 [dir=none]
	140377601958272 [label="result
 (6, 64, 180, 270)" fillcolor=orange]
	140377601922720 [label="ReluBackward0
----------------------
result: [saved tensor]"]
	140377601922864 -> 140377601922720
	140377601922864 [label="AddBackward0
------------
alpha: 1"]
	140377601922960 -> 140377601922864
	140377601922960 -> 140377787140512 [dir=none]
	140377787140512 [label="other
 (1, 64, 1, 1)" fillcolor=orange]
	140377601922960 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140377601923056 -> 140377601922960
	140377601923056 -> 140377787140752 [dir=none]
	140377787140752 [label="input
 (6, 64, 180, 270)" fillcolor=orange]
	140377601923056 -> 140377786556064 [dir=none]
	140377786556064 [label="weight
 (64, 64, 3, 3)" fillcolor=orange]
	140377601923056 [label="ConvolutionBackward0
------------------------------
bias_sizes_opt:           (0,)
dilation      :         (1, 1)
groups        :              1
input         : [saved tensor]
output_padding:         (0, 0)
padding       :         (1, 1)
stride        :         (1, 1)
transposed    :          False
weight        : [saved tensor]"]
	140377601923152 -> 140377601923056
	140377601923152 -> 140377601958432 [dir=none]
	140377601958432 [label="result
 (6, 64, 180, 270)" fillcolor=orange]
	140377601923152 [label="ReluBackward0
----------------------
result: [saved tensor]"]
	140377601923296 -> 140377601923152
	140377601923296 [label="AddBackward0
------------
alpha: 1"]
	140377601923392 -> 140377601923296
	140377601923392 -> 140377787140112 [dir=none]
	140377787140112 [label="other
 (1, 64, 1, 1)" fillcolor=orange]
	140377601923392 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140377601923488 -> 140377601923392
	140377601923488 -> 140377787140592 [dir=none]
	140377787140592 [label="input
 (6, 256, 180, 270)" fillcolor=orange]
	140377601923488 -> 140377786555424 [dir=none]
	140377786555424 [label="weight
 (64, 256, 1, 1)" fillcolor=orange]
	140377601923488 [label="ConvolutionBackward0
------------------------------
bias_sizes_opt:           (0,)
dilation      :         (1, 1)
groups        :              1
input         : [saved tensor]
output_padding:         (0, 0)
padding       :         (0, 0)
stride        :         (1, 1)
transposed    :          False
weight        : [saved tensor]"]
	140377601922336 -> 140377601923488
	140377601922336 -> 140377601958592 [dir=none]
	140377601958592 [label="result
 (6, 256, 180, 270)" fillcolor=orange]
	140377601922336 [label="ReluBackward0
----------------------
result: [saved tensor]"]
	140377601923680 -> 140377601922336
	140377601923680 [label="AddBackward0
------------
alpha: 1"]
	140377601923776 -> 140377601923680
	140377601923776 [label="AddBackward0
------------
alpha: 1"]
	140377601923920 -> 140377601923776
	140377601923920 -> 140377787139792 [dir=none]
	140377787139792 [label="other
 (1, 256, 1, 1)" fillcolor=orange]
	140377601923920 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140377601924016 -> 140377601923920
	140377601924016 -> 140377787140432 [dir=none]
	140377787140432 [label="input
 (6, 64, 180, 270)" fillcolor=orange]
	140377601924016 -> 140377786554864 [dir=none]
	140377786554864 [label="weight
 (256, 64, 1, 1)" fillcolor=orange]
	140377601924016 [label="ConvolutionBackward0
------------------------------
bias_sizes_opt:           (0,)
dilation      :         (1, 1)
groups        :              1
input         : [saved tensor]
output_padding:         (0, 0)
padding       :         (0, 0)
stride        :         (1, 1)
transposed    :          False
weight        : [saved tensor]"]
	140377601924112 -> 140377601924016
	140377601924112 -> 140377601958752 [dir=none]
	140377601958752 [label="result
 (6, 64, 180, 270)" fillcolor=orange]
	140377601924112 [label="ReluBackward0
----------------------
result: [saved tensor]"]
	140377601924256 -> 140377601924112
	140377601924256 [label="AddBackward0
------------
alpha: 1"]
	140377601924352 -> 140377601924256
	140377601924352 -> 140377787139552 [dir=none]
	140377787139552 [label="other
 (1, 64, 1, 1)" fillcolor=orange]
	140377601924352 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140377601924448 -> 140377601924352
	140377601924448 -> 140377787139952 [dir=none]
	140377787139952 [label="input
 (6, 64, 180, 270)" fillcolor=orange]
	140377601924448 -> 140377786554384 [dir=none]
	140377786554384 [label="weight
 (64, 64, 3, 3)" fillcolor=orange]
	140377601924448 [label="ConvolutionBackward0
------------------------------
bias_sizes_opt:           (0,)
dilation      :         (1, 1)
groups        :              1
input         : [saved tensor]
output_padding:         (0, 0)
padding       :         (1, 1)
stride        :         (1, 1)
transposed    :          False
weight        : [saved tensor]"]
	140377601924544 -> 140377601924448
	140377601924544 -> 140377601958192 [dir=none]
	140377601958192 [label="result
 (6, 64, 180, 270)" fillcolor=orange]
	140377601924544 [label="ReluBackward0
----------------------
result: [saved tensor]"]
	140377601924688 -> 140377601924544
	140377601924688 [label="AddBackward0
------------
alpha: 1"]
	140377601924784 -> 140377601924688
	140377601924784 -> 140377787140352 [dir=none]
	140377787140352 [label="other
 (1, 64, 1, 1)" fillcolor=orange]
	140377601924784 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140377601924880 -> 140377601924784
	140377601924880 -> 140377787140032 [dir=none]
	140377787140032 [label="input
 (6, 256, 180, 270)" fillcolor=orange]
	140377601924880 -> 140377786553744 [dir=none]
	140377786553744 [label="weight
 (64, 256, 1, 1)" fillcolor=orange]
	140377601924880 [label="ConvolutionBackward0
------------------------------
bias_sizes_opt:           (0,)
dilation      :         (1, 1)
groups        :              1
input         : [saved tensor]
output_padding:         (0, 0)
padding       :         (0, 0)
stride        :         (1, 1)
transposed    :          False
weight        : [saved tensor]"]
	140377601923728 -> 140377601924880
	140377601923728 -> 140377601958992 [dir=none]
	140377601958992 [label="result
 (6, 256, 180, 270)" fillcolor=orange]
	140377601923728 [label="ReluBackward0
----------------------
result: [saved tensor]"]
	140377601925072 -> 140377601923728
	140377601925072 [label="AddBackward0
------------
alpha: 1"]
	140377601924592 -> 140377601925072
	140377601924592 [label="AddBackward0
------------
alpha: 1"]
	140377601900800 -> 140377601924592
	140377601900800 -> 140377787138992 [dir=none]
	140377787138992 [label="other
 (1, 256, 1, 1)" fillcolor=orange]
	140377601900800 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140377601900896 -> 140377601900800
	140377601900896 -> 140377787139872 [dir=none]
	140377787139872 [label="input
 (6, 64, 180, 270)" fillcolor=orange]
	140377601900896 -> 140377786553184 [dir=none]
	140377786553184 [label="weight
 (256, 64, 1, 1)" fillcolor=orange]
	140377601900896 [label="ConvolutionBackward0
------------------------------
bias_sizes_opt:           (0,)
dilation      :         (1, 1)
groups        :              1
input         : [saved tensor]
output_padding:         (0, 0)
padding       :         (0, 0)
stride        :         (1, 1)
transposed    :          False
weight        : [saved tensor]"]
	140377601900992 -> 140377601900896
	140377601900992 -> 140377601959072 [dir=none]
	140377601959072 [label="result
 (6, 64, 180, 270)" fillcolor=orange]
	140377601900992 [label="ReluBackward0
----------------------
result: [saved tensor]"]
	140377601901136 -> 140377601900992
	140377601901136 [label="AddBackward0
------------
alpha: 1"]
	140377601901232 -> 140377601901136
	140377601901232 -> 140377787138832 [dir=none]
	140377787138832 [label="other
 (1, 64, 1, 1)" fillcolor=orange]
	140377601901232 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140377601901328 -> 140377601901232
	140377601901328 -> 140377787139712 [dir=none]
	140377787139712 [label="input
 (6, 64, 180, 270)" fillcolor=orange]
	140377601901328 -> 140377786552704 [dir=none]
	140377786552704 [label="weight
 (64, 64, 3, 3)" fillcolor=orange]
	140377601901328 [label="ConvolutionBackward0
------------------------------
bias_sizes_opt:           (0,)
dilation      :         (1, 1)
groups        :              1
input         : [saved tensor]
output_padding:         (0, 0)
padding       :         (1, 1)
stride        :         (1, 1)
transposed    :          False
weight        : [saved tensor]"]
	140377601901424 -> 140377601901328
	140377601901424 -> 140377601958672 [dir=none]
	140377601958672 [label="result
 (6, 64, 180, 270)" fillcolor=orange]
	140377601901424 [label="ReluBackward0
----------------------
result: [saved tensor]"]
	140377601901568 -> 140377601901424
	140377601901568 [label="AddBackward0
------------
alpha: 1"]
	140377601901664 -> 140377601901568
	140377601901664 -> 140377787139632 [dir=none]
	140377787139632 [label="other
 (1, 64, 1, 1)" fillcolor=orange]
	140377601901664 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140377601901760 -> 140377601901664
	140377601901760 -> 140377787138912 [dir=none]
	140377787138912 [label="input
 (6, 64, 180, 270)" fillcolor=orange]
	140377601901760 -> 140377787084448 [dir=none]
	140377787084448 [label="weight
 (64, 64, 1, 1)" fillcolor=orange]
	140377601901760 [label="ConvolutionBackward0
------------------------------
bias_sizes_opt:           (0,)
dilation      :         (1, 1)
groups        :              1
input         : [saved tensor]
output_padding:         (0, 0)
padding       :         (0, 0)
stride        :         (1, 1)
transposed    :          False
weight        : [saved tensor]"]
	140377601901856 -> 140377601901760
	140377601901856 -> 140377601959232 [dir=none]
	140377601959232 [label="result1
 (6, 64, 180, 270)" fillcolor=orange]
	140377601901856 -> 140377787139392 [dir=none]
	140377787139392 [label="self
 (6, 64, 360, 540)" fillcolor=orange]
	140377601901856 [label="MaxPool2DWithIndicesBackward0
-----------------------------
ceil_mode  :          False
dilation   :         (1, 1)
kernel_size:         (3, 3)
padding    :         (1, 1)
result1    : [saved tensor]
self       : [saved tensor]
stride     :         (2, 2)"]
	140377601902000 -> 140377601901856
	140377601902000 -> 140377601959312 [dir=none]
	140377601959312 [label="result
 (6, 64, 360, 540)" fillcolor=orange]
	140377601902000 [label="ReluBackward0
----------------------
result: [saved tensor]"]
	140377601902096 -> 140377601902000
	140377601902096 [label="AddBackward0
------------
alpha: 1"]
	140377601902192 -> 140377601902096
	140377601902192 -> 140377787139312 [dir=none]
	140377787139312 [label="other
 (1, 64, 1, 1)" fillcolor=orange]
	140377601902192 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140377601902288 -> 140377601902192
	140377601902288 -> 140377785379392 [dir=none]
	140377785379392 [label="input
 (6, 3, 720, 1080)" fillcolor=orange]
	140377601902288 -> 140377787083248 [dir=none]
	140377787083248 [label="weight
 (64, 3, 7, 7)" fillcolor=orange]
	140377601902288 [label="ConvolutionBackward0
------------------------------
bias_sizes_opt:           (0,)
dilation      :         (1, 1)
groups        :              1
input         : [saved tensor]
output_padding:         (0, 0)
padding       :         (3, 3)
stride        :         (2, 2)
transposed    :          False
weight        : [saved tensor]"]
	140377601902384 -> 140377601902288
	140377787083248 [label="model.backbone.conv_encoder.model.conv1.weight
 (64, 3, 7, 7)" fillcolor=lightblue]
	140377787083248 -> 140377601902384
	140377601902384 [label=AccumulateGrad]
	140377601901808 -> 140377601901760
	140377787084448 [label="model.backbone.conv_encoder.model.layer1.0.conv1.weight
 (64, 64, 1, 1)" fillcolor=lightblue]
	140377787084448 -> 140377601901808
	140377601901808 [label=AccumulateGrad]
	140377601901376 -> 140377601901328
	140377786552704 [label="model.backbone.conv_encoder.model.layer1.0.conv2.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	140377786552704 -> 140377601901376
	140377601901376 [label=AccumulateGrad]
	140377601900944 -> 140377601900896
	140377786553184 [label="model.backbone.conv_encoder.model.layer1.0.conv3.weight
 (256, 64, 1, 1)" fillcolor=lightblue]
	140377786553184 -> 140377601900944
	140377601900944 [label=AccumulateGrad]
	140377601900656 -> 140377601925072
	140377601900656 [label="AddBackward0
------------
alpha: 1"]
	140377601900704 -> 140377601900656
	140377601900704 -> 140377787140192 [dir=none]
	140377787140192 [label="other
 (1, 256, 1, 1)" fillcolor=orange]
	140377601900704 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140377601901184 -> 140377601900704
	140377601901184 -> 140377787138912 [dir=none]
	140377787138912 [label="input
 (6, 64, 180, 270)" fillcolor=orange]
	140377601901184 -> 140377787083968 [dir=none]
	140377787083968 [label="weight
 (256, 64, 1, 1)" fillcolor=orange]
	140377601901184 [label="ConvolutionBackward0
------------------------------
bias_sizes_opt:           (0,)
dilation      :         (1, 1)
groups        :              1
input         : [saved tensor]
output_padding:         (0, 0)
padding       :         (0, 0)
stride        :         (1, 1)
transposed    :          False
weight        : [saved tensor]"]
	140377601901856 -> 140377601901184
	140377601901040 -> 140377601901184
	140377787083968 [label="model.backbone.conv_encoder.model.layer1.0.downsample.0.weight
 (256, 64, 1, 1)" fillcolor=lightblue]
	140377787083968 -> 140377601901040
	140377601901040 [label=AccumulateGrad]
	140377601924976 -> 140377601924880
	140377786553744 [label="model.backbone.conv_encoder.model.layer1.1.conv1.weight
 (64, 256, 1, 1)" fillcolor=lightblue]
	140377786553744 -> 140377601924976
	140377601924976 [label=AccumulateGrad]
	140377601924496 -> 140377601924448
	140377786554384 [label="model.backbone.conv_encoder.model.layer1.1.conv2.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	140377786554384 -> 140377601924496
	140377601924496 [label=AccumulateGrad]
	140377601924064 -> 140377601924016
	140377786554864 [label="model.backbone.conv_encoder.model.layer1.1.conv3.weight
 (256, 64, 1, 1)" fillcolor=lightblue]
	140377786554864 -> 140377601924064
	140377601924064 [label=AccumulateGrad]
	140377601923728 -> 140377601923680
	140377601923584 -> 140377601923488
	140377786555424 [label="model.backbone.conv_encoder.model.layer1.2.conv1.weight
 (64, 256, 1, 1)" fillcolor=lightblue]
	140377786555424 -> 140377601923584
	140377601923584 [label=AccumulateGrad]
	140377601923104 -> 140377601923056
	140377786556064 [label="model.backbone.conv_encoder.model.layer1.2.conv2.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	140377786556064 -> 140377601923104
	140377601923104 [label=AccumulateGrad]
	140377601922672 -> 140377601922624
	140377786659040 [label="model.backbone.conv_encoder.model.layer1.2.conv3.weight
 (256, 64, 1, 1)" fillcolor=lightblue]
	140377786659040 -> 140377601922672
	140377601922672 [label=AccumulateGrad]
	140377601922336 -> 140377601922288
	140377601922096 -> 140377601922048
	140377786660160 [label="model.backbone.conv_encoder.model.layer2.0.conv1.weight
 (128, 256, 1, 1)" fillcolor=lightblue]
	140377786660160 -> 140377601922096
	140377601922096 [label=AccumulateGrad]
	140377601921664 -> 140377601921616
	140377786660800 [label="model.backbone.conv_encoder.model.layer2.0.conv2.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	140377786660800 -> 140377601921664
	140377601921664 [label=AccumulateGrad]
	140377601921232 -> 140377601921184
	140377786661280 [label="model.backbone.conv_encoder.model.layer2.0.conv3.weight
 (512, 128, 1, 1)" fillcolor=lightblue]
	140377786661280 -> 140377601921232
	140377601921232 [label=AccumulateGrad]
	140377601949504 -> 140377601949456
	140377601949504 [label="AddBackward0
------------
alpha: 1"]
	140377601949600 -> 140377601949504
	140377601949600 -> 140377787141712 [dir=none]
	140377787141712 [label="other
 (1, 512, 1, 1)" fillcolor=orange]
	140377601949600 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140377601921472 -> 140377601949600
	140377601921472 -> 140377787141072 [dir=none]
	140377787141072 [label="input
 (6, 256, 180, 270)" fillcolor=orange]
	140377601921472 -> 140377786659680 [dir=none]
	140377786659680 [label="weight
 (512, 256, 1, 1)" fillcolor=orange]
	140377601921472 [label="ConvolutionBackward0
------------------------------
bias_sizes_opt:           (0,)
dilation      :         (1, 1)
groups        :              1
input         : [saved tensor]
output_padding:         (0, 0)
padding       :         (0, 0)
stride        :         (2, 2)
transposed    :          False
weight        : [saved tensor]"]
	140377601922144 -> 140377601921472
	140377601921328 -> 140377601921472
	140377786659680 [label="model.backbone.conv_encoder.model.layer2.0.downsample.0.weight
 (512, 256, 1, 1)" fillcolor=lightblue]
	140377786659680 -> 140377601921328
	140377601921328 [label=AccumulateGrad]
	140377601949360 -> 140377601949264
	140377786661840 [label="model.backbone.conv_encoder.model.layer2.1.conv1.weight
 (128, 512, 1, 1)" fillcolor=lightblue]
	140377786661840 -> 140377601949360
	140377601949360 [label=AccumulateGrad]
	140377601948880 -> 140377601948832
	140377786662480 [label="model.backbone.conv_encoder.model.layer2.1.conv2.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	140377786662480 -> 140377601948880
	140377601948880 [label=AccumulateGrad]
	140377601948448 -> 140377601948400
	140377786228880 [label="model.backbone.conv_encoder.model.layer2.1.conv3.weight
 (512, 128, 1, 1)" fillcolor=lightblue]
	140377786228880 -> 140377601948448
	140377601948448 [label=AccumulateGrad]
	140377601948112 -> 140377601948064
	140377601947968 -> 140377601947872
	140377786229440 [label="model.backbone.conv_encoder.model.layer2.2.conv1.weight
 (128, 512, 1, 1)" fillcolor=lightblue]
	140377786229440 -> 140377601947968
	140377601947968 [label=AccumulateGrad]
	140377601947488 -> 140377601947440
	140377786230080 [label="model.backbone.conv_encoder.model.layer2.2.conv2.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	140377786230080 -> 140377601947488
	140377601947488 [label=AccumulateGrad]
	140377601947056 -> 140377601947008
	140377786230560 [label="model.backbone.conv_encoder.model.layer2.2.conv3.weight
 (512, 128, 1, 1)" fillcolor=lightblue]
	140377786230560 -> 140377601947056
	140377601947056 [label=AccumulateGrad]
	140377601946720 -> 140377601946672
	140377601946576 -> 140377601946480
	140377786231120 [label="model.backbone.conv_encoder.model.layer2.3.conv1.weight
 (128, 512, 1, 1)" fillcolor=lightblue]
	140377786231120 -> 140377601946576
	140377601946576 [label=AccumulateGrad]
	140377601946096 -> 140377601946048
	140377786231600 [label="model.backbone.conv_encoder.model.layer2.3.conv2.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	140377786231600 -> 140377601946096
	140377601946096 [label=AccumulateGrad]
	140377601945664 -> 140377786884048
	140377786232080 [label="model.backbone.conv_encoder.model.layer2.3.conv3.weight
 (512, 128, 1, 1)" fillcolor=lightblue]
	140377786232080 -> 140377601945664
	140377601945664 [label=AccumulateGrad]
	140377786883760 -> 140377786883712
	140377786883520 -> 140377786883472
	140377786323408 [label="model.backbone.conv_encoder.model.layer3.0.conv1.weight
 (256, 512, 1, 1)" fillcolor=lightblue]
	140377786323408 -> 140377786883520
	140377786883520 [label=AccumulateGrad]
	140377786883088 -> 140377786883040
	140377786324048 [label="model.backbone.conv_encoder.model.layer3.0.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	140377786324048 -> 140377786883088
	140377786883088 [label=AccumulateGrad]
	140377786882704 -> 140377786880160
	140377786324528 [label="model.backbone.conv_encoder.model.layer3.0.conv3.weight
 (1024, 256, 1, 1)" fillcolor=lightblue]
	140377786324528 -> 140377786882704
	140377786882704 [label=AccumulateGrad]
	140377786880544 -> 140377786880592
	140377786880544 [label="AddBackward0
------------
alpha: 1"]
	140377786880448 -> 140377786880544
	140377786880448 -> 140377787090560 [dir=none]
	140377787090560 [label="other
 (1, 1024, 1, 1)" fillcolor=orange]
	140377786880448 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140377786882896 -> 140377786880448
	140377786882896 -> 140377787089920 [dir=none]
	140377787089920 [label="input
 (6, 512, 90, 135)" fillcolor=orange]
	140377786882896 -> 140377786232720 [dir=none]
	140377786232720 [label="weight
 (1024, 512, 1, 1)" fillcolor=orange]
	140377786882896 [label="ConvolutionBackward0
------------------------------
bias_sizes_opt:           (0,)
dilation      :         (1, 1)
groups        :              1
input         : [saved tensor]
output_padding:         (0, 0)
padding       :         (0, 0)
stride        :         (2, 2)
transposed    :          False
weight        : [saved tensor]"]
	140377786883568 -> 140377786882896
	140377786882752 -> 140377786882896
	140377786232720 [label="model.backbone.conv_encoder.model.layer3.0.downsample.0.weight
 (1024, 512, 1, 1)" fillcolor=lightblue]
	140377786232720 -> 140377786882752
	140377786882752 [label=AccumulateGrad]
	140377786880784 -> 140377786880976
	140377786325088 [label="model.backbone.conv_encoder.model.layer3.1.conv1.weight
 (256, 1024, 1, 1)" fillcolor=lightblue]
	140377786325088 -> 140377786880784
	140377786880784 [label=AccumulateGrad]
	140377786881648 -> 140377786881792
	140377786325728 [label="model.backbone.conv_encoder.model.layer3.1.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	140377786325728 -> 140377786881648
	140377786881648 [label=AccumulateGrad]
	140377786882368 -> 140377786882416
	140377786326208 [label="model.backbone.conv_encoder.model.layer3.1.conv3.weight
 (1024, 256, 1, 1)" fillcolor=lightblue]
	140377786326208 -> 140377786882368
	140377786882368 [label=AccumulateGrad]
	140377787388112 -> 140377787388640
	140377787389120 -> 140377787389600
	140377786326768 [label="model.backbone.conv_encoder.model.layer3.2.conv1.weight
 (256, 1024, 1, 1)" fillcolor=lightblue]
	140377786326768 -> 140377787389120
	140377787389120 [label=AccumulateGrad]
	140377787391568 -> 140377795914528
	140377786425808 [label="model.backbone.conv_encoder.model.layer3.2.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	140377786425808 -> 140377787391568
	140377787391568 [label=AccumulateGrad]
	140377796582656 -> 140377796579728
	140377786426288 [label="model.backbone.conv_encoder.model.layer3.2.conv3.weight
 (1024, 256, 1, 1)" fillcolor=lightblue]
	140377786426288 -> 140377796582656
	140377796582656 [label=AccumulateGrad]
	140377796034368 -> 140377796032592
	140377796032928 -> 140377796033936
	140377786426848 [label="model.backbone.conv_encoder.model.layer3.3.conv1.weight
 (256, 1024, 1, 1)" fillcolor=lightblue]
	140377786426848 -> 140377796032928
	140377796032928 [label=AccumulateGrad]
	140377796034512 -> 140377796030672
	140377786427488 [label="model.backbone.conv_encoder.model.layer3.3.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	140377786427488 -> 140377796034512
	140377796034512 [label=AccumulateGrad]
	140377796030816 -> 140377796031440
	140377786427968 [label="model.backbone.conv_encoder.model.layer3.3.conv3.weight
 (1024, 256, 1, 1)" fillcolor=lightblue]
	140377786427968 -> 140377796030816
	140377796030816 [label=AccumulateGrad]
	140377787375824 -> 140377787376016
	140377787376304 -> 140377787376640
	140377786428528 [label="model.backbone.conv_encoder.model.layer3.4.conv1.weight
 (256, 1024, 1, 1)" fillcolor=lightblue]
	140377786428528 -> 140377787376304
	140377787376304 [label=AccumulateGrad]
	140377787377168 -> 140377787377072
	140377786429168 [label="model.backbone.conv_encoder.model.layer3.4.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	140377786429168 -> 140377787377168
	140377787377168 [label=AccumulateGrad]
	140377787378896 -> 140377787384592
	140377785987376 [label="model.backbone.conv_encoder.model.layer3.4.conv3.weight
 (1024, 256, 1, 1)" fillcolor=lightblue]
	140377785987376 -> 140377787378896
	140377787378896 [label=AccumulateGrad]
	140377787385168 -> 140377787385120
	140377787385264 -> 140377787385456
	140377785987936 [label="model.backbone.conv_encoder.model.layer3.5.conv1.weight
 (256, 1024, 1, 1)" fillcolor=lightblue]
	140377785987936 -> 140377787385264
	140377787385264 [label=AccumulateGrad]
	140377787386128 -> 140377787386272
	140377785988576 [label="model.backbone.conv_encoder.model.layer3.5.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	140377785988576 -> 140377787386128
	140377787386128 [label=AccumulateGrad]
	140377787386944 -> 140377787386992
	140377785989056 [label="model.backbone.conv_encoder.model.layer3.5.conv3.weight
 (1024, 256, 1, 1)" fillcolor=lightblue]
	140377785989056 -> 140377787386944
	140377787386944 [label=AccumulateGrad]
	140377787387568 -> 140377787387712
	140377787409168 -> 140377787408592
	140377785990176 [label="model.backbone.conv_encoder.model.layer4.0.conv1.weight
 (512, 1024, 1, 1)" fillcolor=lightblue]
	140377785990176 -> 140377787409168
	140377787409168 [label=AccumulateGrad]
	140377787409264 -> 140377787409408
	140377785990816 [label="model.backbone.conv_encoder.model.layer4.0.conv2.weight
 (512, 512, 3, 3)" fillcolor=lightblue]
	140377785990816 -> 140377787409264
	140377787409264 [label=AccumulateGrad]
	140377787410080 -> 140377787410128
	140377786097888 [label="model.backbone.conv_encoder.model.layer4.0.conv3.weight
 (2048, 512, 1, 1)" fillcolor=lightblue]
	140377786097888 -> 140377787410080
	140377787410080 [label=AccumulateGrad]
	140377787410704 -> 140377787410848
	140377787410704 [label="AddBackward0
------------
alpha: 1"]
	140377787410512 -> 140377787410704
	140377787410512 -> 140377787064944 [dir=none]
	140377787064944 [label="other
 (1, 2048, 1, 1)" fillcolor=orange]
	140377787410512 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140377787409648 -> 140377787410512
	140377787409648 -> 140377787092880 [dir=none]
	140377787092880 [label="input
 (6, 1024, 45, 68)" fillcolor=orange]
	140377787409648 -> 140377785989696 [dir=none]
	140377785989696 [label="weight
 (2048, 1024, 1, 1)" fillcolor=orange]
	140377787409648 [label="ConvolutionBackward0
------------------------------
bias_sizes_opt:           (0,)
dilation      :         (1, 1)
groups        :              1
input         : [saved tensor]
output_padding:         (0, 0)
padding       :         (0, 0)
stride        :         (2, 2)
transposed    :          False
weight        : [saved tensor]"]
	140377787408544 -> 140377787409648
	140377787409984 -> 140377787409648
	140377785989696 [label="model.backbone.conv_encoder.model.layer4.0.downsample.0.weight
 (2048, 1024, 1, 1)" fillcolor=lightblue]
	140377785989696 -> 140377787409984
	140377787409984 [label=AccumulateGrad]
	140377787410944 -> 140377787411136
	140377786098448 [label="model.backbone.conv_encoder.model.layer4.1.conv1.weight
 (512, 2048, 1, 1)" fillcolor=lightblue]
	140377786098448 -> 140377787410944
	140377787410944 [label=AccumulateGrad]
	140377787411808 -> 140377787411856
	140377786099088 [label="model.backbone.conv_encoder.model.layer4.1.conv2.weight
 (512, 512, 3, 3)" fillcolor=lightblue]
	140377786099088 -> 140377787411808
	140377787411808 [label=AccumulateGrad]
	140377787437264 -> 140377787437312
	140377786099568 [label="model.backbone.conv_encoder.model.layer4.1.conv3.weight
 (2048, 512, 1, 1)" fillcolor=lightblue]
	140377786099568 -> 140377787437264
	140377787437264 [label=AccumulateGrad]
	140377787437888 -> 140377787437936
	140377787438128 -> 140377787438224
	140377786100128 [label="model.backbone.conv_encoder.model.layer4.2.conv1.weight
 (512, 2048, 1, 1)" fillcolor=lightblue]
	140377786100128 -> 140377787438128
	140377787438128 [label=AccumulateGrad]
	140377787438896 -> 140377787439040
	140377786100768 [label="model.backbone.conv_encoder.model.layer4.2.conv2.weight
 (512, 512, 3, 3)" fillcolor=lightblue]
	140377786100768 -> 140377787438896
	140377787438896 [label=AccumulateGrad]
	140377787439712 -> 140377787439760
	140377786101248 [label="model.backbone.conv_encoder.model.layer4.2.conv3.weight
 (2048, 512, 1, 1)" fillcolor=lightblue]
	140377786101248 -> 140377787439712
	140377787439712 [label=AccumulateGrad]
	140377787440240 -> 140377787440384
	140377787440816 -> 140377787440864
	140377786100528 [label="model.input_projection.weight
 (256, 2048, 1, 1)" fillcolor=lightblue]
	140377786100528 -> 140377787440816
	140377787440816 [label=AccumulateGrad]
	140377787441104 -> 140377787440864
	140377786100368 [label="model.input_projection.bias
 (256)" fillcolor=lightblue]
	140377786100368 -> 140377787441104
	140377787441104 [label=AccumulateGrad]
	140377787462176 -> 140377787462320
	140377787462176 -> 140377601959152 [dir=none]
	140377601959152 [label="other
 (6, 782, 256)" fillcolor=orange]
	140377787462176 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140377787461840 -> 140377787462176
	140377787461840 [label="ViewBackward0
-----------------------
self_sizes: (4692, 256)"]
	140377787440432 -> 140377787461840
	140377787440432 -> 140377601959632 [dir=none]
	140377601959632 [label="mat1
 (4692, 256)" fillcolor=orange]
	140377787440432 -> 140377601959472 [dir=none]
	140377601959472 [label="mat2
 (256, 256)" fillcolor=orange]
	140377787440432 [label="AddmmBackward0
----------------------------
alpha       :              1
beta        :              1
mat1        : [saved tensor]
mat1_sizes  :    (4692, 256)
mat1_strides:       (256, 1)
mat2        : [saved tensor]
mat2_sizes  :     (256, 256)
mat2_strides:       (1, 256)"]
	140377787440000 -> 140377787440432
	140377786101008 [label="model.encoder.layers.0.self_attn.out_proj.bias
 (256)" fillcolor=lightblue]
	140377786101008 -> 140377787440000
	140377787440000 [label=AccumulateGrad]
	140377787440576 -> 140377787440432
	140377787440576 [label="ViewBackward0
-------------------------
self_sizes: (6, 782, 256)"]
	140377787440048 -> 140377787440576
	140377787440048 [label="UnsafeViewBackward0
---------------------------
self_sizes: (6, 782, 8, 32)"]
	140377787439520 -> 140377787440048
	140377787439520 [label=CloneBackward0]
	140377787438464 -> 140377787439520
	140377787438464 [label="TransposeBackward0
------------------
dim0: 1
dim1: 2"]
	140377787438176 -> 140377787438464
	140377787438176 [label="ViewBackward0
-------------------------
self_sizes: (48, 782, 32)"]
	140377787438704 -> 140377787438176
	140377787438704 -> 140377787067824 [dir=none]
	140377787067824 [label="mat2
 (48, 782, 32)" fillcolor=orange]
	140377787438704 -> 140377787067984 [dir=none]
	140377787067984 [label="self
 (48, 782, 782)" fillcolor=orange]
	140377787438704 [label="BmmBackward0
--------------------
mat2: [saved tensor]
self: [saved tensor]"]
	140377787437360 -> 140377787438704
	140377787437360 -> 140377601959712 [dir=none]
	140377601959712 [label="result
 (48, 782, 782)" fillcolor=orange]
	140377787437360 [label="SoftmaxBackward0
----------------------------
dim   : 18446744073709551615
result:       [saved tensor]"]
	140377787437696 -> 140377787437360
	140377787437696 [label="ViewBackward0
----------------------------
self_sizes: (6, 8, 782, 782)"]
	140377787412432 -> 140377787437696
	140377787412432 [label="AddBackward0
------------
alpha: 1"]
	140377787411376 -> 140377787412432
	140377787411376 [label="ViewBackward0
--------------------------
self_sizes: (48, 782, 782)"]
	140377787410992 -> 140377787411376
	140377787410992 -> 140377787067424 [dir=none]
	140377787067424 [label="mat2
 (48, 32, 782)" fillcolor=orange]
	140377787410992 -> 140377787067744 [dir=none]
	140377787067744 [label="self
 (48, 782, 32)" fillcolor=orange]
	140377787410992 [label="BmmBackward0
--------------------
mat2: [saved tensor]
self: [saved tensor]"]
	140377787411712 -> 140377787410992
	140377787411712 [label="ViewBackward0
---------------------------
self_sizes: (6, 8, 782, 32)"]
	140377787409840 -> 140377787411712
	140377787409840 [label=CloneBackward0]
	140377787408736 -> 140377787409840
	140377787408736 [label="TransposeBackward0
------------------
dim0: 1
dim1: 2"]
	140377787409120 -> 140377787408736
	140377787409120 [label="ViewBackward0
-------------------------
self_sizes: (6, 782, 256)"]
	140377787410896 -> 140377787409120
	140377787410896 -> 140377601958032 [dir=none]
	140377601958032 [label="other
 ()" fillcolor=orange]
	140377787410896 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140377787387760 -> 140377787410896
	140377787387760 [label="AddBackward0
------------
alpha: 1"]
	140377787387376 -> 140377787387760
	140377787387376 [label="UnsafeViewBackward0
-------------------------
self_sizes: (6, 782, 256)"]
	140377787386704 -> 140377787387376
	140377787386704 -> 140377601959392 [dir=none]
	140377601959392 [label="mat2
 (6, 256, 256)" fillcolor=orange]
	140377787386704 -> 140377601959792 [dir=none]
	140377601959792 [label="self
 (6, 782, 256)" fillcolor=orange]
	140377787386704 [label="BmmBackward0
--------------------
mat2: [saved tensor]
self: [saved tensor]"]
	140377787385600 -> 140377787386704
	140377787385600 [label="ReshapeAliasBackward0
-------------------------
self_sizes: (6, 782, 256)"]
	140377787385408 -> 140377787385600
	140377787385408 [label="ExpandBackward0
-------------------------
self_sizes: (6, 782, 256)"]
	140377787386032 -> 140377787385408
	140377787386032 [label="AddBackward0
------------
alpha: 1"]
	140377787462032 -> 140377787386032
	140377787386848 -> 140377787386704
	140377787386848 [label="ReshapeAliasBackward0
-------------------------
self_sizes: (6, 256, 256)"]
	140377787385216 -> 140377787386848
	140377787385216 [label="ExpandBackward0
----------------------
self_sizes: (256, 256)"]
	140377787385792 -> 140377787385216
	140377787385792 [label=TBackward0]
	140377787377600 -> 140377787385792
	140377786100048 [label="model.encoder.layers.0.self_attn.q_proj.weight
 (256, 256)" fillcolor=lightblue]
	140377786100048 -> 140377787377600
	140377787377600 [label=AccumulateGrad]
	140377787387136 -> 140377787387760
	140377786100288 [label="model.encoder.layers.0.self_attn.q_proj.bias
 (256)" fillcolor=lightblue]
	140377786100288 -> 140377787387136
	140377787387136 [label=AccumulateGrad]
	140377787410272 -> 140377787410992
	140377787410272 [label="TransposeBackward0
------------------
dim0: 1
dim1: 2"]
	140377787410464 -> 140377787410272
	140377787410464 [label="ViewBackward0
---------------------------
self_sizes: (6, 8, 782, 32)"]
	140377787409456 -> 140377787410464
	140377787409456 [label=CloneBackward0]
	140377787383968 -> 140377787409456
	140377787383968 [label="TransposeBackward0
------------------
dim0: 1
dim1: 2"]
	140377787386512 -> 140377787383968
	140377787386512 [label="ViewBackward0
-------------------------
self_sizes: (6, 782, 256)"]
	140377787385984 -> 140377787386512
	140377787385984 [label="AddBackward0
------------
alpha: 1"]
	140377787378032 -> 140377787385984
	140377787378032 [label="UnsafeViewBackward0
-------------------------
self_sizes: (6, 782, 256)"]
	140377787376544 -> 140377787378032
	140377787376544 -> 140377601959952 [dir=none]
	140377601959952 [label="mat2
 (6, 256, 256)" fillcolor=orange]
	140377787376544 -> 140377601960112 [dir=none]
	140377601960112 [label="self
 (6, 782, 256)" fillcolor=orange]
	140377787376544 [label="BmmBackward0
--------------------
mat2: [saved tensor]
self: [saved tensor]"]
	140377787376832 -> 140377787376544
	140377787376832 [label="ReshapeAliasBackward0
-------------------------
self_sizes: (6, 782, 256)"]
	140377787376064 -> 140377787376832
	140377787376064 [label="ExpandBackward0
-------------------------
self_sizes: (6, 782, 256)"]
	140377787386032 -> 140377787376064
	140377787376736 -> 140377787376544
	140377787376736 [label="ReshapeAliasBackward0
-------------------------
self_sizes: (6, 256, 256)"]
	140377787376352 -> 140377787376736
	140377787376352 [label="ExpandBackward0
----------------------
self_sizes: (256, 256)"]
	140377796030960 -> 140377787376352
	140377796030960 [label=TBackward0]
	140377796031680 -> 140377796030960
	140377786100848 [label="model.encoder.layers.0.self_attn.k_proj.weight
 (256, 256)" fillcolor=lightblue]
	140377786100848 -> 140377796031680
	140377796031680 [label=AccumulateGrad]
	140377787378464 -> 140377787385984
	140377786100688 [label="model.encoder.layers.0.self_attn.k_proj.bias
 (256)" fillcolor=lightblue]
	140377786100688 -> 140377787378464
	140377787378464 [label=AccumulateGrad]
	140377787438080 -> 140377787438704
	140377787438080 [label="ViewBackward0
---------------------------
self_sizes: (6, 8, 782, 32)"]
	140377787411184 -> 140377787438080
	140377787411184 [label=CloneBackward0]
	140377787412000 -> 140377787411184
	140377787412000 [label="TransposeBackward0
------------------
dim0: 1
dim1: 2"]
	140377787408928 -> 140377787412000
	140377787408928 [label="ViewBackward0
-------------------------
self_sizes: (6, 782, 256)"]
	140377787386320 -> 140377787408928
	140377787386320 [label="AddBackward0
------------
alpha: 1"]
	140377787387328 -> 140377787386320
	140377787387328 [label="UnsafeViewBackward0
-------------------------
self_sizes: (6, 782, 256)"]
	140377787378416 -> 140377787387328
	140377787378416 -> 140377601960192 [dir=none]
	140377601960192 [label="mat2
 (6, 256, 256)" fillcolor=orange]
	140377787378416 -> 140377601959552 [dir=none]
	140377601959552 [label="self
 (6, 782, 256)" fillcolor=orange]
	140377787378416 [label="BmmBackward0
--------------------
mat2: [saved tensor]
self: [saved tensor]"]
	140377787376976 -> 140377787378416
	140377787376976 [label="ReshapeAliasBackward0
-------------------------
self_sizes: (6, 782, 256)"]
	140377796031584 -> 140377787376976
	140377796031584 [label="ExpandBackward0
-------------------------
self_sizes: (6, 782, 256)"]
	140377787462032 -> 140377796031584
	140377796030912 -> 140377787378416
	140377796030912 [label="ReshapeAliasBackward0
-------------------------
self_sizes: (6, 256, 256)"]
	140377796033840 -> 140377796030912
	140377796033840 [label="ExpandBackward0
----------------------
self_sizes: (256, 256)"]
	140377796034320 -> 140377796033840
	140377796034320 [label=TBackward0]
	140377796032976 -> 140377796034320
	140377786099888 [label="model.encoder.layers.0.self_attn.v_proj.weight
 (256, 256)" fillcolor=lightblue]
	140377786099888 -> 140377796032976
	140377796032976 [label=AccumulateGrad]
	140377787387808 -> 140377787386320
	140377786100448 [label="model.encoder.layers.0.self_attn.v_proj.bias
 (256)" fillcolor=lightblue]
	140377786100448 -> 140377787387808
	140377787387808 [label=AccumulateGrad]
	140377787441056 -> 140377787440432
	140377787441056 [label=TBackward0]
	140377787438272 -> 140377787441056
	140377786100208 [label="model.encoder.layers.0.self_attn.out_proj.weight
 (256, 256)" fillcolor=lightblue]
	140377786100208 -> 140377787438272
	140377787438272 [label=AccumulateGrad]
	140377787462464 -> 140377787462656
	140377786101488 [label="model.encoder.layers.0.self_attn_layer_norm.weight
 (256)" fillcolor=lightblue]
	140377786101488 -> 140377787462464
	140377787462464 [label=AccumulateGrad]
	140377787462512 -> 140377787462656
	140377786101168 [label="model.encoder.layers.0.self_attn_layer_norm.bias
 (256)" fillcolor=lightblue]
	140377786101168 -> 140377787462512
	140377787462512 [label=AccumulateGrad]
	140377787462704 -> 140377787463040
	140377787462704 -> 140377601960352 [dir=none]
	140377601960352 [label="other
 (6, 782, 256)" fillcolor=orange]
	140377787462704 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140377787462224 -> 140377787462704
	140377787462224 [label="ViewBackward0
-----------------------
self_sizes: (4692, 256)"]
	140377787439904 -> 140377787462224
	140377787439904 -> 140377601960432 [dir=none]
	140377601960432 [label="mat1
 (4692, 2048)" fillcolor=orange]
	140377787439904 -> 140377601959872 [dir=none]
	140377601959872 [label="mat2
 (2048, 256)" fillcolor=orange]
	140377787439904 [label="AddmmBackward0
----------------------------
alpha       :              1
beta        :              1
mat1        : [saved tensor]
mat1_sizes  :   (4692, 2048)
mat1_strides:      (2048, 1)
mat2        : [saved tensor]
mat2_sizes  :    (2048, 256)
mat2_strides:      (1, 2048)"]
	140377787437552 -> 140377787439904
	140377786099728 [label="model.encoder.layers.0.fc2.bias
 (256)" fillcolor=lightblue]
	140377786099728 -> 140377787437552
	140377787437552 [label=AccumulateGrad]
	140377787439376 -> 140377787439904
	140377787439376 [label="ViewBackward0
--------------------------
self_sizes: (6, 782, 2048)"]
	140377787439088 -> 140377787439376
	140377787439088 -> 140377601960672 [dir=none]
	140377601960672 [label="result
 (6, 782, 2048)" fillcolor=orange]
	140377787439088 [label="ReluBackward0
----------------------
result: [saved tensor]"]
	140377787412192 -> 140377787439088
	140377787412192 [label="ViewBackward0
------------------------
self_sizes: (4692, 2048)"]
	140377787387856 -> 140377787412192
	140377787387856 -> 140377601960032 [dir=none]
	140377601960032 [label="mat1
 (4692, 256)" fillcolor=orange]
	140377787387856 -> 140377601960752 [dir=none]
	140377601960752 [label="mat2
 (256, 2048)" fillcolor=orange]
	140377787387856 [label="AddmmBackward0
----------------------------
alpha       :              1
beta        :              1
mat1        : [saved tensor]
mat1_sizes  :    (4692, 256)
mat1_strides:       (256, 1)
mat2        : [saved tensor]
mat2_sizes  :    (256, 2048)
mat2_strides:       (1, 256)"]
	140377787379424 -> 140377787387856
	140377786101328 [label="model.encoder.layers.0.fc1.bias
 (2048)" fillcolor=lightblue]
	140377786101328 -> 140377787379424
	140377787379424 [label=AccumulateGrad]
	140377787379376 -> 140377787387856
	140377787379376 [label="ViewBackward0
-------------------------
self_sizes: (6, 782, 256)"]
	140377787462656 -> 140377787379376
	140377796034128 -> 140377787387856
	140377796034128 [label=TBackward0]
	140377796032640 -> 140377796034128
	140377786101408 [label="model.encoder.layers.0.fc1.weight
 (2048, 256)" fillcolor=lightblue]
	140377786101408 -> 140377796032640
	140377796032640 [label=AccumulateGrad]
	140377787440912 -> 140377787439904
	140377787440912 [label=TBackward0]
	140377787412384 -> 140377787440912
	140377786098768 [label="model.encoder.layers.0.fc2.weight
 (256, 2048)" fillcolor=lightblue]
	140377786098768 -> 140377787412384
	140377787412384 [label=AccumulateGrad]
	140377787463088 -> 140377787463184
	140377786099648 [label="model.encoder.layers.0.final_layer_norm.weight
 (256)" fillcolor=lightblue]
	140377786099648 -> 140377787463088
	140377787463088 [label=AccumulateGrad]
	140377787463136 -> 140377787463184
	140377786098128 [label="model.encoder.layers.0.final_layer_norm.bias
 (256)" fillcolor=lightblue]
	140377786098128 -> 140377787463136
	140377787463136 [label=AccumulateGrad]
	140377787463328 -> 140377787463568
	140377787463328 -> 140377601960912 [dir=none]
	140377601960912 [label="other
 (6, 782, 256)" fillcolor=orange]
	140377787463328 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140377787462272 -> 140377787463328
	140377787462272 [label="ViewBackward0
-----------------------
self_sizes: (4692, 256)"]
	140377787411568 -> 140377787462272
	140377787411568 -> 140377601961072 [dir=none]
	140377601961072 [label="mat1
 (4692, 256)" fillcolor=orange]
	140377787411568 -> 140377601960592 [dir=none]
	140377601960592 [label="mat2
 (256, 256)" fillcolor=orange]
	140377787411568 [label="AddmmBackward0
----------------------------
alpha       :              1
beta        :              1
mat1        : [saved tensor]
mat1_sizes  :    (4692, 256)
mat1_strides:       (256, 1)
mat2        : [saved tensor]
mat2_sizes  :     (256, 256)
mat2_strides:       (1, 256)"]
	140377787439184 -> 140377787411568
	140377785989616 [label="model.encoder.layers.1.self_attn.out_proj.bias
 (256)" fillcolor=lightblue]
	140377785989616 -> 140377787439184
	140377787439184 [label=AccumulateGrad]
	140377787438656 -> 140377787411568
	140377787438656 [label="ViewBackward0
-------------------------
self_sizes: (6, 782, 256)"]
	140377796031392 -> 140377787438656
	140377796031392 [label="UnsafeViewBackward0
---------------------------
self_sizes: (6, 782, 8, 32)"]
	140377796579680 -> 140377796031392
	140377796579680 [label=CloneBackward0]
	140377796580208 -> 140377796579680
	140377796580208 [label="TransposeBackward0
------------------
dim0: 1
dim1: 2"]
	140377795914192 -> 140377796580208
	140377795914192 [label="ViewBackward0
-------------------------
self_sizes: (48, 782, 32)"]
	140377795914336 -> 140377795914192
	140377795914336 -> 140377787036272 [dir=none]
	140377787036272 [label="mat2
 (48, 782, 32)" fillcolor=orange]
	140377795914336 -> 140377787036432 [dir=none]
	140377787036432 [label="self
 (48, 782, 782)" fillcolor=orange]
	140377795914336 [label="BmmBackward0
--------------------
mat2: [saved tensor]
self: [saved tensor]"]
	140377787390560 -> 140377795914336
	140377787390560 -> 140377601960272 [dir=none]
	140377601960272 [label="result
 (48, 782, 782)" fillcolor=orange]
	140377787390560 [label="SoftmaxBackward0
----------------------------
dim   : 18446744073709551615
result:       [saved tensor]"]
	140377787391088 -> 140377787390560
	140377787391088 [label="ViewBackward0
----------------------------
self_sizes: (6, 8, 782, 782)"]
	140377787388592 -> 140377787391088
	140377787388592 [label="AddBackward0
------------
alpha: 1"]
	140377786881840 -> 140377787388592
	140377786881840 [label="ViewBackward0
--------------------------
self_sizes: (48, 782, 782)"]
	140377786882128 -> 140377786881840
	140377786882128 -> 140377787035872 [dir=none]
	140377787035872 [label="mat2
 (48, 32, 782)" fillcolor=orange]
	140377786882128 -> 140377787036192 [dir=none]
	140377787036192 [label="self
 (48, 782, 32)" fillcolor=orange]
	140377786882128 [label="BmmBackward0
--------------------
mat2: [saved tensor]
self: [saved tensor]"]
	140377786881120 -> 140377786882128
	140377786881120 [label="ViewBackward0
---------------------------
self_sizes: (6, 8, 782, 32)"]
	140377786880928 -> 140377786881120
	140377786880928 [label=CloneBackward0]
	140377786881552 -> 140377786880928
	140377786881552 [label="TransposeBackward0
------------------
dim0: 1
dim1: 2"]
	140377786882992 -> 140377786881552
	140377786882992 [label="ViewBackward0
-------------------------
self_sizes: (6, 782, 256)"]
	140377786883328 -> 140377786882992
	140377786883328 -> 140377601961312 [dir=none]
	140377601961312 [label="other
 ()" fillcolor=orange]
	140377786883328 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140377786880304 -> 140377786883328
	140377786880304 [label="ViewBackward0
-----------------------
self_sizes: (4692, 256)"]
	140377786883184 -> 140377786880304
	140377786883184 -> 140377601961392 [dir=none]
	140377601961392 [label="mat1
 (4692, 256)" fillcolor=orange]
	140377786883184 -> 140377601961152 [dir=none]
	140377601961152 [label="mat2
 (256, 256)" fillcolor=orange]
	140377786883184 [label="AddmmBackward0
----------------------------
alpha       :              1
beta        :              1
mat1        : [saved tensor]
mat1_sizes  :    (4692, 256)
mat1_strides:       (256, 1)
mat2        : [saved tensor]
mat2_sizes  :     (256, 256)
mat2_strides:       (1, 256)"]
	140377786883616 -> 140377786883184
	140377785989456 [label="model.encoder.layers.1.self_attn.q_proj.bias
 (256)" fillcolor=lightblue]
	140377785989456 -> 140377786883616
	140377786883616 [label=AccumulateGrad]
	140377786883904 -> 140377786883184
	140377786883904 [label="ViewBackward0
-------------------------
self_sizes: (6, 782, 256)"]
	140377786884000 -> 140377786883904
	140377786884000 [label="AddBackward0
------------
alpha: 1"]
	140377787463184 -> 140377786884000
	140377786881312 -> 140377786883184
	140377786881312 [label=TBackward0]
	140377601945904 -> 140377786881312
	140377785989856 [label="model.encoder.layers.1.self_attn.q_proj.weight
 (256, 256)" fillcolor=lightblue]
	140377785989856 -> 140377601945904
	140377601945904 [label=AccumulateGrad]
	140377786882176 -> 140377786882128
	140377786882176 [label="TransposeBackward0
------------------
dim0: 1
dim1: 2"]
	140377786880736 -> 140377786882176
	140377786880736 [label="ViewBackward0
---------------------------
self_sizes: (6, 8, 782, 32)"]
	140377786883424 -> 140377786880736
	140377786883424 [label=CloneBackward0]
	140377786883664 -> 140377786883424
	140377786883664 [label="TransposeBackward0
------------------
dim0: 1
dim1: 2"]
	140377786881504 -> 140377786883664
	140377786881504 [label="ViewBackward0
-------------------------
self_sizes: (6, 782, 256)"]
	140377601945712 -> 140377786881504
	140377601945712 [label="ViewBackward0
-----------------------
self_sizes: (4692, 256)"]
	140377601946336 -> 140377601945712
	140377601946336 -> 140377601960832 [dir=none]
	140377601960832 [label="mat1
 (4692, 256)" fillcolor=orange]
	140377601946336 -> 140377601961232 [dir=none]
	140377601961232 [label="mat2
 (256, 256)" fillcolor=orange]
	140377601946336 [label="AddmmBackward0
----------------------------
alpha       :              1
beta        :              1
mat1        : [saved tensor]
mat1_sizes  :    (4692, 256)
mat1_strides:       (256, 1)
mat2        : [saved tensor]
mat2_sizes  :     (256, 256)
mat2_strides:       (1, 256)"]
	140377601946528 -> 140377601946336
	140377785990256 [label="model.encoder.layers.1.self_attn.k_proj.bias
 (256)" fillcolor=lightblue]
	140377785990256 -> 140377601946528
	140377601946528 [label=AccumulateGrad]
	140377601946240 -> 140377601946336
	140377601946240 [label="ViewBackward0
-------------------------
self_sizes: (6, 782, 256)"]
	140377786884000 -> 140377601946240
	140377601945808 -> 140377601946336
	140377601945808 [label=TBackward0]
	140377601946960 -> 140377601945808
	140377785990336 [label="model.encoder.layers.1.self_attn.k_proj.weight
 (256, 256)" fillcolor=lightblue]
	140377785990336 -> 140377601946960
	140377601946960 [label=AccumulateGrad]
	140377787390176 -> 140377795914336
	140377787390176 [label="ViewBackward0
---------------------------
self_sizes: (6, 8, 782, 32)"]
	140377787389072 -> 140377787390176
	140377787389072 [label=CloneBackward0]
	140377786882464 -> 140377787389072
	140377786882464 [label="TransposeBackward0
------------------
dim0: 1
dim1: 2"]
	140377786882800 -> 140377786882464
	140377786882800 [label="ViewBackward0
-------------------------
self_sizes: (6, 782, 256)"]
	140377786883232 -> 140377786882800
	140377786883232 [label="ViewBackward0
-----------------------
self_sizes: (4692, 256)"]
	140377601946432 -> 140377786883232
	140377601946432 -> 140377601961552 [dir=none]
	140377601961552 [label="mat1
 (4692, 256)" fillcolor=orange]
	140377601946432 -> 140377601961632 [dir=none]
	140377601961632 [label="mat2
 (256, 256)" fillcolor=orange]
	140377601946432 [label="AddmmBackward0
----------------------------
alpha       :              1
beta        :              1
mat1        : [saved tensor]
mat1_sizes  :    (4692, 256)
mat1_strides:       (256, 1)
mat2        : [saved tensor]
mat2_sizes  :     (256, 256)
mat2_strides:       (1, 256)"]
	140377601946624 -> 140377601946432
	140377785989376 [label="model.encoder.layers.1.self_attn.v_proj.bias
 (256)" fillcolor=lightblue]
	140377785989376 -> 140377601946624
	140377601946624 [label=AccumulateGrad]
	140377601946864 -> 140377601946432
	140377601946864 [label="ViewBackward0
-------------------------
self_sizes: (6, 782, 256)"]
	140377787463184 -> 140377601946864
	140377601945760 -> 140377601946432
	140377601945760 [label=TBackward0]
	140377601947296 -> 140377601945760
	140377785987696 [label="model.encoder.layers.1.self_attn.v_proj.weight
 (256, 256)" fillcolor=lightblue]
	140377785987696 -> 140377601947296
	140377601947296 [label=AccumulateGrad]
	140377787440624 -> 140377787411568
	140377787440624 [label=TBackward0]
	140377796582032 -> 140377787440624
	140377785989776 [label="model.encoder.layers.1.self_attn.out_proj.weight
 (256, 256)" fillcolor=lightblue]
	140377785989776 -> 140377796582032
	140377796582032 [label=AccumulateGrad]
	140377787463712 -> 140377787463904
	140377785988656 [label="model.encoder.layers.1.self_attn_layer_norm.weight
 (256)" fillcolor=lightblue]
	140377785988656 -> 140377787463712
	140377787463712 [label=AccumulateGrad]
	140377787463760 -> 140377787463904
	140377786427168 [label="model.encoder.layers.1.self_attn_layer_norm.bias
 (256)" fillcolor=lightblue]
	140377786427168 -> 140377787463760
	140377787463760 [label=AccumulateGrad]
	140377787463952 -> 140377787464192
	140377787463952 -> 140377601961872 [dir=none]
	140377601961872 [label="other
 (6, 782, 256)" fillcolor=orange]
	140377787463952 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140377787462848 -> 140377787463952
	140377787462848 [label="ViewBackward0
-----------------------
self_sizes: (4692, 256)"]
	140377796581648 -> 140377787462848
	140377796581648 -> 140377601960512 [dir=none]
	140377601960512 [label="mat1
 (4692, 2048)" fillcolor=orange]
	140377796581648 -> 140377601961712 [dir=none]
	140377601961712 [label="mat2
 (2048, 256)" fillcolor=orange]
	140377796581648 [label="AddmmBackward0
----------------------------
alpha       :              1
beta        :              1
mat1        : [saved tensor]
mat1_sizes  :   (4692, 2048)
mat1_strides:      (2048, 1)
mat2        : [saved tensor]
mat2_sizes  :    (2048, 256)
mat2_strides:      (1, 2048)"]
	140377795912320 -> 140377796581648
	140377786326928 [label="model.encoder.layers.1.fc2.bias
 (256)" fillcolor=lightblue]
	140377786326928 -> 140377795912320
	140377795912320 [label=AccumulateGrad]
	140377787463520 -> 140377796581648
	140377787463520 [label="ViewBackward0
--------------------------
self_sizes: (6, 782, 2048)"]
	140377787389552 -> 140377787463520
	140377787389552 -> 140377601961472 [dir=none]
	140377601961472 [label="result
 (6, 782, 2048)" fillcolor=orange]
	140377787389552 [label="ReluBackward0
----------------------
result: [saved tensor]"]
	140377787389936 -> 140377787389552
	140377787389936 [label="ViewBackward0
------------------------
self_sizes: (4692, 2048)"]
	140377786882512 -> 140377787389936
	140377786882512 -> 140377601961792 [dir=none]
	140377601961792 [label="mat1
 (4692, 256)" fillcolor=orange]
	140377786882512 -> 140377601960992 [dir=none]
	140377601960992 [label="mat2
 (256, 2048)" fillcolor=orange]
	140377786882512 [label="AddmmBackward0
----------------------------
alpha       :              1
beta        :              1
mat1        : [saved tensor]
mat1_sizes  :    (4692, 256)
mat1_strides:       (256, 1)
mat2        : [saved tensor]
mat2_sizes  :    (256, 2048)
mat2_strides:       (1, 256)"]
	140377601946192 -> 140377786882512
	140377786428048 [label="model.encoder.layers.1.fc1.bias
 (2048)" fillcolor=lightblue]
	140377786428048 -> 140377601946192
	140377601946192 [label=AccumulateGrad]
	140377601947392 -> 140377786882512
	140377601947392 [label="ViewBackward0
-------------------------
self_sizes: (6, 782, 256)"]
	140377787463904 -> 140377601947392
	140377601946000 -> 140377786882512
	140377601946000 [label=TBackward0]
	140377601947824 -> 140377601946000
	140377786428128 [label="model.encoder.layers.1.fc1.weight
 (2048, 256)" fillcolor=lightblue]
	140377786428128 -> 140377601947824
	140377601947824 [label=AccumulateGrad]
	140377796033456 -> 140377796581648
	140377796033456 [label=TBackward0]
	140377786881936 -> 140377796033456
	140377786426048 [label="model.encoder.layers.1.fc2.weight
 (256, 2048)" fillcolor=lightblue]
	140377786426048 -> 140377786881936
	140377786881936 [label=AccumulateGrad]
	140377787464240 -> 140377787464432
	140377786326848 [label="model.encoder.layers.1.final_layer_norm.weight
 (256)" fillcolor=lightblue]
	140377786326848 -> 140377787464240
	140377787464240 [label=AccumulateGrad]
	140377787464384 -> 140377787464432
	140377786324848 [label="model.encoder.layers.1.final_layer_norm.bias
 (256)" fillcolor=lightblue]
	140377786324848 -> 140377787464384
	140377787464384 [label=AccumulateGrad]
	140377787464576 -> 140377787464816
	140377787464576 -> 140377785049232 [dir=none]
	140377785049232 [label="other
 (6, 782, 256)" fillcolor=orange]
	140377787464576 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140377787463376 -> 140377787464576
	140377787463376 [label="ViewBackward0
-----------------------
self_sizes: (4692, 256)"]
	140377796031056 -> 140377787463376
	140377796031056 -> 140377785049552 [dir=none]
	140377785049552 [label="mat1
 (4692, 256)" fillcolor=orange]
	140377796031056 -> 140377785049152 [dir=none]
	140377785049152 [label="mat2
 (256, 256)" fillcolor=orange]
	140377796031056 [label="AddmmBackward0
----------------------------
alpha       :              1
beta        :              1
mat1        : [saved tensor]
mat1_sizes  :    (4692, 256)
mat1_strides:       (256, 1)
mat2        : [saved tensor]
mat2_sizes  :     (256, 256)
mat2_strides:       (1, 256)"]
	140377787391520 -> 140377796031056
	140377786231840 [label="model.encoder.layers.2.self_attn.out_proj.bias
 (256)" fillcolor=lightblue]
	140377786231840 -> 140377787391520
	140377787391520 [label=AccumulateGrad]
	140377786880208 -> 140377796031056
	140377786880208 [label="ViewBackward0
-------------------------
self_sizes: (6, 782, 256)"]
	140377601947152 -> 140377786880208
	140377601947152 [label="UnsafeViewBackward0
---------------------------
self_sizes: (6, 782, 8, 32)"]
	140377601947920 -> 140377601947152
	140377601947920 [label=CloneBackward0]
	140377601947584 -> 140377601947920
	140377601947584 [label="TransposeBackward0
------------------
dim0: 1
dim1: 2"]
	140377601948352 -> 140377601947584
	140377601948352 [label="ViewBackward0
-------------------------
self_sizes: (48, 782, 32)"]
	140377601948784 -> 140377601948352
	140377601948784 -> 140377787037392 [dir=none]
	140377787037392 [label="mat2
 (48, 782, 32)" fillcolor=orange]
	140377601948784 -> 140377787037552 [dir=none]
	140377787037552 [label="self
 (48, 782, 782)" fillcolor=orange]
	140377601948784 [label="BmmBackward0
--------------------
mat2: [saved tensor]
self: [saved tensor]"]
	140377601948592 -> 140377601948784
	140377601948592 -> 140377785049392 [dir=none]
	140377785049392 [label="result
 (48, 782, 782)" fillcolor=orange]
	140377601948592 [label="SoftmaxBackward0
----------------------------
dim   : 18446744073709551615
result:       [saved tensor]"]
	140377601949120 -> 140377601948592
	140377601949120 [label="ViewBackward0
----------------------------
self_sizes: (6, 8, 782, 782)"]
	140377601949312 -> 140377601949120
	140377601949312 [label="AddBackward0
------------
alpha: 1"]
	140377601949408 -> 140377601949312
	140377601949408 [label="ViewBackward0
--------------------------
self_sizes: (48, 782, 782)"]
	140377601948544 -> 140377601949408
	140377601948544 -> 140377787036992 [dir=none]
	140377787036992 [label="mat2
 (48, 32, 782)" fillcolor=orange]
	140377601948544 -> 140377787037312 [dir=none]
	140377787037312 [label="self
 (48, 782, 32)" fillcolor=orange]
	140377601948544 [label="BmmBackward0
--------------------
mat2: [saved tensor]
self: [saved tensor]"]
	140377601921904 -> 140377601948544
	140377601921904 [label="ViewBackward0
---------------------------
self_sizes: (6, 8, 782, 32)"]
	140377601921808 -> 140377601921904
	140377601921808 [label=CloneBackward0]
	140377601922480 -> 140377601921808
	140377601922480 [label="TransposeBackward0
------------------
dim0: 1
dim1: 2"]
	140377601922240 -> 140377601922480
	140377601922240 [label="ViewBackward0
-------------------------
self_sizes: (6, 782, 256)"]
	140377601922432 -> 140377601922240
	140377601922432 -> 140377785049872 [dir=none]
	140377785049872 [label="other
 ()" fillcolor=orange]
	140377601922432 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140377601922912 -> 140377601922432
	140377601922912 [label="ViewBackward0
-----------------------
self_sizes: (4692, 256)"]
	140377601922768 -> 140377601922912
	140377601922768 -> 140377785049952 [dir=none]
	140377785049952 [label="mat1
 (4692, 256)" fillcolor=orange]
	140377601922768 -> 140377785049712 [dir=none]
	140377785049712 [label="mat2
 (256, 256)" fillcolor=orange]
	140377601922768 [label="AddmmBackward0
----------------------------
alpha       :              1
beta        :              1
mat1        : [saved tensor]
mat1_sizes  :    (4692, 256)
mat1_strides:       (256, 1)
mat2        : [saved tensor]
mat2_sizes  :     (256, 256)
mat2_strides:       (1, 256)"]
	140377601923344 -> 140377601922768
	140377786323088 [label="model.encoder.layers.2.self_attn.q_proj.bias
 (256)" fillcolor=lightblue]
	140377786323088 -> 140377601923344
	140377601923344 [label=AccumulateGrad]
	140377601923440 -> 140377601922768
	140377601923440 [label="ViewBackward0
-------------------------
self_sizes: (6, 782, 256)"]
	140377601923536 -> 140377601923440
	140377601923536 [label="AddBackward0
------------
alpha: 1"]
	140377787464432 -> 140377601923536
	140377601922000 -> 140377601922768
	140377601922000 [label=TBackward0]
	140377601923632 -> 140377601922000
	140377786323968 [label="model.encoder.layers.2.self_attn.q_proj.weight
 (256, 256)" fillcolor=lightblue]
	140377786323968 -> 140377601923632
	140377601923632 [label=AccumulateGrad]
	140377601921376 -> 140377601948544
	140377601921376 [label="TransposeBackward0
------------------
dim0: 1
dim1: 2"]
	140377601922192 -> 140377601921376
	140377601922192 [label="ViewBackward0
---------------------------
self_sizes: (6, 8, 782, 32)"]
	140377601923008 -> 140377601922192
	140377601923008 [label=CloneBackward0]
	140377601923248 -> 140377601923008
	140377601923248 [label="TransposeBackward0
------------------
dim0: 1
dim1: 2"]
	140377601923824 -> 140377601923248
	140377601923824 [label="ViewBackward0
-------------------------
self_sizes: (6, 782, 256)"]
	140377601923872 -> 140377601923824
	140377601923872 [label="ViewBackward0
-----------------------
self_sizes: (4692, 256)"]
	140377601924304 -> 140377601923872
	140377601924304 -> 140377785049312 [dir=none]
	140377785049312 [label="mat1
 (4692, 256)" fillcolor=orange]
	140377601924304 -> 140377785049792 [dir=none]
	140377785049792 [label="mat2
 (256, 256)" fillcolor=orange]
	140377601924304 [label="AddmmBackward0
----------------------------
alpha       :              1
beta        :              1
mat1        : [saved tensor]
mat1_sizes  :    (4692, 256)
mat1_strides:       (256, 1)
mat2        : [saved tensor]
mat2_sizes  :     (256, 256)
mat2_strides:       (1, 256)"]
	140377601924160 -> 140377601924304
	140377786325648 [label="model.encoder.layers.2.self_attn.k_proj.bias
 (256)" fillcolor=lightblue]
	140377786325648 -> 140377601924160
	140377601924160 [label=AccumulateGrad]
	140377601924208 -> 140377601924304
	140377601924208 [label="ViewBackward0
-------------------------
self_sizes: (6, 782, 256)"]
	140377601923536 -> 140377601924208
	140377601921136 -> 140377601924304
	140377601921136 [label=TBackward0]
	140377601924928 -> 140377601921136
	140377786325808 [label="model.encoder.layers.2.self_attn.k_proj.weight
 (256, 256)" fillcolor=lightblue]
	140377786325808 -> 140377601924928
	140377601924928 [label=AccumulateGrad]
	140377601948688 -> 140377601948784
	140377601948688 [label="ViewBackward0
---------------------------
self_sizes: (6, 8, 782, 32)"]
	140377601948976 -> 140377601948688
	140377601948976 [label=CloneBackward0]
	140377601949216 -> 140377601948976
	140377601949216 [label="TransposeBackward0
------------------
dim0: 1
dim1: 2"]
	140377601922576 -> 140377601949216
	140377601922576 [label="ViewBackward0
-------------------------
self_sizes: (6, 782, 256)"]
	140377601923968 -> 140377601922576
	140377601923968 [label="ViewBackward0
-----------------------
self_sizes: (4692, 256)"]
	140377601924400 -> 140377601923968
	140377601924400 -> 140377785050112 [dir=none]
	140377785050112 [label="mat1
 (4692, 256)" fillcolor=orange]
	140377601924400 -> 140377785050192 [dir=none]
	140377785050192 [label="mat2
 (256, 256)" fillcolor=orange]
	140377601924400 [label="AddmmBackward0
----------------------------
alpha       :              1
beta        :              1
mat1        : [saved tensor]
mat1_sizes  :    (4692, 256)
mat1_strides:       (256, 1)
mat2        : [saved tensor]
mat2_sizes  :     (256, 256)
mat2_strides:       (1, 256)"]
	140377601924640 -> 140377601924400
	140377786324128 [label="model.encoder.layers.2.self_attn.v_proj.bias
 (256)" fillcolor=lightblue]
	140377786324128 -> 140377601924640
	140377601924640 [label=AccumulateGrad]
	140377601924832 -> 140377601924400
	140377601924832 [label="ViewBackward0
-------------------------
self_sizes: (6, 782, 256)"]
	140377787464432 -> 140377601924832
	140377601921568 -> 140377601924400
	140377601921568 [label=TBackward0]
	140377601924736 -> 140377601921568
	140377786323168 [label="model.encoder.layers.2.self_attn.v_proj.weight
 (256, 256)" fillcolor=lightblue]
	140377786323168 -> 140377601924736
	140377601924736 [label=AccumulateGrad]
	140377787464048 -> 140377796031056
	140377787464048 [label=TBackward0]
	140377601948256 -> 140377787464048
	140377786323008 [label="model.encoder.layers.2.self_attn.out_proj.weight
 (256, 256)" fillcolor=lightblue]
	140377786323008 -> 140377601948256
	140377601948256 [label=AccumulateGrad]
	140377787464864 -> 140377787465056
	140377786232400 [label="model.encoder.layers.2.self_attn_layer_norm.weight
 (256)" fillcolor=lightblue]
	140377786232400 -> 140377787464864
	140377787464864 [label=AccumulateGrad]
	140377787464912 -> 140377787465056
	140377786232480 [label="model.encoder.layers.2.self_attn_layer_norm.bias
 (256)" fillcolor=lightblue]
	140377786232480 -> 140377787464912
	140377787464912 [label=AccumulateGrad]
	140377787465104 -> 140377787465440
	140377787465104 -> 140377785050432 [dir=none]
	140377785050432 [label="other
 (6, 782, 256)" fillcolor=orange]
	140377787465104 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140377787464000 -> 140377787465104
	140377787464000 [label="ViewBackward0
-----------------------
self_sizes: (4692, 256)"]
	140377787462896 -> 140377787464000
	140377787462896 -> 140377785050512 [dir=none]
	140377785050512 [label="mat1
 (4692, 2048)" fillcolor=orange]
	140377787462896 -> 140377785050272 [dir=none]
	140377785050272 [label="mat2
 (2048, 256)" fillcolor=orange]
	140377787462896 [label="AddmmBackward0
----------------------------
alpha       :              1
beta        :              1
mat1        : [saved tensor]
mat1_sizes  :   (4692, 2048)
mat1_strides:      (2048, 1)
mat2        : [saved tensor]
mat2_sizes  :    (2048, 256)
mat2_strides:      (1, 2048)"]
	140377601948208 -> 140377787462896
	140377786661920 [label="model.encoder.layers.2.fc2.bias
 (256)" fillcolor=lightblue]
	140377786661920 -> 140377601948208
	140377601948208 [label=AccumulateGrad]
	140377601947200 -> 140377787462896
	140377601947200 [label="ViewBackward0
--------------------------
self_sizes: (6, 782, 2048)"]
	140377601947632 -> 140377601947200
	140377601947632 -> 140377785050672 [dir=none]
	140377785050672 [label="result
 (6, 782, 2048)" fillcolor=orange]
	140377601947632 [label="ReluBackward0
----------------------
result: [saved tensor]"]
	140377601949024 -> 140377601947632
	140377601949024 [label="ViewBackward0
------------------------
self_sizes: (4692, 2048)"]
	140377601923200 -> 140377601949024
	140377601923200 -> 140377785050032 [dir=none]
	140377785050032 [label="mat1
 (4692, 256)" fillcolor=orange]
	140377601923200 -> 140377785050752 [dir=none]
	140377785050752 [label="mat2
 (256, 2048)" fillcolor=orange]
	140377601923200 [label="AddmmBackward0
----------------------------
alpha       :              1
beta        :              1
mat1        : [saved tensor]
mat1_sizes  :    (4692, 256)
mat1_strides:       (256, 1)
mat2        : [saved tensor]
mat2_sizes  :    (256, 2048)
mat2_strides:       (1, 256)"]
	140377601925024 -> 140377601923200
	140377786231520 [label="model.encoder.layers.2.fc1.bias
 (2048)" fillcolor=lightblue]
	140377786231520 -> 140377601925024
	140377601925024 [label=AccumulateGrad]
	140377601921088 -> 140377601923200
	140377601921088 [label="ViewBackward0
-------------------------
self_sizes: (6, 782, 256)"]
	140377787465056 -> 140377601921088
	140377601900608 -> 140377601923200
	140377601900608 [label=TBackward0]
	140377601901616 -> 140377601900608
	140377786232640 [label="model.encoder.layers.2.fc1.weight
 (2048, 256)" fillcolor=lightblue]
	140377786232640 -> 140377601901616
	140377601901616 [label=AccumulateGrad]
	140377601946816 -> 140377787462896
	140377601946816 [label=TBackward0]
	140377601922816 -> 140377601946816
	140377786659280 [label="model.encoder.layers.2.fc2.weight
 (256, 2048)" fillcolor=lightblue]
	140377786659280 -> 140377601922816
	140377601922816 [label=AccumulateGrad]
	140377787465488 -> 140377787482224
	140377786662720 [label="model.encoder.layers.2.final_layer_norm.weight
 (256)" fillcolor=lightblue]
	140377786662720 -> 140377787465488
	140377787465488 [label=AccumulateGrad]
	140377787465632 -> 140377787482224
	140377786661040 [label="model.encoder.layers.2.final_layer_norm.bias
 (256)" fillcolor=lightblue]
	140377786661040 -> 140377787465632
	140377787465632 [label=AccumulateGrad]
	140377787482176 -> 140377787482416
	140377787482176 -> 140377785050832 [dir=none]
	140377785050832 [label="other
 (6, 782, 256)" fillcolor=orange]
	140377787482176 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140377787464624 -> 140377787482176
	140377787464624 [label="ViewBackward0
-----------------------
self_sizes: (4692, 256)"]
	140377601921760 -> 140377787464624
	140377601921760 -> 140377785050992 [dir=none]
	140377785050992 [label="mat1
 (4692, 256)" fillcolor=orange]
	140377601921760 -> 140377785049632 [dir=none]
	140377785049632 [label="mat2
 (256, 256)" fillcolor=orange]
	140377601921760 [label="AddmmBackward0
----------------------------
alpha       :              1
beta        :              1
mat1        : [saved tensor]
mat1_sizes  :    (4692, 256)
mat1_strides:       (256, 1)
mat2        : [saved tensor]
mat2_sizes  :     (256, 256)
mat2_strides:       (1, 256)"]
	140377601947728 -> 140377601921760
	140377786659600 [label="model.encoder.layers.3.self_attn.out_proj.bias
 (256)" fillcolor=lightblue]
	140377786659600 -> 140377601947728
	140377601947728 [label=AccumulateGrad]
	140377601948016 -> 140377601921760
	140377601948016 [label="ViewBackward0
-------------------------
self_sizes: (6, 782, 256)"]
	140377601901088 -> 140377601948016
	140377601901088 [label="UnsafeViewBackward0
---------------------------
self_sizes: (6, 782, 8, 32)"]
	140377601901520 -> 140377601901088
	140377601901520 [label=CloneBackward0]
	140377601902144 -> 140377601901520
	140377601902144 [label="TransposeBackward0
------------------
dim0: 1
dim1: 2"]
	140377601901952 -> 140377601902144
	140377601901952 [label="ViewBackward0
-------------------------
self_sizes: (48, 782, 32)"]
	140377601902336 -> 140377601901952
	140377601902336 -> 140377787038512 [dir=none]
	140377787038512 [label="mat2
 (48, 782, 32)" fillcolor=orange]
	140377601902336 -> 140377787038672 [dir=none]
	140377787038672 [label="self
 (48, 782, 782)" fillcolor=orange]
	140377601902336 [label="BmmBackward0
--------------------
mat2: [saved tensor]
self: [saved tensor]"]
	140377601902432 -> 140377601902336
	140377601902432 -> 140377785049472 [dir=none]
	140377785049472 [label="result
 (48, 782, 782)" fillcolor=orange]
	140377601902432 [label="SoftmaxBackward0
----------------------------
dim   : 18446744073709551615
result:       [saved tensor]"]
	140377601902576 -> 140377601902432
	140377601902576 [label="ViewBackward0
----------------------------
self_sizes: (6, 8, 782, 782)"]
	140377601902672 -> 140377601902576
	140377601902672 [label="AddBackward0
------------
alpha: 1"]
	140377601902768 -> 140377601902672
	140377601902768 [label="ViewBackward0
--------------------------
self_sizes: (48, 782, 782)"]
	140377601902864 -> 140377601902768
	140377601902864 -> 140377787038112 [dir=none]
	140377787038112 [label="mat2
 (48, 32, 782)" fillcolor=orange]
	140377601902864 -> 140377787038432 [dir=none]
	140377787038432 [label="self
 (48, 782, 32)" fillcolor=orange]
	140377601902864 [label="BmmBackward0
--------------------
mat2: [saved tensor]
self: [saved tensor]"]
	140377601902960 -> 140377601902864
	140377601902960 [label="ViewBackward0
---------------------------
self_sizes: (6, 8, 782, 32)"]
	140377601903104 -> 140377601902960
	140377601903104 [label=CloneBackward0]
	140377601903200 -> 140377601903104
	140377601903200 [label="TransposeBackward0
------------------
dim0: 1
dim1: 2"]
	140377601903296 -> 140377601903200
	140377601903296 [label="ViewBackward0
-------------------------
self_sizes: (6, 782, 256)"]
	140377601903392 -> 140377601903296
	140377601903392 -> 140377785051232 [dir=none]
	140377785051232 [label="other
 ()" fillcolor=orange]
	140377601903392 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140377601903488 -> 140377601903392
	140377601903488 [label="ViewBackward0
-----------------------
self_sizes: (4692, 256)"]
	140377601903584 -> 140377601903488
	140377601903584 -> 140377785051312 [dir=none]
	140377785051312 [label="mat1
 (4692, 256)" fillcolor=orange]
	140377601903584 -> 140377785051072 [dir=none]
	140377785051072 [label="mat2
 (256, 256)" fillcolor=orange]
	140377601903584 [label="AddmmBackward0
----------------------------
alpha       :              1
beta        :              1
mat1        : [saved tensor]
mat1_sizes  :    (4692, 256)
mat1_strides:       (256, 1)
mat2        : [saved tensor]
mat2_sizes  :     (256, 256)
mat2_strides:       (1, 256)"]
	140377601903680 -> 140377601903584
	140377786659440 [label="model.encoder.layers.3.self_attn.q_proj.bias
 (256)" fillcolor=lightblue]
	140377786659440 -> 140377601903680
	140377601903680 [label=AccumulateGrad]
	140377601903632 -> 140377601903584
	140377601903632 [label="ViewBackward0
-------------------------
self_sizes: (6, 782, 256)"]
	140377601903776 -> 140377601903632
	140377601903776 [label="AddBackward0
------------
alpha: 1"]
	140377787482224 -> 140377601903776
	140377601903008 -> 140377601903584
	140377601903008 [label=TBackward0]
	140377601903920 -> 140377601903008
	140377786659840 [label="model.encoder.layers.3.self_attn.q_proj.weight
 (256, 256)" fillcolor=lightblue]
	140377786659840 -> 140377601903920
	140377601903920 [label=AccumulateGrad]
	140377601902912 -> 140377601902864
	140377601902912 [label="TransposeBackward0
------------------
dim0: 1
dim1: 2"]
	140377601903248 -> 140377601902912
	140377601903248 [label="ViewBackward0
---------------------------
self_sizes: (6, 8, 782, 32)"]
	140377601903440 -> 140377601903248
	140377601903440 [label=CloneBackward0]
	140377601903728 -> 140377601903440
	140377601903728 [label="TransposeBackward0
------------------
dim0: 1
dim1: 2"]
	140377601904016 -> 140377601903728
	140377601904016 [label="ViewBackward0
-------------------------
self_sizes: (6, 782, 256)"]
	140377601903824 -> 140377601904016
	140377601903824 [label="ViewBackward0
-----------------------
self_sizes: (4692, 256)"]
	140377601904112 -> 140377601903824
	140377601904112 -> 140377785050352 [dir=none]
	140377785050352 [label="mat1
 (4692, 256)" fillcolor=orange]
	140377601904112 -> 140377785051152 [dir=none]
	140377785051152 [label="mat2
 (256, 256)" fillcolor=orange]
	140377601904112 [label="AddmmBackward0
----------------------------
alpha       :              1
beta        :              1
mat1        : [saved tensor]
mat1_sizes  :    (4692, 256)
mat1_strides:       (256, 1)
mat2        : [saved tensor]
mat2_sizes  :     (256, 256)
mat2_strides:       (1, 256)"]
	140377601904208 -> 140377601904112
	140377786660240 [label="model.encoder.layers.3.self_attn.k_proj.bias
 (256)" fillcolor=lightblue]
	140377786660240 -> 140377601904208
	140377601904208 [label=AccumulateGrad]
	140377601904160 -> 140377601904112
	140377601904160 [label="ViewBackward0
-------------------------
self_sizes: (6, 782, 256)"]
	140377601903776 -> 140377601904160
	140377601903056 -> 140377601904112
	140377601903056 [label=TBackward0]
	140377601904400 -> 140377601903056
	140377786660320 [label="model.encoder.layers.3.self_attn.k_proj.weight
 (256, 256)" fillcolor=lightblue]
	140377786660320 -> 140377601904400
	140377601904400 [label=AccumulateGrad]
	140377601902480 -> 140377601902336
	140377601902480 [label="ViewBackward0
---------------------------
self_sizes: (6, 8, 782, 32)"]
	140377601902720 -> 140377601902480
	140377601902720 [label=CloneBackward0]
	140377601901904 -> 140377601902720
	140377601901904 [label="TransposeBackward0
------------------
dim0: 1
dim1: 2"]
	140377601903344 -> 140377601901904
	140377601903344 [label="ViewBackward0
-------------------------
self_sizes: (6, 782, 256)"]
	140377601903968 -> 140377601903344
	140377601903968 [label="ViewBackward0
-----------------------
self_sizes: (4692, 256)"]
	140377601904064 -> 140377601903968
	140377601904064 -> 140377785051472 [dir=none]
	140377785051472 [label="mat1
 (4692, 256)" fillcolor=orange]
	140377601904064 -> 140377785051552 [dir=none]
	140377785051552 [label="mat2
 (256, 256)" fillcolor=orange]
	140377601904064 [label="AddmmBackward0
----------------------------
alpha       :              1
beta        :              1
mat1        : [saved tensor]
mat1_sizes  :    (4692, 256)
mat1_strides:       (256, 1)
mat2        : [saved tensor]
mat2_sizes  :     (256, 256)
mat2_strides:       (1, 256)"]
	140377601904352 -> 140377601904064
	140377786659360 [label="model.encoder.layers.3.self_attn.v_proj.bias
 (256)" fillcolor=lightblue]
	140377786659360 -> 140377601904352
	140377601904352 [label=AccumulateGrad]
	140377601904256 -> 140377601904064
	140377601904256 [label="ViewBackward0
-------------------------
self_sizes: (6, 782, 256)"]
	140377787482224 -> 140377601904256
	140377601902528 -> 140377601904064
	140377601902528 [label=TBackward0]
	140377601904544 -> 140377601902528
	140377786658960 [label="model.encoder.layers.3.self_attn.v_proj.weight
 (256, 256)" fillcolor=lightblue]
	140377786658960 -> 140377601904544
	140377601904544 [label=AccumulateGrad]
	140377787465296 -> 140377601921760
	140377787465296 [label=TBackward0]
	140377601901472 -> 140377787465296
	140377786659760 [label="model.encoder.layers.3.self_attn.out_proj.weight
 (256, 256)" fillcolor=lightblue]
	140377786659760 -> 140377601901472
	140377601901472 [label=AccumulateGrad]
	140377787482560 -> 140377787482752
	140377786556304 [label="model.encoder.layers.3.self_attn_layer_norm.weight
 (256)" fillcolor=lightblue]
	140377786556304 -> 140377787482560
	140377787482560 [label=AccumulateGrad]
	140377787482608 -> 140377787482752
	140377787083648 [label="model.encoder.layers.3.self_attn_layer_norm.bias
 (256)" fillcolor=lightblue]
	140377787083648 -> 140377787482608
	140377787482608 [label=AccumulateGrad]
	140377787482800 -> 140377787483040
	140377787482800 -> 140377785051792 [dir=none]
	140377785051792 [label="other
 (6, 782, 256)" fillcolor=orange]
	140377787482800 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140377787482368 -> 140377787482800
	140377787482368 [label="ViewBackward0
-----------------------
self_sizes: (4692, 256)"]
	140377787464768 -> 140377787482368
	140377787464768 -> 140377785051872 [dir=none]
	140377785051872 [label="mat1
 (4692, 2048)" fillcolor=orange]
	140377787464768 -> 140377785051632 [dir=none]
	140377785051632 [label="mat2
 (2048, 256)" fillcolor=orange]
	140377787464768 [label="AddmmBackward0
----------------------------
alpha       :              1
beta        :              1
mat1        : [saved tensor]
mat1_sizes  :   (4692, 2048)
mat1_strides:      (2048, 1)
mat2        : [saved tensor]
mat2_sizes  :    (2048, 256)
mat2_strides:      (1, 2048)"]
	140377601902240 -> 140377787464768
	140377787083328 [label="model.encoder.layers.3.fc2.bias
 (256)" fillcolor=lightblue]
	140377787083328 -> 140377601902240
	140377601902240 [label=AccumulateGrad]
	140377601901280 -> 140377787464768
	140377601901280 [label="ViewBackward0
--------------------------
self_sizes: (6, 782, 2048)"]
	140377601900752 -> 140377601901280
	140377601900752 -> 140377785052032 [dir=none]
	140377785052032 [label="result
 (6, 782, 2048)" fillcolor=orange]
	140377601900752 [label="ReluBackward0
----------------------
result: [saved tensor]"]
	140377601903152 -> 140377601900752
	140377601903152 [label="ViewBackward0
------------------------
self_sizes: (4692, 2048)"]
	140377601903872 -> 140377601903152
	140377601903872 -> 140377785051392 [dir=none]
	140377785051392 [label="mat1
 (4692, 256)" fillcolor=orange]
	140377601903872 -> 140377785052112 [dir=none]
	140377785052112 [label="mat2
 (256, 2048)" fillcolor=orange]
	140377601903872 [label="AddmmBackward0
----------------------------
alpha       :              1
beta        :              1
mat1        : [saved tensor]
mat1_sizes  :    (4692, 256)
mat1_strides:       (256, 1)
mat2        : [saved tensor]
mat2_sizes  :    (256, 2048)
mat2_strides:       (1, 256)"]
	140377601904304 -> 140377601903872
	140377787083088 [label="model.encoder.layers.3.fc1.bias
 (2048)" fillcolor=lightblue]
	140377787083088 -> 140377601904304
	140377601904304 [label=AccumulateGrad]
	140377601904496 -> 140377601903872
	140377601904496 [label="ViewBackward0
-------------------------
self_sizes: (6, 782, 256)"]
	140377787482752 -> 140377601904496
	140377601902624 -> 140377601903872
	140377601902624 [label=TBackward0]
	140377601904448 -> 140377601902624
	140377787083488 [label="model.encoder.layers.3.fc1.weight
 (2048, 256)" fillcolor=lightblue]
	140377787083488 -> 140377601904448
	140377601904448 [label=AccumulateGrad]
	140377601900848 -> 140377787464768
	140377601900848 [label=TBackward0]
	140377601903536 -> 140377601900848
	140377787083408 [label="model.encoder.layers.3.fc2.weight
 (256, 2048)" fillcolor=lightblue]
	140377787083408 -> 140377601903536
	140377601903536 [label=AccumulateGrad]
	140377787483088 -> 140377787483280
	140377787084608 [label="model.encoder.layers.3.final_layer_norm.weight
 (256)" fillcolor=lightblue]
	140377787084608 -> 140377787483088
	140377787483088 [label=AccumulateGrad]
	140377787483232 -> 140377787483280
	140377787084528 [label="model.encoder.layers.3.final_layer_norm.bias
 (256)" fillcolor=lightblue]
	140377787084528 -> 140377787483232
	140377787483232 [label=AccumulateGrad]
	140377787483424 -> 140377787483664
	140377787483424 -> 140377785052192 [dir=none]
	140377785052192 [label="other
 (6, 782, 256)" fillcolor=orange]
	140377787483424 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140377787482944 -> 140377787483424
	140377787482944 [label="ViewBackward0
-----------------------
self_sizes: (4692, 256)"]
	140377787465680 -> 140377787482944
	140377787465680 -> 140377785052352 [dir=none]
	140377785052352 [label="mat1
 (4692, 256)" fillcolor=orange]
	140377787465680 -> 140377785050592 [dir=none]
	140377785050592 [label="mat2
 (256, 256)" fillcolor=orange]
	140377787465680 [label="AddmmBackward0
----------------------------
alpha       :              1
beta        :              1
mat1        : [saved tensor]
mat1_sizes  :    (4692, 256)
mat1_strides:       (256, 1)
mat2        : [saved tensor]
mat2_sizes  :     (256, 256)
mat2_strides:       (1, 256)"]
	140377601904592 -> 140377787465680
	140377785801120 [label="model.encoder.layers.4.self_attn.out_proj.bias
 (256)" fillcolor=lightblue]
	140377785801120 -> 140377601904592
	140377601904592 [label=AccumulateGrad]
	140377601902816 -> 140377787465680
	140377601902816 [label="ViewBackward0
-------------------------
self_sizes: (6, 782, 256)"]
	140377784991904 -> 140377601902816
	140377784991904 [label="UnsafeViewBackward0
---------------------------
self_sizes: (6, 782, 8, 32)"]
	140377784992000 -> 140377784991904
	140377784992000 [label=CloneBackward0]
	140377784992096 -> 140377784992000
	140377784992096 [label="TransposeBackward0
------------------
dim0: 1
dim1: 2"]
	140377784992192 -> 140377784992096
	140377784992192 [label="ViewBackward0
-------------------------
self_sizes: (48, 782, 32)"]
	140377784992288 -> 140377784992192
	140377784992288 -> 140377787039632 [dir=none]
	140377787039632 [label="mat2
 (48, 782, 32)" fillcolor=orange]
	140377784992288 -> 140377787003024 [dir=none]
	140377787003024 [label="self
 (48, 782, 782)" fillcolor=orange]
	140377784992288 [label="BmmBackward0
--------------------
mat2: [saved tensor]
self: [saved tensor]"]
	140377784992384 -> 140377784992288
	140377784992384 -> 140377785050912 [dir=none]
	140377785050912 [label="result
 (48, 782, 782)" fillcolor=orange]
	140377784992384 [label="SoftmaxBackward0
----------------------------
dim   : 18446744073709551615
result:       [saved tensor]"]
	140377784992528 -> 140377784992384
	140377784992528 [label="ViewBackward0
----------------------------
self_sizes: (6, 8, 782, 782)"]
	140377784992624 -> 140377784992528
	140377784992624 [label="AddBackward0
------------
alpha: 1"]
	140377784992720 -> 140377784992624
	140377784992720 [label="ViewBackward0
--------------------------
self_sizes: (48, 782, 782)"]
	140377784992816 -> 140377784992720
	140377784992816 -> 140377787039232 [dir=none]
	140377787039232 [label="mat2
 (48, 32, 782)" fillcolor=orange]
	140377784992816 -> 140377787039552 [dir=none]
	140377787039552 [label="self
 (48, 782, 32)" fillcolor=orange]
	140377784992816 [label="BmmBackward0
--------------------
mat2: [saved tensor]
self: [saved tensor]"]
	140377784992912 -> 140377784992816
	140377784992912 [label="ViewBackward0
---------------------------
self_sizes: (6, 8, 782, 32)"]
	140377784993056 -> 140377784992912
	140377784993056 [label=CloneBackward0]
	140377784993152 -> 140377784993056
	140377784993152 [label="TransposeBackward0
------------------
dim0: 1
dim1: 2"]
	140377784993248 -> 140377784993152
	140377784993248 [label="ViewBackward0
-------------------------
self_sizes: (6, 782, 256)"]
	140377784993344 -> 140377784993248
	140377784993344 -> 140377785052592 [dir=none]
	140377785052592 [label="other
 ()" fillcolor=orange]
	140377784993344 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140377784993440 -> 140377784993344
	140377784993440 [label="ViewBackward0
-----------------------
self_sizes: (4692, 256)"]
	140377784993536 -> 140377784993440
	140377784993536 -> 140377785052672 [dir=none]
	140377785052672 [label="mat1
 (4692, 256)" fillcolor=orange]
	140377784993536 -> 140377785052432 [dir=none]
	140377785052432 [label="mat2
 (256, 256)" fillcolor=orange]
	140377784993536 [label="AddmmBackward0
----------------------------
alpha       :              1
beta        :              1
mat1        : [saved tensor]
mat1_sizes  :    (4692, 256)
mat1_strides:       (256, 1)
mat2        : [saved tensor]
mat2_sizes  :     (256, 256)
mat2_strides:       (1, 256)"]
	140377784993632 -> 140377784993536
	140377785802240 [label="model.encoder.layers.4.self_attn.q_proj.bias
 (256)" fillcolor=lightblue]
	140377785802240 -> 140377784993632
	140377784993632 [label=AccumulateGrad]
	140377784993584 -> 140377784993536
	140377784993584 [label="ViewBackward0
-------------------------
self_sizes: (6, 782, 256)"]
	140377784993728 -> 140377784993584
	140377784993728 [label="AddBackward0
------------
alpha: 1"]
	140377787483280 -> 140377784993728
	140377784992960 -> 140377784993536
	140377784992960 [label=TBackward0]
	140377784993872 -> 140377784992960
	140377787083888 [label="model.encoder.layers.4.self_attn.q_proj.weight
 (256, 256)" fillcolor=lightblue]
	140377787083888 -> 140377784993872
	140377784993872 [label=AccumulateGrad]
	140377784992864 -> 140377784992816
	140377784992864 [label="TransposeBackward0
------------------
dim0: 1
dim1: 2"]
	140377784993200 -> 140377784992864
	140377784993200 [label="ViewBackward0
---------------------------
self_sizes: (6, 8, 782, 32)"]
	140377784993392 -> 140377784993200
	140377784993392 [label=CloneBackward0]
	140377784993680 -> 140377784993392
	140377784993680 [label="TransposeBackward0
------------------
dim0: 1
dim1: 2"]
	140377784993968 -> 140377784993680
	140377784993968 [label="ViewBackward0
-------------------------
self_sizes: (6, 782, 256)"]
	140377784993776 -> 140377784993968
	140377784993776 [label="ViewBackward0
-----------------------
self_sizes: (4692, 256)"]
	140377784994064 -> 140377784993776
	140377784994064 -> 140377785051712 [dir=none]
	140377785051712 [label="mat1
 (4692, 256)" fillcolor=orange]
	140377784994064 -> 140377785052512 [dir=none]
	140377785052512 [label="mat2
 (256, 256)" fillcolor=orange]
	140377784994064 [label="AddmmBackward0
----------------------------
alpha       :              1
beta        :              1
mat1        : [saved tensor]
mat1_sizes  :    (4692, 256)
mat1_strides:       (256, 1)
mat2        : [saved tensor]
mat2_sizes  :     (256, 256)
mat2_strides:       (1, 256)"]
	140377784994160 -> 140377784994064
	140377787084128 [label="model.encoder.layers.4.self_attn.k_proj.bias
 (256)" fillcolor=lightblue]
	140377787084128 -> 140377784994160
	140377784994160 [label=AccumulateGrad]
	140377784994112 -> 140377784994064
	140377784994112 [label="ViewBackward0
-------------------------
self_sizes: (6, 782, 256)"]
	140377784993728 -> 140377784994112
	140377784993008 -> 140377784994064
	140377784993008 [label=TBackward0]
	140377784994352 -> 140377784993008
	140377787083568 [label="model.encoder.layers.4.self_attn.k_proj.weight
 (256, 256)" fillcolor=lightblue]
	140377787083568 -> 140377784994352
	140377784994352 [label=AccumulateGrad]
	140377784992336 -> 140377784992288
	140377784992336 [label="ViewBackward0
---------------------------
self_sizes: (6, 8, 782, 32)"]
	140377784992672 -> 140377784992336
	140377784992672 [label=CloneBackward0]
	140377784992432 -> 140377784992672
	140377784992432 [label="TransposeBackward0
------------------
dim0: 1
dim1: 2"]
	140377784993296 -> 140377784992432
	140377784993296 [label="ViewBackward0
-------------------------
self_sizes: (6, 782, 256)"]
	140377784993920 -> 140377784993296
	140377784993920 [label="ViewBackward0
-----------------------
self_sizes: (4692, 256)"]
	140377784994016 -> 140377784993920
	140377784994016 -> 140377785052832 [dir=none]
	140377785052832 [label="mat1
 (4692, 256)" fillcolor=orange]
	140377784994016 -> 140377785052912 [dir=none]
	140377785052912 [label="mat2
 (256, 256)" fillcolor=orange]
	140377784994016 [label="AddmmBackward0
----------------------------
alpha       :              1
beta        :              1
mat1        : [saved tensor]
mat1_sizes  :    (4692, 256)
mat1_strides:       (256, 1)
mat2        : [saved tensor]
mat2_sizes  :     (256, 256)
mat2_strides:       (1, 256)"]
	140377784994304 -> 140377784994016
	140377787084048 [label="model.encoder.layers.4.self_attn.v_proj.bias
 (256)" fillcolor=lightblue]
	140377787084048 -> 140377784994304
	140377784994304 [label=AccumulateGrad]
	140377784994208 -> 140377784994016
	140377784994208 [label="ViewBackward0
-------------------------
self_sizes: (6, 782, 256)"]
	140377787483280 -> 140377784994208
	140377784992480 -> 140377784994016
	140377784992480 [label=TBackward0]
	140377784994496 -> 140377784992480
	140377787083728 [label="model.encoder.layers.4.self_attn.v_proj.weight
 (256, 256)" fillcolor=lightblue]
	140377787083728 -> 140377784994496
	140377784994496 [label=AccumulateGrad]
	140377601902048 -> 140377787465680
	140377601902048 [label=TBackward0]
	140377784992048 -> 140377601902048
	140377785802000 [label="model.encoder.layers.4.self_attn.out_proj.weight
 (256, 256)" fillcolor=lightblue]
	140377785802000 -> 140377784992048
	140377784992048 [label=AccumulateGrad]
	140377787483808 -> 140377787483904
	140377785800560 [label="model.encoder.layers.4.self_attn_layer_norm.weight
 (256)" fillcolor=lightblue]
	140377785800560 -> 140377787483808
	140377787483808 [label=AccumulateGrad]
	140377787483856 -> 140377787483904
	140377785800640 [label="model.encoder.layers.4.self_attn_layer_norm.bias
 (256)" fillcolor=lightblue]
	140377785800640 -> 140377787483856
	140377787483856 [label=AccumulateGrad]
	140377787483952 -> 140377787484288
	140377787483952 -> 140377785052752 [dir=none]
	140377785052752 [label="other
 (6, 782, 256)" fillcolor=orange]
	140377787483952 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140377787482992 -> 140377787483952
	140377787482992 [label="ViewBackward0
-----------------------
self_sizes: (4692, 256)"]
	140377601901712 -> 140377787482992
	140377601901712 -> 140377785053072 [dir=none]
	140377785053072 [label="mat1
 (4692, 2048)" fillcolor=orange]
	140377601901712 -> 140377785052992 [dir=none]
	140377785052992 [label="mat2
 (2048, 256)" fillcolor=orange]
	140377601901712 [label="AddmmBackward0
----------------------------
alpha       :              1
beta        :              1
mat1        : [saved tensor]
mat1_sizes  :   (4692, 2048)
mat1_strides:      (2048, 1)
mat2        : [saved tensor]
mat2_sizes  :    (2048, 256)
mat2_strides:      (1, 2048)"]
	140377787483616 -> 140377601901712
	140377785800320 [label="model.encoder.layers.4.fc2.bias
 (256)" fillcolor=lightblue]
	140377785800320 -> 140377787483616
	140377787483616 [label=AccumulateGrad]
	140377784992240 -> 140377601901712
	140377784992240 [label="ViewBackward0
--------------------------
self_sizes: (6, 782, 2048)"]
	140377784991856 -> 140377784992240
	140377784991856 -> 140377785051952 [dir=none]
	140377785051952 [label="result
 (6, 782, 2048)" fillcolor=orange]
	140377784991856 [label="ReluBackward0
----------------------
result: [saved tensor]"]
	140377784993104 -> 140377784991856
	140377784993104 [label="ViewBackward0
------------------------
self_sizes: (4692, 2048)"]
	140377784993824 -> 140377784993104
	140377784993824 -> 140377785052272 [dir=none]
	140377785052272 [label="mat1
 (4692, 256)" fillcolor=orange]
	140377784993824 -> 140377785020560 [dir=none]
	140377785020560 [label="mat2
 (256, 2048)" fillcolor=orange]
	140377784993824 [label="AddmmBackward0
----------------------------
alpha       :              1
beta        :              1
mat1        : [saved tensor]
mat1_sizes  :    (4692, 256)
mat1_strides:       (256, 1)
mat2        : [saved tensor]
mat2_sizes  :    (256, 2048)
mat2_strides:       (1, 256)"]
	140377784994256 -> 140377784993824
	140377785800480 [label="model.encoder.layers.4.fc1.bias
 (2048)" fillcolor=lightblue]
	140377785800480 -> 140377784994256
	140377784994256 [label=AccumulateGrad]
	140377784994448 -> 140377784993824
	140377784994448 [label="ViewBackward0
-------------------------
self_sizes: (6, 782, 256)"]
	140377787483904 -> 140377784994448
	140377784992576 -> 140377784993824
	140377784992576 [label=TBackward0]
	140377784994640 -> 140377784992576
	140377785800400 [label="model.encoder.layers.4.fc1.weight
 (2048, 256)" fillcolor=lightblue]
	140377785800400 -> 140377784994640
	140377784994640 [label=AccumulateGrad]
	140377784991952 -> 140377601901712
	140377784991952 [label=TBackward0]
	140377784993488 -> 140377784991952
	140377785800240 [label="model.encoder.layers.4.fc2.weight
 (256, 2048)" fillcolor=lightblue]
	140377785800240 -> 140377784993488
	140377784993488 [label=AccumulateGrad]
	140377787484336 -> 140377787484528
	140377785800080 [label="model.encoder.layers.4.final_layer_norm.weight
 (256)" fillcolor=lightblue]
	140377785800080 -> 140377787484336
	140377787484336 [label=AccumulateGrad]
	140377787484480 -> 140377787484528
	140377785800160 [label="model.encoder.layers.4.final_layer_norm.bias
 (256)" fillcolor=lightblue]
	140377785800160 -> 140377787484480
	140377787484480 [label=AccumulateGrad]
	140377787484672 -> 140377787484816
	140377787484672 -> 140377785020800 [dir=none]
	140377785020800 [label="other
 (6, 782, 256)" fillcolor=orange]
	140377787484672 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140377787483472 -> 140377787484672
	140377787483472 [label="ViewBackward0
-----------------------
self_sizes: (4692, 256)"]
	140377787465248 -> 140377787483472
	140377787465248 -> 140377785020960 [dir=none]
	140377785020960 [label="mat1
 (4692, 256)" fillcolor=orange]
	140377787465248 -> 140377785020480 [dir=none]
	140377785020480 [label="mat2
 (256, 256)" fillcolor=orange]
	140377787465248 [label="AddmmBackward0
----------------------------
alpha       :              1
beta        :              1
mat1        : [saved tensor]
mat1_sizes  :    (4692, 256)
mat1_strides:       (256, 1)
mat2        : [saved tensor]
mat2_sizes  :     (256, 256)
mat2_strides:       (1, 256)"]
	140377784994400 -> 140377787465248
	140377785799520 [label="model.encoder.layers.5.self_attn.out_proj.bias
 (256)" fillcolor=lightblue]
	140377785799520 -> 140377784994400
	140377784994400 [label=AccumulateGrad]
	140377784992768 -> 140377787465248
	140377784992768 [label="ViewBackward0
-------------------------
self_sizes: (6, 782, 256)"]
	140377784991808 -> 140377784992768
	140377784991808 [label="UnsafeViewBackward0
---------------------------
self_sizes: (6, 782, 8, 32)"]
	140377784994784 -> 140377784991808
	140377784994784 [label=CloneBackward0]
	140377784994880 -> 140377784994784
	140377784994880 [label="TransposeBackward0
------------------
dim0: 1
dim1: 2"]
	140377784994976 -> 140377784994880
	140377784994976 [label="ViewBackward0
-------------------------
self_sizes: (48, 782, 32)"]
	140377784995072 -> 140377784994976
	140377784995072 -> 140377787003984 [dir=none]
	140377787003984 [label="mat2
 (48, 782, 32)" fillcolor=orange]
	140377784995072 -> 140377787004144 [dir=none]
	140377787004144 [label="self
 (48, 782, 782)" fillcolor=orange]
	140377784995072 [label="BmmBackward0
--------------------
mat2: [saved tensor]
self: [saved tensor]"]
	140377784995168 -> 140377784995072
	140377784995168 -> 140377785020640 [dir=none]
	140377785020640 [label="result
 (48, 782, 782)" fillcolor=orange]
	140377784995168 [label="SoftmaxBackward0
----------------------------
dim   : 18446744073709551615
result:       [saved tensor]"]
	140377784995312 -> 140377784995168
	140377784995312 [label="ViewBackward0
----------------------------
self_sizes: (6, 8, 782, 782)"]
	140377784995408 -> 140377784995312
	140377784995408 [label="AddBackward0
------------
alpha: 1"]
	140377784995504 -> 140377784995408
	140377784995504 [label="ViewBackward0
--------------------------
self_sizes: (48, 782, 782)"]
	140377784995600 -> 140377784995504
	140377784995600 -> 140377787003584 [dir=none]
	140377787003584 [label="mat2
 (48, 32, 782)" fillcolor=orange]
	140377784995600 -> 140377787003904 [dir=none]
	140377787003904 [label="self
 (48, 782, 32)" fillcolor=orange]
	140377784995600 [label="BmmBackward0
--------------------
mat2: [saved tensor]
self: [saved tensor]"]
	140377784995696 -> 140377784995600
	140377784995696 [label="ViewBackward0
---------------------------
self_sizes: (6, 8, 782, 32)"]
	140377784995792 -> 140377784995696
	140377784995792 [label=CloneBackward0]
	140377785045152 -> 140377784995792
	140377785045152 [label="TransposeBackward0
------------------
dim0: 1
dim1: 2"]
	140377785045248 -> 140377785045152
	140377785045248 [label="ViewBackward0
-------------------------
self_sizes: (6, 782, 256)"]
	140377785045344 -> 140377785045248
	140377785045344 -> 140377785021280 [dir=none]
	140377785021280 [label="other
 ()" fillcolor=orange]
	140377785045344 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140377785045440 -> 140377785045344
	140377785045440 [label="ViewBackward0
-----------------------
self_sizes: (4692, 256)"]
	140377785045536 -> 140377785045440
	140377785045536 -> 140377785021360 [dir=none]
	140377785021360 [label="mat1
 (4692, 256)" fillcolor=orange]
	140377785045536 -> 140377785021120 [dir=none]
	140377785021120 [label="mat2
 (256, 256)" fillcolor=orange]
	140377785045536 [label="AddmmBackward0
----------------------------
alpha       :              1
beta        :              1
mat1        : [saved tensor]
mat1_sizes  :    (4692, 256)
mat1_strides:       (256, 1)
mat2        : [saved tensor]
mat2_sizes  :     (256, 256)
mat2_strides:       (1, 256)"]
	140377785045632 -> 140377785045536
	140377785799680 [label="model.encoder.layers.5.self_attn.q_proj.bias
 (256)" fillcolor=lightblue]
	140377785799680 -> 140377785045632
	140377785045632 [label=AccumulateGrad]
	140377785045584 -> 140377785045536
	140377785045584 [label="ViewBackward0
-------------------------
self_sizes: (6, 782, 256)"]
	140377785045728 -> 140377785045584
	140377785045728 [label="AddBackward0
------------
alpha: 1"]
	140377787484528 -> 140377785045728
	140377785045056 -> 140377785045536
	140377785045056 [label=TBackward0]
	140377785045872 -> 140377785045056
	140377785799600 [label="model.encoder.layers.5.self_attn.q_proj.weight
 (256, 256)" fillcolor=lightblue]
	140377785799600 -> 140377785045872
	140377785045872 [label=AccumulateGrad]
	140377784995648 -> 140377784995600
	140377784995648 [label="TransposeBackward0
------------------
dim0: 1
dim1: 2"]
	140377784995744 -> 140377784995648
	140377784995744 [label="ViewBackward0
---------------------------
self_sizes: (6, 8, 782, 32)"]
	140377785045392 -> 140377784995744
	140377785045392 [label=CloneBackward0]
	140377785045680 -> 140377785045392
	140377785045680 [label="TransposeBackward0
------------------
dim0: 1
dim1: 2"]
	140377785045968 -> 140377785045680
	140377785045968 [label="ViewBackward0
-------------------------
self_sizes: (6, 782, 256)"]
	140377785045776 -> 140377785045968
	140377785045776 [label="ViewBackward0
-----------------------
self_sizes: (4692, 256)"]
	140377785046064 -> 140377785045776
	140377785046064 -> 140377785020720 [dir=none]
	140377785020720 [label="mat1
 (4692, 256)" fillcolor=orange]
	140377785046064 -> 140377785021200 [dir=none]
	140377785021200 [label="mat2
 (256, 256)" fillcolor=orange]
	140377785046064 [label="AddmmBackward0
----------------------------
alpha       :              1
beta        :              1
mat1        : [saved tensor]
mat1_sizes  :    (4692, 256)
mat1_strides:       (256, 1)
mat2        : [saved tensor]
mat2_sizes  :     (256, 256)
mat2_strides:       (1, 256)"]
	140377785046160 -> 140377785046064
	140377785800000 [label="model.encoder.layers.5.self_attn.k_proj.bias
 (256)" fillcolor=lightblue]
	140377785800000 -> 140377785046160
	140377785046160 [label=AccumulateGrad]
	140377785046112 -> 140377785046064
	140377785046112 [label="ViewBackward0
-------------------------
self_sizes: (6, 782, 256)"]
	140377785045728 -> 140377785046112
	140377785045104 -> 140377785046064
	140377785045104 [label=TBackward0]
	140377785046352 -> 140377785045104
	140377785799920 [label="model.encoder.layers.5.self_attn.k_proj.weight
 (256, 256)" fillcolor=lightblue]
	140377785799920 -> 140377785046352
	140377785046352 [label=AccumulateGrad]
	140377784995120 -> 140377784995072
	140377784995120 [label="ViewBackward0
---------------------------
self_sizes: (6, 8, 782, 32)"]
	140377784995456 -> 140377784995120
	140377784995456 [label=CloneBackward0]
	140377784995216 -> 140377784995456
	140377784995216 [label="TransposeBackward0
------------------
dim0: 1
dim1: 2"]
	140377784995264 -> 140377784995216
	140377784995264 [label="ViewBackward0
-------------------------
self_sizes: (6, 782, 256)"]
	140377785045920 -> 140377784995264
	140377785045920 [label="ViewBackward0
-----------------------
self_sizes: (4692, 256)"]
	140377785046016 -> 140377785045920
	140377785046016 -> 140377785021520 [dir=none]
	140377785021520 [label="mat1
 (4692, 256)" fillcolor=orange]
	140377785046016 -> 140377785021600 [dir=none]
	140377785021600 [label="mat2
 (256, 256)" fillcolor=orange]
	140377785046016 [label="AddmmBackward0
----------------------------
alpha       :              1
beta        :              1
mat1        : [saved tensor]
mat1_sizes  :    (4692, 256)
mat1_strides:       (256, 1)
mat2        : [saved tensor]
mat2_sizes  :     (256, 256)
mat2_strides:       (1, 256)"]
	140377785046304 -> 140377785046016
	140377785799840 [label="model.encoder.layers.5.self_attn.v_proj.bias
 (256)" fillcolor=lightblue]
	140377785799840 -> 140377785046304
	140377785046304 [label=AccumulateGrad]
	140377785046208 -> 140377785046016
	140377785046208 [label="ViewBackward0
-------------------------
self_sizes: (6, 782, 256)"]
	140377787484528 -> 140377785046208
	140377785045200 -> 140377785046016
	140377785045200 [label=TBackward0]
	140377785046496 -> 140377785045200
	140377785799760 [label="model.encoder.layers.5.self_attn.v_proj.weight
 (256, 256)" fillcolor=lightblue]
	140377785799760 -> 140377785046496
	140377785046496 [label=AccumulateGrad]
	140377784992144 -> 140377787465248
	140377784992144 [label=TBackward0]
	140377784994832 -> 140377784992144
	140377785799440 [label="model.encoder.layers.5.self_attn.out_proj.weight
 (256, 256)" fillcolor=lightblue]
	140377785799440 -> 140377784994832
	140377784994832 [label=AccumulateGrad]
	140377787484960 -> 140377787485152
	140377785799280 [label="model.encoder.layers.5.self_attn_layer_norm.weight
 (256)" fillcolor=lightblue]
	140377785799280 -> 140377787484960
	140377787484960 [label=AccumulateGrad]
	140377787485008 -> 140377787485152
	140377785799360 [label="model.encoder.layers.5.self_attn_layer_norm.bias
 (256)" fillcolor=lightblue]
	140377785799360 -> 140377787485008
	140377787485008 [label=AccumulateGrad]
	140377787485200 -> 140377787485536
	140377787485200 -> 140377785021840 [dir=none]
	140377785021840 [label="other
 (6, 782, 256)" fillcolor=orange]
	140377787485200 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140377787484096 -> 140377787485200
	140377787484096 [label="ViewBackward0
-----------------------
self_sizes: (4692, 256)"]
	140377787484144 -> 140377787484096
	140377787484144 -> 140377785021920 [dir=none]
	140377785021920 [label="mat1
 (4692, 2048)" fillcolor=orange]
	140377787484144 -> 140377785021680 [dir=none]
	140377785021680 [label="mat2
 (2048, 256)" fillcolor=orange]
	140377787484144 [label="AddmmBackward0
----------------------------
alpha       :              1
beta        :              1
mat1        : [saved tensor]
mat1_sizes  :   (4692, 2048)
mat1_strides:      (2048, 1)
mat2        : [saved tensor]
mat2_sizes  :    (2048, 256)
mat2_strides:      (1, 2048)"]
	140377784995024 -> 140377787484144
	140377785799040 [label="model.encoder.layers.5.fc2.bias
 (256)" fillcolor=lightblue]
	140377785799040 -> 140377784995024
	140377784995024 [label=AccumulateGrad]
	140377784994544 -> 140377787484144
	140377784994544 [label="ViewBackward0
--------------------------
self_sizes: (6, 782, 2048)"]
	140377784994736 -> 140377784994544
	140377784994736 -> 140377785022080 [dir=none]
	140377785022080 [label="result
 (6, 782, 2048)" fillcolor=orange]
	140377784994736 [label="ReluBackward0
----------------------
result: [saved tensor]"]
	140377784995552 -> 140377784994736
	140377784995552 [label="ViewBackward0
------------------------
self_sizes: (4692, 2048)"]
	140377785045824 -> 140377784995552
	140377785045824 -> 140377785021440 [dir=none]
	140377785021440 [label="mat1
 (4692, 256)" fillcolor=orange]
	140377785045824 -> 140377785022160 [dir=none]
	140377785022160 [label="mat2
 (256, 2048)" fillcolor=orange]
	140377785045824 [label="AddmmBackward0
----------------------------
alpha       :              1
beta        :              1
mat1        : [saved tensor]
mat1_sizes  :    (4692, 256)
mat1_strides:       (256, 1)
mat2        : [saved tensor]
mat2_sizes  :    (256, 2048)
mat2_strides:       (1, 256)"]
	140377785046256 -> 140377785045824
	140377785799200 [label="model.encoder.layers.5.fc1.bias
 (2048)" fillcolor=lightblue]
	140377785799200 -> 140377785046256
	140377785046256 [label=AccumulateGrad]
	140377785046448 -> 140377785045824
	140377785046448 [label="ViewBackward0
-------------------------
self_sizes: (6, 782, 256)"]
	140377787485152 -> 140377785046448
	140377785045296 -> 140377785045824
	140377785045296 [label=TBackward0]
	140377785046640 -> 140377785045296
	140377785799120 [label="model.encoder.layers.5.fc1.weight
 (2048, 256)" fillcolor=lightblue]
	140377785799120 -> 140377785046640
	140377785046640 [label=AccumulateGrad]
	140377784994592 -> 140377787484144
	140377784994592 [label=TBackward0]
	140377784995360 -> 140377784994592
	140377785798960 [label="model.encoder.layers.5.fc2.weight
 (256, 2048)" fillcolor=lightblue]
	140377785798960 -> 140377784995360
	140377784995360 [label=AccumulateGrad]
	140377787485584 -> 140377787485632
	140377785798800 [label="model.encoder.layers.5.final_layer_norm.weight
 (256)" fillcolor=lightblue]
	140377785798800 -> 140377787485584
	140377787485584 [label=AccumulateGrad]
	140377787485824 -> 140377787485632
	140377785798880 [label="model.encoder.layers.5.final_layer_norm.bias
 (256)" fillcolor=lightblue]
	140377785798880 -> 140377787485824
	140377787485824 [label=AccumulateGrad]
	140377787512864 -> 140377787511040
	140377787512864 [label=TBackward0]
	140377787485392 -> 140377787512864
	140377785687152 [label="model.decoder.layers.0.encoder_attn.k_proj.weight
 (256, 256)" fillcolor=lightblue]
	140377785687152 -> 140377787485392
	140377787485392 [label=AccumulateGrad]
	140377787513872 -> 140377787514016
	140377787513872 [label="ViewBackward0
---------------------------
self_sizes: (6, 8, 782, 32)"]
	140377787513392 -> 140377787513872
	140377787513392 [label=CloneBackward0]
	140377787513824 -> 140377787513392
	140377787513824 [label="TransposeBackward0
------------------
dim0: 1
dim1: 2"]
	140377787512288 -> 140377787513824
	140377787512288 [label="ViewBackward0
-------------------------
self_sizes: (6, 782, 256)"]
	140377787511184 -> 140377787512288
	140377787511184 [label="ViewBackward0
-----------------------
self_sizes: (4692, 256)"]
	140377787511088 -> 140377787511184
	140377787511088 -> 140377785021040 [dir=none]
	140377785021040 [label="mat1
 (4692, 256)" fillcolor=orange]
	140377787511088 -> 140377785022240 [dir=none]
	140377785022240 [label="mat2
 (256, 256)" fillcolor=orange]
	140377787511088 [label="AddmmBackward0
----------------------------
alpha       :              1
beta        :              1
mat1        : [saved tensor]
mat1_sizes  :    (4692, 256)
mat1_strides:       (256, 1)
mat2        : [saved tensor]
mat2_sizes  :     (256, 256)
mat2_strides:       (1, 256)"]
	140377787513776 -> 140377787511088
	140377785686912 [label="model.decoder.layers.0.encoder_attn.v_proj.bias
 (256)" fillcolor=lightblue]
	140377785686912 -> 140377787513776
	140377787513776 [label=AccumulateGrad]
	140377787485680 -> 140377787511088
	140377787485680 [label="ViewBackward0
-------------------------
self_sizes: (6, 782, 256)"]
	140377787485632 -> 140377787485680
	140377787486064 -> 140377787511088
	140377787486064 [label=TBackward0]
	140377787484768 -> 140377787486064
	140377785686992 [label="model.decoder.layers.0.encoder_attn.v_proj.weight
 (256, 256)" fillcolor=lightblue]
	140377785686992 -> 140377787484768
	140377787484768 [label=AccumulateGrad]
	140377787098736 -> 140377787099600
	140377787098736 [label=TBackward0]
	140377787126016 -> 140377787098736
	140377785686672 [label="model.decoder.layers.0.encoder_attn.out_proj.weight
 (256, 256)" fillcolor=lightblue]
	140377785686672 -> 140377787126016
	140377787126016 [label=AccumulateGrad]
	140377787100512 -> 140377787097728
	140377785686512 [label="model.decoder.layers.0.encoder_attn_layer_norm.weight
 (256)" fillcolor=lightblue]
	140377785686512 -> 140377787100512
	140377787100512 [label=AccumulateGrad]
	140377787097584 -> 140377787097728
	140377785686432 [label="model.decoder.layers.0.encoder_attn_layer_norm.bias
 (256)" fillcolor=lightblue]
	140377785686432 -> 140377787097584
	140377787097584 [label=AccumulateGrad]
	140377787097776 -> 140377787100080
	140377787097776 -> 140377785022000 [dir=none]
	140377785022000 [label="other
 (6, 100, 256)" fillcolor=orange]
	140377787097776 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140377787098832 -> 140377787097776
	140377787098832 [label="ViewBackward0
----------------------
self_sizes: (600, 256)"]
	140377787099696 -> 140377787098832
	140377787099696 -> 140377785022560 [dir=none]
	140377785022560 [label="mat1
 (600, 2048)" fillcolor=orange]
	140377787099696 -> 140377785022320 [dir=none]
	140377785022320 [label="mat2
 (2048, 256)" fillcolor=orange]
	140377787099696 [label="AddmmBackward0
----------------------------
alpha       :              1
beta        :              1
mat1        : [saved tensor]
mat1_sizes  :    (600, 2048)
mat1_strides:      (2048, 1)
mat2        : [saved tensor]
mat2_sizes  :    (2048, 256)
mat2_strides:      (1, 2048)"]
	140377787157664 -> 140377787099696
	140377785686112 [label="model.decoder.layers.0.fc2.bias
 (256)" fillcolor=lightblue]
	140377785686112 -> 140377787157664
	140377787157664 [label=AccumulateGrad]
	140377787125920 -> 140377787099696
	140377787125920 [label="ViewBackward0
--------------------------
self_sizes: (6, 100, 2048)"]
	140377787514256 -> 140377787125920
	140377787514256 -> 140377785022720 [dir=none]
	140377785022720 [label="result
 (6, 100, 2048)" fillcolor=orange]
	140377787514256 [label="ReluBackward0
----------------------
result: [saved tensor]"]
	140377787512672 -> 140377787514256
	140377787512672 [label="ViewBackward0
-----------------------
self_sizes: (600, 2048)"]
	140377787511280 -> 140377787512672
	140377787511280 -> 140377785022480 [dir=none]
	140377785022480 [label="mat1
 (600, 256)" fillcolor=orange]
	140377787511280 -> 140377785022800 [dir=none]
	140377785022800 [label="mat2
 (256, 2048)" fillcolor=orange]
	140377787511280 [label="AddmmBackward0
----------------------------
alpha       :              1
beta        :              1
mat1        : [saved tensor]
mat1_sizes  :     (600, 256)
mat1_strides:       (256, 1)
mat2        : [saved tensor]
mat2_sizes  :    (256, 2048)
mat2_strides:       (1, 256)"]
	140377784994928 -> 140377787511280
	140377785686272 [label="model.decoder.layers.0.fc1.bias
 (2048)" fillcolor=lightblue]
	140377785686272 -> 140377784994928
	140377784994928 [label=AccumulateGrad]
	140377784994688 -> 140377787511280
	140377784994688 [label="ViewBackward0
-------------------------
self_sizes: (6, 100, 256)"]
	140377787097728 -> 140377784994688
	140377787513584 -> 140377787511280
	140377787513584 [label=TBackward0]
	140377787485344 -> 140377787513584
	140377785686352 [label="model.decoder.layers.0.fc1.weight
 (2048, 256)" fillcolor=lightblue]
	140377785686352 -> 140377787485344
	140377787485344 [label=AccumulateGrad]
	140377787098160 -> 140377787099696
	140377787098160 [label=TBackward0]
	140377787512000 -> 140377787098160
	140377785686192 [label="model.decoder.layers.0.fc2.weight
 (256, 2048)" fillcolor=lightblue]
	140377785686192 -> 140377787512000
	140377787512000 [label=AccumulateGrad]
	140377787097824 -> 140377787097392
	140377785686032 [label="model.decoder.layers.0.final_layer_norm.weight
 (256)" fillcolor=lightblue]
	140377785686032 -> 140377787097824
	140377787097824 [label=AccumulateGrad]
	140377787097344 -> 140377787097392
	140377785685952 [label="model.decoder.layers.0.final_layer_norm.bias
 (256)" fillcolor=lightblue]
	140377785685952 -> 140377787097344
	140377787097344 [label=AccumulateGrad]
	140377787097872 -> 140377787097200
	140377787097872 -> 140377785022880 [dir=none]
	140377785022880 [label="other
 (6, 100, 256)" fillcolor=orange]
	140377787097872 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140377787098496 -> 140377787097872
	140377787098496 [label="ViewBackward0
----------------------
self_sizes: (600, 256)"]
	140377787100032 -> 140377787098496
	140377787100032 -> 140377785023040 [dir=none]
	140377785023040 [label="mat1
 (600, 256)" fillcolor=orange]
	140377787100032 -> 140377785021760 [dir=none]
	140377785021760 [label="mat2
 (256, 256)" fillcolor=orange]
	140377787100032 [label="AddmmBackward0
----------------------------
alpha       :              1
beta        :              1
mat1        : [saved tensor]
mat1_sizes  :     (600, 256)
mat1_strides:       (256, 1)
mat2        : [saved tensor]
mat2_sizes  :     (256, 256)
mat2_strides:       (1, 256)"]
	140377787484720 -> 140377787100032
	140377785685312 [label="model.decoder.layers.1.self_attn.out_proj.bias
 (256)" fillcolor=lightblue]
	140377785685312 -> 140377787484720
	140377787484720 [label=AccumulateGrad]
	140377787485872 -> 140377787100032
	140377787485872 [label="ViewBackward0
-------------------------
self_sizes: (6, 100, 256)"]
	140377787514448 -> 140377787485872
	140377787514448 [label="UnsafeViewBackward0
---------------------------
self_sizes: (6, 100, 8, 32)"]
	140377785046736 -> 140377787514448
	140377785046736 [label=CloneBackward0]
	140377785046544 -> 140377785046736
	140377785046544 [label="TransposeBackward0
------------------
dim0: 1
dim1: 2"]
	140377785046832 -> 140377785046544
	140377785046832 [label="ViewBackward0
-------------------------
self_sizes: (48, 100, 32)"]
	140377785046928 -> 140377785046832
	140377785046928 -> 140377786819104 [dir=none]
	140377786819104 [label="mat2
 (48, 100, 32)" fillcolor=orange]
	140377785046928 -> 140377786819264 [dir=none]
	140377786819264 [label="self
 (48, 100, 100)" fillcolor=orange]
	140377785046928 [label="BmmBackward0
--------------------
mat2: [saved tensor]
self: [saved tensor]"]
	140377785047024 -> 140377785046928
	140377785047024 -> 140377785020880 [dir=none]
	140377785020880 [label="result
 (48, 100, 100)" fillcolor=orange]
	140377785047024 [label="SoftmaxBackward0
----------------------------
dim   : 18446744073709551615
result:       [saved tensor]"]
	140377785047168 -> 140377785047024
	140377785047168 -> 140377786818704 [dir=none]
	140377786818704 [label="mat2
 (48, 32, 100)" fillcolor=orange]
	140377785047168 -> 140377786819024 [dir=none]
	140377786819024 [label="self
 (48, 100, 32)" fillcolor=orange]
	140377785047168 [label="BmmBackward0
--------------------
mat2: [saved tensor]
self: [saved tensor]"]
	140377785047264 -> 140377785047168
	140377785047264 [label="ViewBackward0
---------------------------
self_sizes: (6, 8, 100, 32)"]
	140377785047408 -> 140377785047264
	140377785047408 [label=CloneBackward0]
	140377785047504 -> 140377785047408
	140377785047504 [label="TransposeBackward0
------------------
dim0: 1
dim1: 2"]
	140377785047600 -> 140377785047504
	140377785047600 [label="ViewBackward0
-------------------------
self_sizes: (6, 100, 256)"]
	140377785047696 -> 140377785047600
	140377785047696 -> 140377785023200 [dir=none]
	140377785023200 [label="other
 ()" fillcolor=orange]
	140377785047696 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140377785047792 -> 140377785047696
	140377785047792 [label="ViewBackward0
----------------------
self_sizes: (600, 256)"]
	140377785047888 -> 140377785047792
	140377785047888 -> 140377785023360 [dir=none]
	140377785023360 [label="mat1
 (600, 256)" fillcolor=orange]
	140377785047888 -> 140377785023120 [dir=none]
	140377785023120 [label="mat2
 (256, 256)" fillcolor=orange]
	140377785047888 [label="AddmmBackward0
----------------------------
alpha       :              1
beta        :              1
mat1        : [saved tensor]
mat1_sizes  :     (600, 256)
mat1_strides:       (256, 1)
mat2        : [saved tensor]
mat2_sizes  :     (256, 256)
mat2_strides:       (1, 256)"]
	140377785047984 -> 140377785047888
	140377785685472 [label="model.decoder.layers.1.self_attn.q_proj.bias
 (256)" fillcolor=lightblue]
	140377785685472 -> 140377785047984
	140377785047984 [label=AccumulateGrad]
	140377785047936 -> 140377785047888
	140377785047936 [label="ViewBackward0
-------------------------
self_sizes: (6, 100, 256)"]
	140377785048080 -> 140377785047936
	140377785048080 [label="AddBackward0
------------
alpha: 1"]
	140377787097392 -> 140377785048080
	140377796616496 -> 140377785048080
	140377785047312 -> 140377785047888
	140377785047312 [label=TBackward0]
	140377785048224 -> 140377785047312
	140377785685552 [label="model.decoder.layers.1.self_attn.q_proj.weight
 (256, 256)" fillcolor=lightblue]
	140377785685552 -> 140377785048224
	140377785048224 [label=AccumulateGrad]
	140377785047216 -> 140377785047168
	140377785047216 [label="TransposeBackward0
------------------
dim0: 1
dim1: 2"]
	140377785047552 -> 140377785047216
	140377785047552 [label="ViewBackward0
---------------------------
self_sizes: (6, 8, 100, 32)"]
	140377785047744 -> 140377785047552
	140377785047744 [label=CloneBackward0]
	140377785048032 -> 140377785047744
	140377785048032 [label="TransposeBackward0
------------------
dim0: 1
dim1: 2"]
	140377785048320 -> 140377785048032
	140377785048320 [label="ViewBackward0
-------------------------
self_sizes: (6, 100, 256)"]
	140377785048128 -> 140377785048320
	140377785048128 [label="ViewBackward0
----------------------
self_sizes: (600, 256)"]
	140377785048416 -> 140377785048128
	140377785048416 -> 140377785022400 [dir=none]
	140377785022400 [label="mat1
 (600, 256)" fillcolor=orange]
	140377785048416 -> 140377785022960 [dir=none]
	140377785022960 [label="mat2
 (256, 256)" fillcolor=orange]
	140377785048416 [label="AddmmBackward0
----------------------------
alpha       :              1
beta        :              1
mat1        : [saved tensor]
mat1_sizes  :     (600, 256)
mat1_strides:       (256, 1)
mat2        : [saved tensor]
mat2_sizes  :     (256, 256)
mat2_strides:       (1, 256)"]
	140377785048512 -> 140377785048416
	140377785685792 [label="model.decoder.layers.1.self_attn.k_proj.bias
 (256)" fillcolor=lightblue]
	140377785685792 -> 140377785048512
	140377785048512 [label=AccumulateGrad]
	140377785048464 -> 140377785048416
	140377785048464 [label="ViewBackward0
-------------------------
self_sizes: (6, 100, 256)"]
	140377785048080 -> 140377785048464
	140377785047360 -> 140377785048416
	140377785047360 [label=TBackward0]
	140377785048704 -> 140377785047360
	140377785685872 [label="model.decoder.layers.1.self_attn.k_proj.weight
 (256, 256)" fillcolor=lightblue]
	140377785685872 -> 140377785048704
	140377785048704 [label=AccumulateGrad]
	140377785046976 -> 140377785046928
	140377785046976 [label="ViewBackward0
---------------------------
self_sizes: (6, 8, 100, 32)"]
	140377785047456 -> 140377785046976
	140377785047456 [label=CloneBackward0]
	140377785047840 -> 140377785047456
	140377785047840 [label="TransposeBackward0
------------------
dim0: 1
dim1: 2"]
	140377785048176 -> 140377785047840
	140377785048176 [label="ViewBackward0
-------------------------
self_sizes: (6, 100, 256)"]
	140377785048560 -> 140377785048176
	140377785048560 [label="ViewBackward0
----------------------
self_sizes: (600, 256)"]
	140377785048800 -> 140377785048560
	140377785048800 -> 140377785023520 [dir=none]
	140377785023520 [label="mat1
 (600, 256)" fillcolor=orange]
	140377785048800 -> 140377785023600 [dir=none]
	140377785023600 [label="mat2
 (256, 256)" fillcolor=orange]
	140377785048800 [label="AddmmBackward0
----------------------------
alpha       :              1
beta        :              1
mat1        : [saved tensor]
mat1_sizes  :     (600, 256)
mat1_strides:       (256, 1)
mat2        : [saved tensor]
mat2_sizes  :     (256, 256)
mat2_strides:       (1, 256)"]
	140377785048608 -> 140377785048800
	140377785685632 [label="model.decoder.layers.1.self_attn.v_proj.bias
 (256)" fillcolor=lightblue]
	140377785685632 -> 140377785048608
	140377785048608 [label=AccumulateGrad]
	140377785048752 -> 140377785048800
	140377785048752 [label="ViewBackward0
-------------------------
self_sizes: (6, 100, 256)"]
	140377787097392 -> 140377785048752
	140377785047120 -> 140377785048800
	140377785047120 [label=TBackward0]
	140377785048992 -> 140377785047120
	140377785685712 [label="model.decoder.layers.1.self_attn.v_proj.weight
 (256, 256)" fillcolor=lightblue]
	140377785685712 -> 140377785048992
	140377785048992 [label=AccumulateGrad]
	140377787514064 -> 140377787100032
	140377787514064 [label=TBackward0]
	140377785046688 -> 140377787514064
	140377785685392 [label="model.decoder.layers.1.self_attn.out_proj.weight
 (256, 256)" fillcolor=lightblue]
	140377785685392 -> 140377785046688
	140377785046688 [label=AccumulateGrad]
	140377787101136 -> 140377787072080
	140377785685232 [label="model.decoder.layers.1.self_attn_layer_norm.weight
 (256)" fillcolor=lightblue]
	140377785685232 -> 140377787101136
	140377787101136 [label=AccumulateGrad]
	140377787097152 -> 140377787072080
	140377785685152 [label="model.decoder.layers.1.self_attn_layer_norm.bias
 (256)" fillcolor=lightblue]
	140377785685152 -> 140377787097152
	140377787097152 [label=AccumulateGrad]
	140377787072272 -> 140377787071696
	140377787072272 -> 140377785023840 [dir=none]
	140377785023840 [label="other
 (6, 100, 256)" fillcolor=orange]
	140377787072272 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140377787098112 -> 140377787072272
	140377787098112 [label="ViewBackward0
----------------------
self_sizes: (600, 256)"]
	140377787513200 -> 140377787098112
	140377787513200 -> 140377785023920 [dir=none]
	140377785023920 [label="mat1
 (600, 256)" fillcolor=orange]
	140377787513200 -> 140377785023680 [dir=none]
	140377785023680 [label="mat2
 (256, 256)" fillcolor=orange]
	140377787513200 [label="AddmmBackward0
----------------------------
alpha       :              1
beta        :              1
mat1        : [saved tensor]
mat1_sizes  :     (600, 256)
mat1_strides:       (256, 1)
mat2        : [saved tensor]
mat2_sizes  :     (256, 256)
mat2_strides:       (1, 256)"]
	140377787100416 -> 140377787513200
	140377787314560 [label="model.decoder.layers.1.encoder_attn.out_proj.bias
 (256)" fillcolor=lightblue]
	140377787314560 -> 140377787100416
	140377787100416 [label=AccumulateGrad]
	140377785046880 -> 140377787513200
	140377785046880 [label="ViewBackward0
-------------------------
self_sizes: (6, 100, 256)"]
	140377785045488 -> 140377785046880
	140377785045488 [label="UnsafeViewBackward0
---------------------------
self_sizes: (6, 100, 8, 32)"]
	140377785048272 -> 140377785045488
	140377785048272 [label=CloneBackward0]
	140377785048656 -> 140377785048272
	140377785048656 [label="TransposeBackward0
------------------
dim0: 1
dim1: 2"]
	140377785048944 -> 140377785048656
	140377785048944 [label="ViewBackward0
-------------------------
self_sizes: (48, 100, 32)"]
	140377785048896 -> 140377785048944
	140377785048896 -> 140377786820064 [dir=none]
	140377786820064 [label="mat2
 (48, 782, 32)" fillcolor=orange]
	140377785048896 -> 140377786820224 [dir=none]
	140377786820224 [label="self
 (48, 100, 782)" fillcolor=orange]
	140377785048896 [label="BmmBackward0
--------------------
mat2: [saved tensor]
self: [saved tensor]"]
	140377785047072 -> 140377785048896
	140377785047072 -> 140377785023280 [dir=none]
	140377785023280 [label="result
 (48, 100, 782)" fillcolor=orange]
	140377785047072 [label="SoftmaxBackward0
----------------------------
dim   : 18446744073709551615
result:       [saved tensor]"]
	140377784934656 -> 140377785047072
	140377784934656 [label="ViewBackward0
----------------------------
self_sizes: (6, 8, 100, 782)"]
	140377784934752 -> 140377784934656
	140377784934752 [label="AddBackward0
------------
alpha: 1"]
	140377784934848 -> 140377784934752
	140377784934848 [label="ViewBackward0
--------------------------
self_sizes: (48, 100, 782)"]
	140377784934944 -> 140377784934848
	140377784934944 -> 140377786819664 [dir=none]
	140377786819664 [label="mat2
 (48, 32, 782)" fillcolor=orange]
	140377784934944 -> 140377786819984 [dir=none]
	140377786819984 [label="self
 (48, 100, 32)" fillcolor=orange]
	140377784934944 [label="BmmBackward0
--------------------
mat2: [saved tensor]
self: [saved tensor]"]
	140377784935040 -> 140377784934944
	140377784935040 [label="ViewBackward0
---------------------------
self_sizes: (6, 8, 100, 32)"]
	140377784935184 -> 140377784935040
	140377784935184 [label=CloneBackward0]
	140377784935280 -> 140377784935184
	140377784935280 [label="TransposeBackward0
------------------
dim0: 1
dim1: 2"]
	140377784935376 -> 140377784935280
	140377784935376 [label="ViewBackward0
-------------------------
self_sizes: (6, 100, 256)"]
	140377784935472 -> 140377784935376
	140377784935472 -> 140377785024160 [dir=none]
	140377785024160 [label="other
 ()" fillcolor=orange]
	140377784935472 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140377784935568 -> 140377784935472
	140377784935568 [label="ViewBackward0
----------------------
self_sizes: (600, 256)"]
	140377784935664 -> 140377784935568
	140377784935664 -> 140377785024240 [dir=none]
	140377785024240 [label="mat1
 (600, 256)" fillcolor=orange]
	140377784935664 -> 140377785022640 [dir=none]
	140377785022640 [label="mat2
 (256, 256)" fillcolor=orange]
	140377784935664 [label="AddmmBackward0
----------------------------
alpha       :              1
beta        :              1
mat1        : [saved tensor]
mat1_sizes  :     (600, 256)
mat1_strides:       (256, 1)
mat2        : [saved tensor]
mat2_sizes  :     (256, 256)
mat2_strides:       (1, 256)"]
	140377784935760 -> 140377784935664
	140377787314960 [label="model.decoder.layers.1.encoder_attn.q_proj.bias
 (256)" fillcolor=lightblue]
	140377787314960 -> 140377784935760
	140377784935760 [label=AccumulateGrad]
	140377784935712 -> 140377784935664
	140377784935712 [label="ViewBackward0
-------------------------
self_sizes: (6, 100, 256)"]
	140377784935856 -> 140377784935712
	140377784935856 [label="AddBackward0
------------
alpha: 1"]
	140377787072080 -> 140377784935856
	140377796616496 -> 140377784935856
	140377784935088 -> 140377784935664
	140377784935088 [label=TBackward0]
	140377784936000 -> 140377784935088
	140377787314480 [label="model.decoder.layers.1.encoder_attn.q_proj.weight
 (256, 256)" fillcolor=lightblue]
	140377787314480 -> 140377784936000
	140377784936000 [label=AccumulateGrad]
	140377784934992 -> 140377784934944
	140377784934992 [label="TransposeBackward0
------------------
dim0: 1
dim1: 2"]
	140377784935328 -> 140377784934992
	140377784935328 [label="ViewBackward0
---------------------------
self_sizes: (6, 8, 782, 32)"]
	140377784935520 -> 140377784935328
	140377784935520 [label=CloneBackward0]
	140377784935808 -> 140377784935520
	140377784935808 [label="TransposeBackward0
------------------
dim0: 1
dim1: 2"]
	140377784936096 -> 140377784935808
	140377784936096 [label="ViewBackward0
-------------------------
self_sizes: (6, 782, 256)"]
	140377784935904 -> 140377784936096
	140377784935904 [label="ViewBackward0
-----------------------
self_sizes: (4692, 256)"]
	140377784936192 -> 140377784935904
	140377784936192 -> 140377785023760 [dir=none]
	140377785023760 [label="mat1
 (4692, 256)" fillcolor=orange]
	140377784936192 -> 140377785024080 [dir=none]
	140377785024080 [label="mat2
 (256, 256)" fillcolor=orange]
	140377784936192 [label="AddmmBackward0
----------------------------
alpha       :              1
beta        :              1
mat1        : [saved tensor]
mat1_sizes  :    (4692, 256)
mat1_strides:       (256, 1)
mat2        : [saved tensor]
mat2_sizes  :     (256, 256)
mat2_strides:       (1, 256)"]
	140377784936288 -> 140377784936192
	140377785684592 [label="model.decoder.layers.1.encoder_attn.k_proj.bias
 (256)" fillcolor=lightblue]
	140377785684592 -> 140377784936288
	140377784936288 [label=AccumulateGrad]
	140377784936240 -> 140377784936192
	140377784936240 [label="ViewBackward0
-------------------------
self_sizes: (6, 782, 256)"]
	140377784936384 -> 140377784936240
	140377784936384 [label="AddBackward0
------------
alpha: 1"]
	140377787485632 -> 140377784936384
	140377784935136 -> 140377784936192
	140377784935136 [label=TBackward0]
	140377784936528 -> 140377784935136
	140377785684272 [label="model.decoder.layers.1.encoder_attn.k_proj.weight
 (256, 256)" fillcolor=lightblue]
	140377785684272 -> 140377784936528
	140377784936528 [label=AccumulateGrad]
	140377784934512 -> 140377785048896
	140377784934512 [label="ViewBackward0
---------------------------
self_sizes: (6, 8, 782, 32)"]
	140377784934800 -> 140377784934512
	140377784934800 [label=CloneBackward0]
	140377784934560 -> 140377784934800
	140377784934560 [label="TransposeBackward0
------------------
dim0: 1
dim1: 2"]
	140377784935424 -> 140377784934560
	140377784935424 [label="ViewBackward0
-------------------------
self_sizes: (6, 782, 256)"]
	140377784936048 -> 140377784935424
	140377784936048 [label="ViewBackward0
-----------------------
self_sizes: (4692, 256)"]
	140377784936144 -> 140377784936048
	140377784936144 -> 140377785024400 [dir=none]
	140377785024400 [label="mat1
 (4692, 256)" fillcolor=orange]
	140377784936144 -> 140377785024320 [dir=none]
	140377785024320 [label="mat2
 (256, 256)" fillcolor=orange]
	140377784936144 [label="AddmmBackward0
----------------------------
alpha       :              1
beta        :              1
mat1        : [saved tensor]
mat1_sizes  :    (4692, 256)
mat1_strides:       (256, 1)
mat2        : [saved tensor]
mat2_sizes  :     (256, 256)
mat2_strides:       (1, 256)"]
	140377784936576 -> 140377784936144
	140377785684432 [label="model.decoder.layers.1.encoder_attn.v_proj.bias
 (256)" fillcolor=lightblue]
	140377785684432 -> 140377784936576
	140377784936576 [label=AccumulateGrad]
	140377784936336 -> 140377784936144
	140377784936336 [label="ViewBackward0
-------------------------
self_sizes: (6, 782, 256)"]
	140377787485632 -> 140377784936336
	140377784934608 -> 140377784936144
	140377784934608 [label=TBackward0]
	140377784936672 -> 140377784934608
	140377785684352 [label="model.decoder.layers.1.encoder_attn.v_proj.weight
 (256, 256)" fillcolor=lightblue]
	140377785684352 -> 140377784936672
	140377784936672 [label=AccumulateGrad]
	140377785046400 -> 140377787513200
	140377785046400 [label=TBackward0]
	140377785048368 -> 140377785046400
	140377787314800 [label="model.decoder.layers.1.encoder_attn.out_proj.weight
 (256, 256)" fillcolor=lightblue]
	140377787314800 -> 140377785048368
	140377785048368 [label=AccumulateGrad]
	140377787071648 -> 140377787071456
	140377787314640 [label="model.decoder.layers.1.encoder_attn_layer_norm.weight
 (256)" fillcolor=lightblue]
	140377787314640 -> 140377787071648
	140377787071648 [label=AccumulateGrad]
	140377787071408 -> 140377787071456
	140377787314400 [label="model.decoder.layers.1.encoder_attn_layer_norm.bias
 (256)" fillcolor=lightblue]
	140377787314400 -> 140377787071408
	140377787071408 [label=AccumulateGrad]
	140377787071888 -> 140377787070832
	140377787071888 -> 140377785024000 [dir=none]
	140377785024000 [label="other
 (6, 100, 256)" fillcolor=orange]
	140377787071888 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140377787072032 -> 140377787071888
	140377787072032 [label="ViewBackward0
----------------------
self_sizes: (600, 256)"]
	140377787097536 -> 140377787072032
	140377787097536 -> 140377785023440 [dir=none]
	140377785023440 [label="mat1
 (600, 2048)" fillcolor=orange]
	140377787097536 -> 140377784926352 [dir=none]
	140377784926352 [label="mat2
 (2048, 256)" fillcolor=orange]
	140377787097536 [label="AddmmBackward0
----------------------------
alpha       :              1
beta        :              1
mat1        : [saved tensor]
mat1_sizes  :    (600, 2048)
mat1_strides:      (2048, 1)
mat2        : [saved tensor]
mat2_sizes  :    (2048, 256)
mat2_strides:      (1, 2048)"]
	140377785049040 -> 140377787097536
	140378123963984 [label="model.decoder.layers.1.fc2.bias
 (256)" fillcolor=lightblue]
	140378123963984 -> 140377785049040
	140377785049040 [label=AccumulateGrad]
	140377785047648 -> 140377787097536
	140377785047648 [label="ViewBackward0
--------------------------
self_sizes: (6, 100, 2048)"]
	140377785046592 -> 140377785047648
	140377785046592 -> 140377784926672 [dir=none]
	140377784926672 [label="result
 (6, 100, 2048)" fillcolor=orange]
	140377785046592 [label="ReluBackward0
----------------------
result: [saved tensor]"]
	140377784935232 -> 140377785046592
	140377784935232 [label="ViewBackward0
-----------------------
self_sizes: (600, 2048)"]
	140377784935952 -> 140377784935232
	140377784935952 -> 140377784926432 [dir=none]
	140377784926432 [label="mat1
 (600, 256)" fillcolor=orange]
	140377784935952 -> 140377784926752 [dir=none]
	140377784926752 [label="mat2
 (256, 2048)" fillcolor=orange]
	140377784935952 [label="AddmmBackward0
----------------------------
alpha       :              1
beta        :              1
mat1        : [saved tensor]
mat1_sizes  :     (600, 256)
mat1_strides:       (256, 1)
mat2        : [saved tensor]
mat2_sizes  :    (256, 2048)
mat2_strides:       (1, 256)"]
	140377784936432 -> 140377784935952
	140377787314240 [label="model.decoder.layers.1.fc1.bias
 (2048)" fillcolor=lightblue]
	140377787314240 -> 140377784936432
	140377784936432 [label=AccumulateGrad]
	140377784936624 -> 140377784935952
	140377784936624 [label="ViewBackward0
-------------------------
self_sizes: (6, 100, 256)"]
	140377787071456 -> 140377784936624
	140377784934704 -> 140377784935952
	140377784934704 [label=TBackward0]
	140377784936816 -> 140377784934704
	140377787314720 [label="model.decoder.layers.1.fc1.weight
 (2048, 256)" fillcolor=lightblue]
	140377787314720 -> 140377784936816
	140377784936816 [label=AccumulateGrad]
	140377785046784 -> 140377787097536
	140377785046784 [label=TBackward0]
	140377784935616 -> 140377785046784
	140378328944864 [label="model.decoder.layers.1.fc2.weight
 (256, 2048)" fillcolor=lightblue]
	140378328944864 -> 140377784935616
	140377784935616 [label=AccumulateGrad]
	140377787071360 -> 140377787071168
	140377785585728 [label="model.decoder.layers.1.final_layer_norm.weight
 (256)" fillcolor=lightblue]
	140377785585728 -> 140377787071360
	140377787071360 [label=AccumulateGrad]
	140377787070592 -> 140377787071168
	140377785585808 [label="model.decoder.layers.1.final_layer_norm.bias
 (256)" fillcolor=lightblue]
	140377785585808 -> 140377787070592
	140377787070592 [label=AccumulateGrad]
	140377787070112 -> 140377787072224
	140377787070112 -> 140377784926912 [dir=none]
	140377784926912 [label="other
 (6, 100, 256)" fillcolor=orange]
	140377787070112 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140377787070640 -> 140377787070112
	140377787070640 [label="ViewBackward0
----------------------
self_sizes: (600, 256)"]
	140377785048848 -> 140377787070640
	140377785048848 -> 140377784927072 [dir=none]
	140377784927072 [label="mat1
 (600, 256)" fillcolor=orange]
	140377785048848 -> 140377784926592 [dir=none]
	140377784926592 [label="mat2
 (256, 256)" fillcolor=orange]
	140377785048848 [label="AddmmBackward0
----------------------------
alpha       :              1
beta        :              1
mat1        : [saved tensor]
mat1_sizes  :     (600, 256)
mat1_strides:       (256, 1)
mat2        : [saved tensor]
mat2_sizes  :     (256, 256)
mat2_strides:       (1, 256)"]
	140377787100944 -> 140377785048848
	140377785586448 [label="model.decoder.layers.2.self_attn.out_proj.bias
 (256)" fillcolor=lightblue]
	140377785586448 -> 140377787100944
	140377787100944 [label=AccumulateGrad]
	140377784936480 -> 140377785048848
	140377784936480 [label="ViewBackward0
-------------------------
self_sizes: (6, 100, 256)"]
	140377784934464 -> 140377784936480
	140377784934464 [label="UnsafeViewBackward0
---------------------------
self_sizes: (6, 100, 8, 32)"]
	140377784936960 -> 140377784934464
	140377784936960 [label=CloneBackward0]
	140377784937056 -> 140377784936960
	140377784937056 [label="TransposeBackward0
------------------
dim0: 1
dim1: 2"]
	140377784937152 -> 140377784937056
	140377784937152 [label="ViewBackward0
-------------------------
self_sizes: (48, 100, 32)"]
	140377784937248 -> 140377784937152
	140377784937248 -> 140377786821104 [dir=none]
	140377786821104 [label="mat2
 (48, 100, 32)" fillcolor=orange]
	140377784937248 -> 140377786821264 [dir=none]
	140377786821264 [label="self
 (48, 100, 100)" fillcolor=orange]
	140377784937248 [label="BmmBackward0
--------------------
mat2: [saved tensor]
self: [saved tensor]"]
	140377784937344 -> 140377784937248
	140377784937344 -> 140377784926272 [dir=none]
	140377784926272 [label="result
 (48, 100, 100)" fillcolor=orange]
	140377784937344 [label="SoftmaxBackward0
----------------------------
dim   : 18446744073709551615
result:       [saved tensor]"]
	140377784937488 -> 140377784937344
	140377784937488 -> 140377786820704 [dir=none]
	140377786820704 [label="mat2
 (48, 32, 100)" fillcolor=orange]
	140377784937488 -> 140377786821024 [dir=none]
	140377786821024 [label="self
 (48, 100, 32)" fillcolor=orange]
	140377784937488 [label="BmmBackward0
--------------------
mat2: [saved tensor]
self: [saved tensor]"]
	140377784937584 -> 140377784937488
	140377784937584 [label="ViewBackward0
---------------------------
self_sizes: (6, 8, 100, 32)"]
	140377784937728 -> 140377784937584
	140377784937728 [label=CloneBackward0]
	140377784937824 -> 140377784937728
	140377784937824 [label="TransposeBackward0
------------------
dim0: 1
dim1: 2"]
	140377784937920 -> 140377784937824
	140377784937920 [label="ViewBackward0
-------------------------
self_sizes: (6, 100, 256)"]
	140377784938016 -> 140377784937920
	140377784938016 -> 140377784927232 [dir=none]
	140377784927232 [label="other
 ()" fillcolor=orange]
	140377784938016 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140377784938112 -> 140377784938016
	140377784938112 [label="ViewBackward0
----------------------
self_sizes: (600, 256)"]
	140377784938208 -> 140377784938112
	140377784938208 -> 140377784927392 [dir=none]
	140377784927392 [label="mat1
 (600, 256)" fillcolor=orange]
	140377784938208 -> 140377784927152 [dir=none]
	140377784927152 [label="mat2
 (256, 256)" fillcolor=orange]
	140377784938208 [label="AddmmBackward0
----------------------------
alpha       :              1
beta        :              1
mat1        : [saved tensor]
mat1_sizes  :     (600, 256)
mat1_strides:       (256, 1)
mat2        : [saved tensor]
mat2_sizes  :     (256, 256)
mat2_strides:       (1, 256)"]
	140377784938304 -> 140377784938208
	140377785586288 [label="model.decoder.layers.2.self_attn.q_proj.bias
 (256)" fillcolor=lightblue]
	140377785586288 -> 140377784938304
	140377784938304 [label=AccumulateGrad]
	140377784938256 -> 140377784938208
	140377784938256 [label="ViewBackward0
-------------------------
self_sizes: (6, 100, 256)"]
	140377784938400 -> 140377784938256
	140377784938400 [label="AddBackward0
------------
alpha: 1"]
	140377787071168 -> 140377784938400
	140377796616496 -> 140377784938400
	140377784937632 -> 140377784938208
	140377784937632 [label=TBackward0]
	140377784938448 -> 140377784937632
	140377785586208 [label="model.decoder.layers.2.self_attn.q_proj.weight
 (256, 256)" fillcolor=lightblue]
	140377785586208 -> 140377784938448
	140377784938448 [label=AccumulateGrad]
	140377784937536 -> 140377784937488
	140377784937536 [label="TransposeBackward0
------------------
dim0: 1
dim1: 2"]
	140377784937872 -> 140377784937536
	140377784937872 [label="ViewBackward0
---------------------------
self_sizes: (6, 8, 100, 32)"]
	140377784938064 -> 140377784937872
	140377784938064 [label=CloneBackward0]
	140377784938352 -> 140377784938064
	140377784938352 [label="TransposeBackward0
------------------
dim0: 1
dim1: 2"]
	140377784937680 -> 140377784938352
	140377784937680 [label="ViewBackward0
-------------------------
self_sizes: (6, 100, 256)"]
	140377602257104 -> 140377784937680
	140377602257104 [label="ViewBackward0
----------------------
self_sizes: (600, 256)"]
	140377602257200 -> 140377602257104
	140377602257200 -> 140377784926832 [dir=none]
	140377784926832 [label="mat1
 (600, 256)" fillcolor=orange]
	140377602257200 -> 140377784926992 [dir=none]
	140377784926992 [label="mat2
 (256, 256)" fillcolor=orange]
	140377602257200 [label="AddmmBackward0
----------------------------
alpha       :              1
beta        :              1
mat1        : [saved tensor]
mat1_sizes  :     (600, 256)
mat1_strides:       (256, 1)
mat2        : [saved tensor]
mat2_sizes  :     (256, 256)
mat2_strides:       (1, 256)"]
	140377602257296 -> 140377602257200
	140377785585968 [label="model.decoder.layers.2.self_attn.k_proj.bias
 (256)" fillcolor=lightblue]
	140377785585968 -> 140377602257296
	140377602257296 [label=AccumulateGrad]
	140377602257248 -> 140377602257200
	140377602257248 [label="ViewBackward0
-------------------------
self_sizes: (6, 100, 256)"]
	140377784938400 -> 140377602257248
	140377602256960 -> 140377602257200
	140377602256960 [label=TBackward0]
	140377602257488 -> 140377602256960
	140377785585888 [label="model.decoder.layers.2.self_attn.k_proj.weight
 (256, 256)" fillcolor=lightblue]
	140377785585888 -> 140377602257488
	140377602257488 [label=AccumulateGrad]
	140377784937296 -> 140377784937248
	140377784937296 [label="ViewBackward0
---------------------------
self_sizes: (6, 8, 100, 32)"]
	140377784937776 -> 140377784937296
	140377784937776 [label=CloneBackward0]
	140377784938160 -> 140377784937776
	140377784938160 [label="TransposeBackward0
------------------
dim0: 1
dim1: 2"]
	140377784937440 -> 140377784938160
	140377784937440 [label="ViewBackward0
-------------------------
self_sizes: (6, 100, 256)"]
	140377602257344 -> 140377784937440
	140377602257344 [label="ViewBackward0
----------------------
self_sizes: (600, 256)"]
	140377602257584 -> 140377602257344
	140377602257584 -> 140377784927552 [dir=none]
	140377784927552 [label="mat1
 (600, 256)" fillcolor=orange]
	140377602257584 -> 140377784927632 [dir=none]
	140377784927632 [label="mat2
 (256, 256)" fillcolor=orange]
	140377602257584 [label="AddmmBackward0
----------------------------
alpha       :              1
beta        :              1
mat1        : [saved tensor]
mat1_sizes  :     (600, 256)
mat1_strides:       (256, 1)
mat2        : [saved tensor]
mat2_sizes  :     (256, 256)
mat2_strides:       (1, 256)"]
	140377602257392 -> 140377602257584
	140377785586128 [label="model.decoder.layers.2.self_attn.v_proj.bias
 (256)" fillcolor=lightblue]
	140377785586128 -> 140377602257392
	140377602257392 [label=AccumulateGrad]
	140377602257536 -> 140377602257584
	140377602257536 [label="ViewBackward0
-------------------------
self_sizes: (6, 100, 256)"]
	140377787071168 -> 140377602257536
	140377602257008 -> 140377602257584
	140377602257008 [label=TBackward0]
	140377602257776 -> 140377602257008
	140377785586048 [label="model.decoder.layers.2.self_attn.v_proj.weight
 (256, 256)" fillcolor=lightblue]
	140377785586048 -> 140377602257776
	140377602257776 [label=AccumulateGrad]
	140377784934896 -> 140377785048848
	140377784934896 [label=TBackward0]
	140377784937008 -> 140377784934896
	140377785586368 [label="model.decoder.layers.2.self_attn.out_proj.weight
 (256, 256)" fillcolor=lightblue]
	140377785586368 -> 140377784937008
	140377784937008 [label=AccumulateGrad]
	140377787070496 -> 140377787070544
	140377785586528 [label="model.decoder.layers.2.self_attn_layer_norm.weight
 (256)" fillcolor=lightblue]
	140377785586528 -> 140377787070496
	140377787070496 [label=AccumulateGrad]
	140377787069776 -> 140377787070544
	140377785586608 [label="model.decoder.layers.2.self_attn_layer_norm.bias
 (256)" fillcolor=lightblue]
	140377785586608 -> 140377787069776
	140377787069776 [label=AccumulateGrad]
	140377787071216 -> 140377787070304
	140377787071216 -> 140377784927872 [dir=none]
	140377784927872 [label="other
 (6, 100, 256)" fillcolor=orange]
	140377787071216 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140377787070784 -> 140377787071216
	140377787070784 [label="ViewBackward0
----------------------
self_sizes: (600, 256)"]
	140377787098976 -> 140377787070784
	140377787098976 -> 140377784927952 [dir=none]
	140377784927952 [label="mat1
 (600, 256)" fillcolor=orange]
	140377787098976 -> 140377784927712 [dir=none]
	140377784927712 [label="mat2
 (256, 256)" fillcolor=orange]
	140377787098976 [label="AddmmBackward0
----------------------------
alpha       :              1
beta        :              1
mat1        : [saved tensor]
mat1_sizes  :     (600, 256)
mat1_strides:       (256, 1)
mat2        : [saved tensor]
mat2_sizes  :     (256, 256)
mat2_strides:       (1, 256)"]
	140377784937200 -> 140377787098976
	140377785587248 [label="model.decoder.layers.2.encoder_attn.out_proj.bias
 (256)" fillcolor=lightblue]
	140377785587248 -> 140377784937200
	140377784937200 [label=AccumulateGrad]
	140377784936720 -> 140377787098976
	140377784936720 [label="ViewBackward0
-------------------------
self_sizes: (6, 100, 256)"]
	140377784936912 -> 140377784936720
	140377784936912 [label="UnsafeViewBackward0
---------------------------
self_sizes: (6, 100, 8, 32)"]
	140377784937968 -> 140377784936912
	140377784937968 [label=CloneBackward0]
	140377602257440 -> 140377784937968
	140377602257440 [label="TransposeBackward0
------------------
dim0: 1
dim1: 2"]
	140377602257728 -> 140377602257440
	140377602257728 [label="ViewBackward0
-------------------------
self_sizes: (48, 100, 32)"]
	140377602257824 -> 140377602257728
	140377602257824 -> 140377786822064 [dir=none]
	140377786822064 [label="mat2
 (48, 782, 32)" fillcolor=orange]
	140377602257824 -> 140377786822224 [dir=none]
	140377786822224 [label="self
 (48, 100, 782)" fillcolor=orange]
	140377602257824 [label="BmmBackward0
--------------------
mat2: [saved tensor]
self: [saved tensor]"]
	140377602257920 -> 140377602257824
	140377602257920 -> 140377784927312 [dir=none]
	140377784927312 [label="result
 (48, 100, 782)" fillcolor=orange]
	140377602257920 [label="SoftmaxBackward0
----------------------------
dim   : 18446744073709551615
result:       [saved tensor]"]
	140377602258064 -> 140377602257920
	140377602258064 [label="ViewBackward0
----------------------------
self_sizes: (6, 8, 100, 782)"]
	140377602258160 -> 140377602258064
	140377602258160 [label="AddBackward0
------------
alpha: 1"]
	140377602258256 -> 140377602258160
	140377602258256 [label="ViewBackward0
--------------------------
self_sizes: (48, 100, 782)"]
	140377602258352 -> 140377602258256
	140377602258352 -> 140377786821664 [dir=none]
	140377786821664 [label="mat2
 (48, 32, 782)" fillcolor=orange]
	140377602258352 -> 140377786821984 [dir=none]
	140377786821984 [label="self
 (48, 100, 32)" fillcolor=orange]
	140377602258352 [label="BmmBackward0
--------------------
mat2: [saved tensor]
self: [saved tensor]"]
	140377602258448 -> 140377602258352
	140377602258448 [label="ViewBackward0
---------------------------
self_sizes: (6, 8, 100, 32)"]
	140377602258592 -> 140377602258448
	140377602258592 [label=CloneBackward0]
	140377602258688 -> 140377602258592
	140377602258688 [label="TransposeBackward0
------------------
dim0: 1
dim1: 2"]
	140377602258784 -> 140377602258688
	140377602258784 [label="ViewBackward0
-------------------------
self_sizes: (6, 100, 256)"]
	140377602258880 -> 140377602258784
	140377602258880 -> 140377784928192 [dir=none]
	140377784928192 [label="other
 ()" fillcolor=orange]
	140377602258880 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140377602258976 -> 140377602258880
	140377602258976 [label="ViewBackward0
----------------------
self_sizes: (600, 256)"]
	140377602259072 -> 140377602258976
	140377602259072 -> 140377784928272 [dir=none]
	140377784928272 [label="mat1
 (600, 256)" fillcolor=orange]
	140377602259072 -> 140377784926512 [dir=none]
	140377784926512 [label="mat2
 (256, 256)" fillcolor=orange]
	140377602259072 [label="AddmmBackward0
----------------------------
alpha       :              1
beta        :              1
mat1        : [saved tensor]
mat1_sizes  :     (600, 256)
mat1_strides:       (256, 1)
mat2        : [saved tensor]
mat2_sizes  :     (256, 256)
mat2_strides:       (1, 256)"]
	140377602259168 -> 140377602259072
	140377785587088 [label="model.decoder.layers.2.encoder_attn.q_proj.bias
 (256)" fillcolor=lightblue]
	140377785587088 -> 140377602259168
	140377602259168 [label=AccumulateGrad]
	140377602259120 -> 140377602259072
	140377602259120 [label="ViewBackward0
-------------------------
self_sizes: (6, 100, 256)"]
	140377602259264 -> 140377602259120
	140377602259264 [label="AddBackward0
------------
alpha: 1"]
	140377787070544 -> 140377602259264
	140377796616496 -> 140377602259264
	140377602258496 -> 140377602259072
	140377602258496 [label=TBackward0]
	140377602259408 -> 140377602258496
	140377785587008 [label="model.decoder.layers.2.encoder_attn.q_proj.weight
 (256, 256)" fillcolor=lightblue]
	140377785587008 -> 140377602259408
	140377602259408 [label=AccumulateGrad]
	140377602258400 -> 140377602258352
	140377602258400 [label="TransposeBackward0
------------------
dim0: 1
dim1: 2"]
	140377602258736 -> 140377602258400
	140377602258736 [label="ViewBackward0
---------------------------
self_sizes: (6, 8, 782, 32)"]
	140377602258928 -> 140377602258736
	140377602258928 [label=CloneBackward0]
	140377602259216 -> 140377602258928
	140377602259216 [label="TransposeBackward0
------------------
dim0: 1
dim1: 2"]
	140377602259504 -> 140377602259216
	140377602259504 [label="ViewBackward0
-------------------------
self_sizes: (6, 782, 256)"]
	140377602259312 -> 140377602259504
	140377602259312 [label="ViewBackward0
-----------------------
self_sizes: (4692, 256)"]
	140377602259600 -> 140377602259312
	140377602259600 -> 140377784927792 [dir=none]
	140377784927792 [label="mat1
 (4692, 256)" fillcolor=orange]
	140377602259600 -> 140377784928112 [dir=none]
	140377784928112 [label="mat2
 (256, 256)" fillcolor=orange]
	140377602259600 [label="AddmmBackward0
----------------------------
alpha       :              1
beta        :              1
mat1        : [saved tensor]
mat1_sizes  :    (4692, 256)
mat1_strides:       (256, 1)
mat2        : [saved tensor]
mat2_sizes  :     (256, 256)
mat2_strides:       (1, 256)"]
	140377602259696 -> 140377602259600
	140377785586768 [label="model.decoder.layers.2.encoder_attn.k_proj.bias
 (256)" fillcolor=lightblue]
	140377785586768 -> 140377602259696
	140377602259696 [label=AccumulateGrad]
	140377602259648 -> 140377602259600
	140377602259648 [label="ViewBackward0
-------------------------
self_sizes: (6, 782, 256)"]
	140377602259792 -> 140377602259648
	140377602259792 [label="AddBackward0
------------
alpha: 1"]
	140377787485632 -> 140377602259792
	140377602258544 -> 140377602259600
	140377602258544 [label=TBackward0]
	140377602259936 -> 140377602258544
	140377785586688 [label="model.decoder.layers.2.encoder_attn.k_proj.weight
 (256, 256)" fillcolor=lightblue]
	140377785586688 -> 140377602259936
	140377602259936 [label=AccumulateGrad]
	140377602257680 -> 140377602257824
	140377602257680 [label="ViewBackward0
---------------------------
self_sizes: (6, 8, 782, 32)"]
	140377602258208 -> 140377602257680
	140377602258208 [label=CloneBackward0]
	140377602257968 -> 140377602258208
	140377602257968 [label="TransposeBackward0
------------------
dim0: 1
dim1: 2"]
	140377602258832 -> 140377602257968
	140377602258832 [label="ViewBackward0
-------------------------
self_sizes: (6, 782, 256)"]
	140377602259456 -> 140377602258832
	140377602259456 [label="ViewBackward0
-----------------------
self_sizes: (4692, 256)"]
	140377602259552 -> 140377602259456
	140377602259552 -> 140377784928432 [dir=none]
	140377784928432 [label="mat1
 (4692, 256)" fillcolor=orange]
	140377602259552 -> 140377784928512 [dir=none]
	140377784928512 [label="mat2
 (256, 256)" fillcolor=orange]
	140377602259552 [label="AddmmBackward0
----------------------------
alpha       :              1
beta        :              1
mat1        : [saved tensor]
mat1_sizes  :    (4692, 256)
mat1_strides:       (256, 1)
mat2        : [saved tensor]
mat2_sizes  :     (256, 256)
mat2_strides:       (1, 256)"]
	140377602259984 -> 140377602259552
	140377785586928 [label="model.decoder.layers.2.encoder_attn.v_proj.bias
 (256)" fillcolor=lightblue]
	140377785586928 -> 140377602259984
	140377602259984 [label=AccumulateGrad]
	140377602259744 -> 140377602259552
	140377602259744 [label="ViewBackward0
-------------------------
self_sizes: (6, 782, 256)"]
	140377787485632 -> 140377602259744
	140377602258016 -> 140377602259552
	140377602258016 [label=TBackward0]
	140377602260080 -> 140377602258016
	140377785586848 [label="model.decoder.layers.2.encoder_attn.v_proj.weight
 (256, 256)" fillcolor=lightblue]
	140377785586848 -> 140377602260080
	140377602260080 [label=AccumulateGrad]
	140377784936768 -> 140377787098976
	140377784936768 [label=TBackward0]
	140377784937392 -> 140377784936768
	140377785587168 [label="model.decoder.layers.2.encoder_attn.out_proj.weight
 (256, 256)" fillcolor=lightblue]
	140377785587168 -> 140377784937392
	140377784937392 [label=AccumulateGrad]
	140377787071504 -> 140377787069440
	140377785587328 [label="model.decoder.layers.2.encoder_attn_layer_norm.weight
 (256)" fillcolor=lightblue]
	140377785587328 -> 140377787071504
	140377787071504 [label=AccumulateGrad]
	140377787069296 -> 140377787069440
	140377785587408 [label="model.decoder.layers.2.encoder_attn_layer_norm.bias
 (256)" fillcolor=lightblue]
	140377785587408 -> 140377787069296
	140377787069296 [label=AccumulateGrad]
	140377787069632 -> 140377787068576
	140377787069632 -> 140377784928752 [dir=none]
	140377784928752 [label="other
 (6, 100, 256)" fillcolor=orange]
	140377787069632 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140377787069968 -> 140377787069632
	140377787069968 [label="ViewBackward0
----------------------
self_sizes: (600, 256)"]
	140377787070352 -> 140377787069968
	140377787070352 -> 140377784928832 [dir=none]
	140377784928832 [label="mat1
 (600, 2048)" fillcolor=orange]
	140377787070352 -> 140377784928592 [dir=none]
	140377784928592 [label="mat2
 (2048, 256)" fillcolor=orange]
	140377787070352 [label="AddmmBackward0
----------------------------
alpha       :              1
beta        :              1
mat1        : [saved tensor]
mat1_sizes  :    (600, 2048)
mat1_strides:      (2048, 1)
mat2        : [saved tensor]
mat2_sizes  :    (2048, 256)
mat2_strides:      (1, 2048)"]
	140377784936864 -> 140377787070352
	140377785587728 [label="model.decoder.layers.2.fc2.bias
 (256)" fillcolor=lightblue]
	140377785587728 -> 140377784936864
	140377784936864 [label=AccumulateGrad]
	140377784937104 -> 140377787070352
	140377784937104 [label="ViewBackward0
--------------------------
self_sizes: (6, 100, 2048)"]
	140377602257152 -> 140377784937104
	140377602257152 -> 140377784928992 [dir=none]
	140377784928992 [label="result
 (6, 100, 2048)" fillcolor=orange]
	140377602257152 [label="ReluBackward0
----------------------
result: [saved tensor]"]
	140377602258640 -> 140377602257152
	140377602258640 [label="ViewBackward0
-----------------------
self_sizes: (600, 2048)"]
	140377602259360 -> 140377602258640
	140377602259360 -> 140377784928352 [dir=none]
	140377784928352 [label="mat1
 (600, 256)" fillcolor=orange]
	140377602259360 -> 140377784929072 [dir=none]
	140377784929072 [label="mat2
 (256, 2048)" fillcolor=orange]
	140377602259360 [label="AddmmBackward0
----------------------------
alpha       :              1
beta        :              1
mat1        : [saved tensor]
mat1_sizes  :     (600, 256)
mat1_strides:       (256, 1)
mat2        : [saved tensor]
mat2_sizes  :    (256, 2048)
mat2_strides:       (1, 256)"]
	140377602259840 -> 140377602259360
	140377785587568 [label="model.decoder.layers.2.fc1.bias
 (2048)" fillcolor=lightblue]
	140377785587568 -> 140377602259840
	140377602259840 [label=AccumulateGrad]
	140377602260032 -> 140377602259360
	140377602260032 [label="ViewBackward0
-------------------------
self_sizes: (6, 100, 256)"]
	140377787069440 -> 140377602260032
	140377602258112 -> 140377602259360
	140377602258112 [label=TBackward0]
	140377602260224 -> 140377602258112
	140377785587488 [label="model.decoder.layers.2.fc1.weight
 (2048, 256)" fillcolor=lightblue]
	140377785587488 -> 140377602260224
	140377602260224 [label=AccumulateGrad]
	140377602257872 -> 140377787070352
	140377602257872 [label=TBackward0]
	140377602259024 -> 140377602257872
	140377785587648 [label="model.decoder.layers.2.fc2.weight
 (256, 2048)" fillcolor=lightblue]
	140377785587648 -> 140377602259024
	140377602259024 [label=AccumulateGrad]
	140377787068624 -> 140377787070976
	140377785587808 [label="model.decoder.layers.2.final_layer_norm.weight
 (256)" fillcolor=lightblue]
	140377785587808 -> 140377787068624
	140377787068624 [label=AccumulateGrad]
	140377787069104 -> 140377787070976
	140377785587888 [label="model.decoder.layers.2.final_layer_norm.bias
 (256)" fillcolor=lightblue]
	140377785587888 -> 140377787069104
	140377787069104 [label=AccumulateGrad]
	140377787069248 -> 140377787069056
	140377787069248 -> 140377784929152 [dir=none]
	140377784929152 [label="other
 (6, 100, 256)" fillcolor=orange]
	140377787069248 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140377787069680 -> 140377787069248
	140377787069680 [label="ViewBackward0
----------------------
self_sizes: (600, 256)"]
	140377787069728 -> 140377787069680
	140377787069728 -> 140377784929312 [dir=none]
	140377784929312 [label="mat1
 (600, 256)" fillcolor=orange]
	140377787069728 -> 140377784928032 [dir=none]
	140377784928032 [label="mat2
 (256, 256)" fillcolor=orange]
	140377787069728 [label="AddmmBackward0
----------------------------
alpha       :              1
beta        :              1
mat1        : [saved tensor]
mat1_sizes  :     (600, 256)
mat1_strides:       (256, 1)
mat2        : [saved tensor]
mat2_sizes  :     (256, 256)
mat2_strides:       (1, 256)"]
	140377602259888 -> 140377787069728
	140377785588528 [label="model.decoder.layers.3.self_attn.out_proj.bias
 (256)" fillcolor=lightblue]
	140377785588528 -> 140377602259888
	140377602259888 [label=AccumulateGrad]
	140377602258304 -> 140377787069728
	140377602258304 [label="ViewBackward0
-------------------------
self_sizes: (6, 100, 256)"]
	140377602257056 -> 140377602258304
	140377602257056 [label="UnsafeViewBackward0
---------------------------
self_sizes: (6, 100, 8, 32)"]
	140377602260368 -> 140377602257056
	140377602260368 [label=CloneBackward0]
	140377602260464 -> 140377602260368
	140377602260464 [label="TransposeBackward0
------------------
dim0: 1
dim1: 2"]
	140377602260560 -> 140377602260464
	140377602260560 [label="ViewBackward0
-------------------------
self_sizes: (48, 100, 32)"]
	140377602260656 -> 140377602260560
	140377602260656 -> 140377786913312 [dir=none]
	140377786913312 [label="mat2
 (48, 100, 32)" fillcolor=orange]
	140377602260656 -> 140377786913472 [dir=none]
	140377786913472 [label="self
 (48, 100, 100)" fillcolor=orange]
	140377602260656 [label="BmmBackward0
--------------------
mat2: [saved tensor]
self: [saved tensor]"]
	140377602260752 -> 140377602260656
	140377602260752 -> 140377784927472 [dir=none]
	140377784927472 [label="result
 (48, 100, 100)" fillcolor=orange]
	140377602260752 [label="SoftmaxBackward0
----------------------------
dim   : 18446744073709551615
result:       [saved tensor]"]
	140377602260896 -> 140377602260752
	140377602260896 -> 140377786912912 [dir=none]
	140377786912912 [label="mat2
 (48, 32, 100)" fillcolor=orange]
	140377602260896 -> 140377786913232 [dir=none]
	140377786913232 [label="self
 (48, 100, 32)" fillcolor=orange]
	140377602260896 [label="BmmBackward0
--------------------
mat2: [saved tensor]
self: [saved tensor]"]
	140377602260944 -> 140377602260896
	140377602260944 [label="ViewBackward0
---------------------------
self_sizes: (6, 8, 100, 32)"]
	140377602179280 -> 140377602260944
	140377602179280 [label=CloneBackward0]
	140377602179376 -> 140377602179280
	140377602179376 [label="TransposeBackward0
------------------
dim0: 1
dim1: 2"]
	140377602179472 -> 140377602179376
	140377602179472 [label="ViewBackward0
-------------------------
self_sizes: (6, 100, 256)"]
	140377602179568 -> 140377602179472
	140377602179568 -> 140377784929472 [dir=none]
	140377784929472 [label="other
 ()" fillcolor=orange]
	140377602179568 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140377602179664 -> 140377602179568
	140377602179664 [label="ViewBackward0
----------------------
self_sizes: (600, 256)"]
	140377602179760 -> 140377602179664
	140377602179760 -> 140377784929632 [dir=none]
	140377784929632 [label="mat1
 (600, 256)" fillcolor=orange]
	140377602179760 -> 140377784929392 [dir=none]
	140377784929392 [label="mat2
 (256, 256)" fillcolor=orange]
	140377602179760 [label="AddmmBackward0
----------------------------
alpha       :              1
beta        :              1
mat1        : [saved tensor]
mat1_sizes  :     (600, 256)
mat1_strides:       (256, 1)
mat2        : [saved tensor]
mat2_sizes  :     (256, 256)
mat2_strides:       (1, 256)"]
	140377602179856 -> 140377602179760
	140377785588368 [label="model.decoder.layers.3.self_attn.q_proj.bias
 (256)" fillcolor=lightblue]
	140377785588368 -> 140377602179856
	140377602179856 [label=AccumulateGrad]
	140377602179808 -> 140377602179760
	140377602179808 [label="ViewBackward0
-------------------------
self_sizes: (6, 100, 256)"]
	140377602179952 -> 140377602179808
	140377602179952 [label="AddBackward0
------------
alpha: 1"]
	140377787070976 -> 140377602179952
	140377796616496 -> 140377602179952
	140377602179184 -> 140377602179760
	140377602179184 [label=TBackward0]
	140377602180096 -> 140377602179184
	140377785588288 [label="model.decoder.layers.3.self_attn.q_proj.weight
 (256, 256)" fillcolor=lightblue]
	140377785588288 -> 140377602180096
	140377602180096 [label=AccumulateGrad]
	140377602260800 -> 140377602260896
	140377602260800 [label="TransposeBackward0
------------------
dim0: 1
dim1: 2"]
	140377602179424 -> 140377602260800
	140377602179424 [label="ViewBackward0
---------------------------
self_sizes: (6, 8, 100, 32)"]
	140377602179616 -> 140377602179424
	140377602179616 [label=CloneBackward0]
	140377602179904 -> 140377602179616
	140377602179904 [label="TransposeBackward0
------------------
dim0: 1
dim1: 2"]
	140377602180192 -> 140377602179904
	140377602180192 [label="ViewBackward0
-------------------------
self_sizes: (6, 100, 256)"]
	140377602180000 -> 140377602180192
	140377602180000 [label="ViewBackward0
----------------------
self_sizes: (600, 256)"]
	140377602180288 -> 140377602180000
	140377602180288 -> 140377784928672 [dir=none]
	140377784928672 [label="mat1
 (600, 256)" fillcolor=orange]
	140377602180288 -> 140377784929232 [dir=none]
	140377784929232 [label="mat2
 (256, 256)" fillcolor=orange]
	140377602180288 [label="AddmmBackward0
----------------------------
alpha       :              1
beta        :              1
mat1        : [saved tensor]
mat1_sizes  :     (600, 256)
mat1_strides:       (256, 1)
mat2        : [saved tensor]
mat2_sizes  :     (256, 256)
mat2_strides:       (1, 256)"]
	140377602180384 -> 140377602180288
	140377785588048 [label="model.decoder.layers.3.self_attn.k_proj.bias
 (256)" fillcolor=lightblue]
	140377785588048 -> 140377602180384
	140377602180384 [label=AccumulateGrad]
	140377602180336 -> 140377602180288
	140377602180336 [label="ViewBackward0
-------------------------
self_sizes: (6, 100, 256)"]
	140377602179952 -> 140377602180336
	140377602179232 -> 140377602180288
	140377602179232 [label=TBackward0]
	140377602180576 -> 140377602179232
	140377785587968 [label="model.decoder.layers.3.self_attn.k_proj.weight
 (256, 256)" fillcolor=lightblue]
	140377785587968 -> 140377602180576
	140377602180576 [label=AccumulateGrad]
	140377602260704 -> 140377602260656
	140377602260704 [label="ViewBackward0
---------------------------
self_sizes: (6, 8, 100, 32)"]
	140377602260848 -> 140377602260704
	140377602260848 [label=CloneBackward0]
	140377602179712 -> 140377602260848
	140377602179712 [label="TransposeBackward0
------------------
dim0: 1
dim1: 2"]
	140377602180048 -> 140377602179712
	140377602180048 [label="ViewBackward0
-------------------------
self_sizes: (6, 100, 256)"]
	140377602180432 -> 140377602180048
	140377602180432 [label="ViewBackward0
----------------------
self_sizes: (600, 256)"]
	140377602180672 -> 140377602180432
	140377602180672 -> 140377784929792 [dir=none]
	140377784929792 [label="mat1
 (600, 256)" fillcolor=orange]
	140377602180672 -> 140377784929872 [dir=none]
	140377784929872 [label="mat2
 (256, 256)" fillcolor=orange]
	140377602180672 [label="AddmmBackward0
----------------------------
alpha       :              1
beta        :              1
mat1        : [saved tensor]
mat1_sizes  :     (600, 256)
mat1_strides:       (256, 1)
mat2        : [saved tensor]
mat2_sizes  :     (256, 256)
mat2_strides:       (1, 256)"]
	140377602180480 -> 140377602180672
	140377785588208 [label="model.decoder.layers.3.self_attn.v_proj.bias
 (256)" fillcolor=lightblue]
	140377785588208 -> 140377602180480
	140377602180480 [label=AccumulateGrad]
	140377602180624 -> 140377602180672
	140377602180624 [label="ViewBackward0
-------------------------
self_sizes: (6, 100, 256)"]
	140377787070976 -> 140377602180624
	140377602179136 -> 140377602180672
	140377602179136 [label=TBackward0]
	140377602180864 -> 140377602179136
	140377785588128 [label="model.decoder.layers.3.self_attn.v_proj.weight
 (256, 256)" fillcolor=lightblue]
	140377785588128 -> 140377602180864
	140377602180864 [label=AccumulateGrad]
	140377602257632 -> 140377787069728
	140377602257632 [label=TBackward0]
	140377602260416 -> 140377602257632
	140377785588448 [label="model.decoder.layers.3.self_attn.out_proj.weight
 (256, 256)" fillcolor=lightblue]
	140377785588448 -> 140377602260416
	140377602260416 [label=AccumulateGrad]
	140377787071840 -> 140377787047840
	140377785588608 [label="model.decoder.layers.3.self_attn_layer_norm.weight
 (256)" fillcolor=lightblue]
	140377785588608 -> 140377787071840
	140377787071840 [label=AccumulateGrad]
	140377787068864 -> 140377787047840
	140377785588688 [label="model.decoder.layers.3.self_attn_layer_norm.bias
 (256)" fillcolor=lightblue]
	140377785588688 -> 140377787068864
	140377787068864 [label=AccumulateGrad]
	140377787070160 -> 140377787047888
	140377787070160 -> 140377784930112 [dir=none]
	140377784930112 [label="other
 (6, 100, 256)" fillcolor=orange]
	140377787070160 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140377787069488 -> 140377787070160
	140377787069488 [label="ViewBackward0
----------------------
self_sizes: (600, 256)"]
	140377787071024 -> 140377787069488
	140377787071024 -> 140377784930192 [dir=none]
	140377784930192 [label="mat1
 (600, 256)" fillcolor=orange]
	140377787071024 -> 140377784929952 [dir=none]
	140377784929952 [label="mat2
 (256, 256)" fillcolor=orange]
	140377787071024 [label="AddmmBackward0
----------------------------
alpha       :              1
beta        :              1
mat1        : [saved tensor]
mat1_sizes  :     (600, 256)
mat1_strides:       (256, 1)
mat2        : [saved tensor]
mat2_sizes  :     (256, 256)
mat2_strides:       (1, 256)"]
	140377602260608 -> 140377787071024
	140377785589328 [label="model.decoder.layers.3.encoder_attn.out_proj.bias
 (256)" fillcolor=lightblue]
	140377785589328 -> 140377602260608
	140377602260608 [label=AccumulateGrad]
	140377602260128 -> 140377787071024
	140377602260128 [label="ViewBackward0
-------------------------
self_sizes: (6, 100, 256)"]
	140377602260320 -> 140377602260128
	140377602260320 [label="UnsafeViewBackward0
---------------------------
self_sizes: (6, 100, 8, 32)"]
	140377602180144 -> 140377602260320
	140377602180144 [label=CloneBackward0]
	140377602180528 -> 140377602180144
	140377602180528 [label="TransposeBackward0
------------------
dim0: 1
dim1: 2"]
	140377602180816 -> 140377602180528
	140377602180816 [label="ViewBackward0
-------------------------
self_sizes: (48, 100, 32)"]
	140377602180912 -> 140377602180816
	140377602180912 -> 140377786914272 [dir=none]
	140377786914272 [label="mat2
 (48, 782, 32)" fillcolor=orange]
	140377602180912 -> 140377786914432 [dir=none]
	140377786914432 [label="self
 (48, 100, 782)" fillcolor=orange]
	140377602180912 [label="BmmBackward0
--------------------
mat2: [saved tensor]
self: [saved tensor]"]
	140377602181008 -> 140377602180912
	140377602181008 -> 140377784930032 [dir=none]
	140377784930032 [label="result
 (48, 100, 782)" fillcolor=orange]
	140377602181008 [label="SoftmaxBackward0
----------------------------
dim   : 18446744073709551615
result:       [saved tensor]"]
	140377602181152 -> 140377602181008
	140377602181152 [label="ViewBackward0
----------------------------
self_sizes: (6, 8, 100, 782)"]
	140377602181248 -> 140377602181152
	140377602181248 [label="AddBackward0
------------
alpha: 1"]
	140377602181344 -> 140377602181248
	140377602181344 [label="ViewBackward0
--------------------------
self_sizes: (48, 100, 782)"]
	140377602181440 -> 140377602181344
	140377602181440 -> 140377786913872 [dir=none]
	140377786913872 [label="mat2
 (48, 32, 782)" fillcolor=orange]
	140377602181440 -> 140377786914192 [dir=none]
	140377786914192 [label="self
 (48, 100, 32)" fillcolor=orange]
	140377602181440 [label="BmmBackward0
--------------------
mat2: [saved tensor]
self: [saved tensor]"]
	140377602181536 -> 140377602181440
	140377602181536 [label="ViewBackward0
---------------------------
self_sizes: (6, 8, 100, 32)"]
	140377602181680 -> 140377602181536
	140377602181680 [label=CloneBackward0]
	140377602181776 -> 140377602181680
	140377602181776 [label="TransposeBackward0
------------------
dim0: 1
dim1: 2"]
	140377602181872 -> 140377602181776
	140377602181872 [label="ViewBackward0
-------------------------
self_sizes: (6, 100, 256)"]
	140377602181968 -> 140377602181872
	140377602181968 -> 140377784929712 [dir=none]
	140377784929712 [label="other
 ()" fillcolor=orange]
	140377602181968 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140377602182064 -> 140377602181968
	140377602182064 [label="ViewBackward0
----------------------
self_sizes: (600, 256)"]
	140377602182160 -> 140377602182064
	140377602182160 -> 140377784929552 [dir=none]
	140377784929552 [label="mat1
 (600, 256)" fillcolor=orange]
	140377602182160 -> 140377784928912 [dir=none]
	140377784928912 [label="mat2
 (256, 256)" fillcolor=orange]
	140377602182160 [label="AddmmBackward0
----------------------------
alpha       :              1
beta        :              1
mat1        : [saved tensor]
mat1_sizes  :     (600, 256)
mat1_strides:       (256, 1)
mat2        : [saved tensor]
mat2_sizes  :     (256, 256)
mat2_strides:       (1, 256)"]
	140377602182256 -> 140377602182160
	140377785589168 [label="model.decoder.layers.3.encoder_attn.q_proj.bias
 (256)" fillcolor=lightblue]
	140377785589168 -> 140377602182256
	140377602182256 [label=AccumulateGrad]
	140377602182208 -> 140377602182160
	140377602182208 [label="ViewBackward0
-------------------------
self_sizes: (6, 100, 256)"]
	140377602182352 -> 140377602182208
	140377602182352 [label="AddBackward0
------------
alpha: 1"]
	140377787047840 -> 140377602182352
	140377796616496 -> 140377602182352
	140377602181584 -> 140377602182160
	140377602181584 [label=TBackward0]
	140377602182496 -> 140377602181584
	140377785589088 [label="model.decoder.layers.3.encoder_attn.q_proj.weight
 (256, 256)" fillcolor=lightblue]
	140377785589088 -> 140377602182496
	140377602182496 [label=AccumulateGrad]
	140377602181488 -> 140377602181440
	140377602181488 [label="TransposeBackward0
------------------
dim0: 1
dim1: 2"]
	140377602181824 -> 140377602181488
	140377602181824 [label="ViewBackward0
---------------------------
self_sizes: (6, 8, 782, 32)"]
	140377602182016 -> 140377602181824
	140377602182016 [label=CloneBackward0]
	140377602182304 -> 140377602182016
	140377602182304 [label="TransposeBackward0
------------------
dim0: 1
dim1: 2"]
	140377602182592 -> 140377602182304
	140377602182592 [label="ViewBackward0
-------------------------
self_sizes: (6, 782, 256)"]
	140377602182400 -> 140377602182592
	140377602182400 [label="ViewBackward0
-----------------------
self_sizes: (4692, 256)"]
	140377602182688 -> 140377602182400
	140377602182688 -> 140377602105568 [dir=none]
	140377602105568 [label="mat1
 (4692, 256)" fillcolor=orange]
	140377602182688 -> 140377602105408 [dir=none]
	140377602105408 [label="mat2
 (256, 256)" fillcolor=orange]
	140377602182688 [label="AddmmBackward0
----------------------------
alpha       :              1
beta        :              1
mat1        : [saved tensor]
mat1_sizes  :    (4692, 256)
mat1_strides:       (256, 1)
mat2        : [saved tensor]
mat2_sizes  :     (256, 256)
mat2_strides:       (1, 256)"]
	140377602182784 -> 140377602182688
	140377785588848 [label="model.decoder.layers.3.encoder_attn.k_proj.bias
 (256)" fillcolor=lightblue]
	140377785588848 -> 140377602182784
	140377602182784 [label=AccumulateGrad]
	140377602182736 -> 140377602182688
	140377602182736 [label="ViewBackward0
-------------------------
self_sizes: (6, 782, 256)"]
	140377602182880 -> 140377602182736
	140377602182880 [label="AddBackward0
------------
alpha: 1"]
	140377787485632 -> 140377602182880
	140377602181632 -> 140377602182688
	140377602181632 [label=TBackward0]
	140377602183024 -> 140377602181632
	140377785588768 [label="model.decoder.layers.3.encoder_attn.k_proj.weight
 (256, 256)" fillcolor=lightblue]
	140377785588768 -> 140377602183024
	140377602183024 [label=AccumulateGrad]
	140377602180768 -> 140377602180912
	140377602180768 [label="ViewBackward0
---------------------------
self_sizes: (6, 8, 782, 32)"]
	140377602181296 -> 140377602180768
	140377602181296 [label=CloneBackward0]
	140377602181056 -> 140377602181296
	140377602181056 [label="TransposeBackward0
------------------
dim0: 1
dim1: 2"]
	140377602181920 -> 140377602181056
	140377602181920 [label="ViewBackward0
-------------------------
self_sizes: (6, 782, 256)"]
	140377602182544 -> 140377602181920
	140377602182544 [label="ViewBackward0
-----------------------
self_sizes: (4692, 256)"]
	140377602182640 -> 140377602182544
	140377602182640 -> 140377602105488 [dir=none]
	140377602105488 [label="mat1
 (4692, 256)" fillcolor=orange]
	140377602182640 -> 140377602105808 [dir=none]
	140377602105808 [label="mat2
 (256, 256)" fillcolor=orange]
	140377602182640 [label="AddmmBackward0
----------------------------
alpha       :              1
beta        :              1
mat1        : [saved tensor]
mat1_sizes  :    (4692, 256)
mat1_strides:       (256, 1)
mat2        : [saved tensor]
mat2_sizes  :     (256, 256)
mat2_strides:       (1, 256)"]
	140377602183072 -> 140377602182640
	140377785589008 [label="model.decoder.layers.3.encoder_attn.v_proj.bias
 (256)" fillcolor=lightblue]
	140377785589008 -> 140377602183072
	140377602183072 [label=AccumulateGrad]
	140377602182832 -> 140377602182640
	140377602182832 [label="ViewBackward0
-------------------------
self_sizes: (6, 782, 256)"]
	140377787485632 -> 140377602182832
	140377602181104 -> 140377602182640
	140377602181104 [label=TBackward0]
	140377602182928 -> 140377602181104
	140377785588928 [label="model.decoder.layers.3.encoder_attn.v_proj.weight
 (256, 256)" fillcolor=lightblue]
	140377785588928 -> 140377602182928
	140377602182928 [label=AccumulateGrad]
	140377602260176 -> 140377787071024
	140377602260176 [label=TBackward0]
	140377602260272 -> 140377602260176
	140377785589248 [label="model.decoder.layers.3.encoder_attn.out_proj.weight
 (256, 256)" fillcolor=lightblue]
	140377785589248 -> 140377602260272
	140377602260272 [label=AccumulateGrad]
	140377787047648 -> 140377787047696
	140377785589408 [label="model.decoder.layers.3.encoder_attn_layer_norm.weight
 (256)" fillcolor=lightblue]
	140377785589408 -> 140377787047648
	140377787047648 [label=AccumulateGrad]
	140377787047024 -> 140377787047696
	140377785589488 [label="model.decoder.layers.3.encoder_attn_layer_norm.bias
 (256)" fillcolor=lightblue]
	140377785589488 -> 140377787047024
	140377787047024 [label=AccumulateGrad]
	140377787046976 -> 140377787046352
	140377787046976 -> 140377602106048 [dir=none]
	140377602106048 [label="other
 (6, 100, 256)" fillcolor=orange]
	140377787046976 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140377787047504 -> 140377787046976
	140377787047504 [label="ViewBackward0
----------------------
self_sizes: (600, 256)"]
	140377602260512 -> 140377787047504
	140377602260512 -> 140377602106208 [dir=none]
	140377602106208 [label="mat1
 (600, 2048)" fillcolor=orange]
	140377602260512 -> 140377602105888 [dir=none]
	140377602105888 [label="mat2
 (2048, 256)" fillcolor=orange]
	140377602260512 [label="AddmmBackward0
----------------------------
alpha       :              1
beta        :              1
mat1        : [saved tensor]
mat1_sizes  :    (600, 2048)
mat1_strides:      (2048, 1)
mat2        : [saved tensor]
mat2_sizes  :    (2048, 256)
mat2_strides:      (1, 2048)"]
	140377787072320 -> 140377602260512
	140377785180304 [label="model.decoder.layers.3.fc2.bias
 (256)" fillcolor=lightblue]
	140377785180304 -> 140377787072320
	140377787072320 [label=AccumulateGrad]
	140377602180960 -> 140377602260512
	140377602180960 [label="ViewBackward0
--------------------------
self_sizes: (6, 100, 2048)"]
	140377602179520 -> 140377602180960
	140377602179520 -> 140377602106368 [dir=none]
	140377602106368 [label="result
 (6, 100, 2048)" fillcolor=orange]
	140377602179520 [label="ReluBackward0
----------------------
result: [saved tensor]"]
	140377602181728 -> 140377602179520
	140377602181728 [label="ViewBackward0
-----------------------
self_sizes: (600, 2048)"]
	140377602182448 -> 140377602181728
	140377602182448 -> 140377602106128 [dir=none]
	140377602106128 [label="mat1
 (600, 256)" fillcolor=orange]
	140377602182448 -> 140377602106448 [dir=none]
	140377602106448 [label="mat2
 (256, 2048)" fillcolor=orange]
	140377602182448 [label="AddmmBackward0
----------------------------
alpha       :              1
beta        :              1
mat1        : [saved tensor]
mat1_sizes  :     (600, 256)
mat1_strides:       (256, 1)
mat2        : [saved tensor]
mat2_sizes  :    (256, 2048)
mat2_strides:       (1, 256)"]
	140377602182976 -> 140377602182448
	140377785589648 [label="model.decoder.layers.3.fc1.bias
 (2048)" fillcolor=lightblue]
	140377785589648 -> 140377602182976
	140377602182976 [label=AccumulateGrad]
	140377602183120 -> 140377602182448
	140377602183120 [label="ViewBackward0
-------------------------
self_sizes: (6, 100, 256)"]
	140377787047696 -> 140377602183120
	140377602181200 -> 140377602182448
	140377602181200 [label=TBackward0]
	140377602306256 -> 140377602181200
	140377785589568 [label="model.decoder.layers.3.fc1.weight
 (2048, 256)" fillcolor=lightblue]
	140377785589568 -> 140377602306256
	140377602306256 [label=AccumulateGrad]
	140377602180240 -> 140377602260512
	140377602180240 [label=TBackward0]
	140377602182112 -> 140377602180240
	140377785180224 [label="model.decoder.layers.3.fc2.weight
 (256, 2048)" fillcolor=lightblue]
	140377785180224 -> 140377602182112
	140377602182112 [label=AccumulateGrad]
	140377787046496 -> 140377787045680
	140377785180384 [label="model.decoder.layers.3.final_layer_norm.weight
 (256)" fillcolor=lightblue]
	140377785180384 -> 140377787046496
	140377787046496 [label=AccumulateGrad]
	140377787047216 -> 140377787045680
	140377785180464 [label="model.decoder.layers.3.final_layer_norm.bias
 (256)" fillcolor=lightblue]
	140377785180464 -> 140377787047216
	140377787047216 [label=AccumulateGrad]
	140377787045728 -> 140377787045632
	140377787045728 -> 140377602106528 [dir=none]
	140377602106528 [label="other
 (6, 100, 256)" fillcolor=orange]
	140377787045728 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140377787046640 -> 140377787045728
	140377787046640 [label="ViewBackward0
----------------------
self_sizes: (600, 256)"]
	140377787068768 -> 140377787046640
	140377787068768 -> 140377602106688 [dir=none]
	140377602106688 [label="mat1
 (600, 256)" fillcolor=orange]
	140377787068768 -> 140377602105728 [dir=none]
	140377602105728 [label="mat2
 (256, 256)" fillcolor=orange]
	140377787068768 [label="AddmmBackward0
----------------------------
alpha       :              1
beta        :              1
mat1        : [saved tensor]
mat1_sizes  :     (600, 256)
mat1_strides:       (256, 1)
mat2        : [saved tensor]
mat2_sizes  :     (256, 256)
mat2_strides:       (1, 256)"]
	140377602179328 -> 140377787068768
	140377785181104 [label="model.decoder.layers.4.self_attn.out_proj.bias
 (256)" fillcolor=lightblue]
	140377785181104 -> 140377602179328
	140377602179328 [label=AccumulateGrad]
	140377602181392 -> 140377787068768
	140377602181392 [label="ViewBackward0
-------------------------
self_sizes: (6, 100, 256)"]
	140377602306112 -> 140377602181392
	140377602306112 [label="UnsafeViewBackward0
---------------------------
self_sizes: (6, 100, 8, 32)"]
	140377602306400 -> 140377602306112
	140377602306400 [label=CloneBackward0]
	140377602306496 -> 140377602306400
	140377602306496 [label="TransposeBackward0
------------------
dim0: 1
dim1: 2"]
	140377602306592 -> 140377602306496
	140377602306592 [label="ViewBackward0
-------------------------
self_sizes: (48, 100, 32)"]
	140377602306688 -> 140377602306592
	140377602306688 -> 140377786915312 [dir=none]
	140377786915312 [label="mat2
 (48, 100, 32)" fillcolor=orange]
	140377602306688 -> 140377786915472 [dir=none]
	140377786915472 [label="self
 (48, 100, 100)" fillcolor=orange]
	140377602306688 [label="BmmBackward0
--------------------
mat2: [saved tensor]
self: [saved tensor]"]
	140377602306784 -> 140377602306688
	140377602306784 -> 140377602105648 [dir=none]
	140377602105648 [label="result
 (48, 100, 100)" fillcolor=orange]
	140377602306784 [label="SoftmaxBackward0
----------------------------
dim   : 18446744073709551615
result:       [saved tensor]"]
	140377602306928 -> 140377602306784
	140377602306928 -> 140377786914912 [dir=none]
	140377786914912 [label="mat2
 (48, 32, 100)" fillcolor=orange]
	140377602306928 -> 140377786915232 [dir=none]
	140377786915232 [label="self
 (48, 100, 32)" fillcolor=orange]
	140377602306928 [label="BmmBackward0
--------------------
mat2: [saved tensor]
self: [saved tensor]"]
	140377602307024 -> 140377602306928
	140377602307024 [label="ViewBackward0
---------------------------
self_sizes: (6, 8, 100, 32)"]
	140377602307168 -> 140377602307024
	140377602307168 [label=CloneBackward0]
	140377602307264 -> 140377602307168
	140377602307264 [label="TransposeBackward0
------------------
dim0: 1
dim1: 2"]
	140377602307360 -> 140377602307264
	140377602307360 [label="ViewBackward0
-------------------------
self_sizes: (6, 100, 256)"]
	140377602307456 -> 140377602307360
	140377602307456 -> 140377602106848 [dir=none]
	140377602106848 [label="other
 ()" fillcolor=orange]
	140377602307456 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140377602307552 -> 140377602307456
	140377602307552 [label="ViewBackward0
----------------------
self_sizes: (600, 256)"]
	140377602307648 -> 140377602307552
	140377602307648 -> 140377602107008 [dir=none]
	140377602107008 [label="mat1
 (600, 256)" fillcolor=orange]
	140377602307648 -> 140377602106768 [dir=none]
	140377602106768 [label="mat2
 (256, 256)" fillcolor=orange]
	140377602307648 [label="AddmmBackward0
----------------------------
alpha       :              1
beta        :              1
mat1        : [saved tensor]
mat1_sizes  :     (600, 256)
mat1_strides:       (256, 1)
mat2        : [saved tensor]
mat2_sizes  :     (256, 256)
mat2_strides:       (1, 256)"]
	140377602307744 -> 140377602307648
	140377785180944 [label="model.decoder.layers.4.self_attn.q_proj.bias
 (256)" fillcolor=lightblue]
	140377785180944 -> 140377602307744
	140377602307744 [label=AccumulateGrad]
	140377602307696 -> 140377602307648
	140377602307696 [label="ViewBackward0
-------------------------
self_sizes: (6, 100, 256)"]
	140377602307840 -> 140377602307696
	140377602307840 [label="AddBackward0
------------
alpha: 1"]
	140377787045680 -> 140377602307840
	140377796616496 -> 140377602307840
	140377602307072 -> 140377602307648
	140377602307072 [label=TBackward0]
	140377602307984 -> 140377602307072
	140377785180864 [label="model.decoder.layers.4.self_attn.q_proj.weight
 (256, 256)" fillcolor=lightblue]
	140377785180864 -> 140377602307984
	140377602307984 [label=AccumulateGrad]
	140377602306976 -> 140377602306928
	140377602306976 [label="TransposeBackward0
------------------
dim0: 1
dim1: 2"]
	140377602307312 -> 140377602306976
	140377602307312 [label="ViewBackward0
---------------------------
self_sizes: (6, 8, 100, 32)"]
	140377602307504 -> 140377602307312
	140377602307504 [label=CloneBackward0]
	140377602307792 -> 140377602307504
	140377602307792 [label="TransposeBackward0
------------------
dim0: 1
dim1: 2"]
	140377602308080 -> 140377602307792
	140377602308080 [label="ViewBackward0
-------------------------
self_sizes: (6, 100, 256)"]
	140377602307888 -> 140377602308080
	140377602307888 [label="ViewBackward0
----------------------
self_sizes: (600, 256)"]
	140377602308176 -> 140377602307888
	140377602308176 -> 140377602105968 [dir=none]
	140377602105968 [label="mat1
 (600, 256)" fillcolor=orange]
	140377602308176 -> 140377602106608 [dir=none]
	140377602106608 [label="mat2
 (256, 256)" fillcolor=orange]
	140377602308176 [label="AddmmBackward0
----------------------------
alpha       :              1
beta        :              1
mat1        : [saved tensor]
mat1_sizes  :     (600, 256)
mat1_strides:       (256, 1)
mat2        : [saved tensor]
mat2_sizes  :     (256, 256)
mat2_strides:       (1, 256)"]
	140377602308272 -> 140377602308176
	140377785180624 [label="model.decoder.layers.4.self_attn.k_proj.bias
 (256)" fillcolor=lightblue]
	140377785180624 -> 140377602308272
	140377602308272 [label=AccumulateGrad]
	140377602308224 -> 140377602308176
	140377602308224 [label="ViewBackward0
-------------------------
self_sizes: (6, 100, 256)"]
	140377602307840 -> 140377602308224
	140377602307120 -> 140377602308176
	140377602307120 [label=TBackward0]
	140377602308464 -> 140377602307120
	140377785180544 [label="model.decoder.layers.4.self_attn.k_proj.weight
 (256, 256)" fillcolor=lightblue]
	140377785180544 -> 140377602308464
	140377602308464 [label=AccumulateGrad]
	140377602306736 -> 140377602306688
	140377602306736 [label="ViewBackward0
---------------------------
self_sizes: (6, 8, 100, 32)"]
	140377602307216 -> 140377602306736
	140377602307216 [label=CloneBackward0]
	140377602307600 -> 140377602307216
	140377602307600 [label="TransposeBackward0
------------------
dim0: 1
dim1: 2"]
	140377602307936 -> 140377602307600
	140377602307936 [label="ViewBackward0
-------------------------
self_sizes: (6, 100, 256)"]
	140377602308320 -> 140377602307936
	140377602308320 [label="ViewBackward0
----------------------
self_sizes: (600, 256)"]
	140377602308560 -> 140377602308320
	140377602308560 -> 140377602107168 [dir=none]
	140377602107168 [label="mat1
 (600, 256)" fillcolor=orange]
	140377602308560 -> 140377602107248 [dir=none]
	140377602107248 [label="mat2
 (256, 256)" fillcolor=orange]
	140377602308560 [label="AddmmBackward0
----------------------------
alpha       :              1
beta        :              1
mat1        : [saved tensor]
mat1_sizes  :     (600, 256)
mat1_strides:       (256, 1)
mat2        : [saved tensor]
mat2_sizes  :     (256, 256)
mat2_strides:       (1, 256)"]
	140377602308368 -> 140377602308560
	140377785180784 [label="model.decoder.layers.4.self_attn.v_proj.bias
 (256)" fillcolor=lightblue]
	140377785180784 -> 140377602308368
	140377602308368 [label=AccumulateGrad]
	140377602308512 -> 140377602308560
	140377602308512 [label="ViewBackward0
-------------------------
self_sizes: (6, 100, 256)"]
	140377787045680 -> 140377602308512
	140377602306880 -> 140377602308560
	140377602306880 [label=TBackward0]
	140377602308752 -> 140377602306880
	140377785180704 [label="model.decoder.layers.4.self_attn.v_proj.weight
 (256, 256)" fillcolor=lightblue]
	140377785180704 -> 140377602308752
	140377602308752 [label=AccumulateGrad]
	140377602180720 -> 140377787068768
	140377602180720 [label=TBackward0]
	140377602306448 -> 140377602180720
	140377785181024 [label="model.decoder.layers.4.self_attn.out_proj.weight
 (256, 256)" fillcolor=lightblue]
	140377785181024 -> 140377602306448
	140377602306448 [label=AccumulateGrad]
	140377787047408 -> 140377787044912
	140377785181184 [label="model.decoder.layers.4.self_attn_layer_norm.weight
 (256)" fillcolor=lightblue]
	140377785181184 -> 140377787047408
	140377787047408 [label=AccumulateGrad]
	140377787045056 -> 140377787044912
	140377785181264 [label="model.decoder.layers.4.self_attn_layer_norm.bias
 (256)" fillcolor=lightblue]
	140377785181264 -> 140377787045056
	140377787045056 [label=AccumulateGrad]
	140377787045296 -> 140377787044816
	140377787045296 -> 140377602107488 [dir=none]
	140377602107488 [label="other
 (6, 100, 256)" fillcolor=orange]
	140377787045296 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140377787046592 -> 140377787045296
	140377787046592 [label="ViewBackward0
----------------------
self_sizes: (600, 256)"]
	140377787072368 -> 140377787046592
	140377787072368 -> 140377602107568 [dir=none]
	140377602107568 [label="mat1
 (600, 256)" fillcolor=orange]
	140377787072368 -> 140377602107328 [dir=none]
	140377602107328 [label="mat2
 (256, 256)" fillcolor=orange]
	140377787072368 [label="AddmmBackward0
----------------------------
alpha       :              1
beta        :              1
mat1        : [saved tensor]
mat1_sizes  :     (600, 256)
mat1_strides:       (256, 1)
mat2        : [saved tensor]
mat2_sizes  :     (256, 256)
mat2_strides:       (1, 256)"]
	140377602306640 -> 140377787072368
	140377785181904 [label="model.decoder.layers.4.encoder_attn.out_proj.bias
 (256)" fillcolor=lightblue]
	140377785181904 -> 140377602306640
	140377602306640 [label=AccumulateGrad]
	140377602306160 -> 140377787072368
	140377602306160 [label="ViewBackward0
-------------------------
self_sizes: (6, 100, 256)"]
	140377602306352 -> 140377602306160
	140377602306352 [label="UnsafeViewBackward0
---------------------------
self_sizes: (6, 100, 8, 32)"]
	140377602308032 -> 140377602306352
	140377602308032 [label=CloneBackward0]
	140377602308416 -> 140377602308032
	140377602308416 [label="TransposeBackward0
------------------
dim0: 1
dim1: 2"]
	140377602308704 -> 140377602308416
	140377602308704 [label="ViewBackward0
-------------------------
self_sizes: (48, 100, 32)"]
	140377602308800 -> 140377602308704
	140377602308800 -> 140377786916272 [dir=none]
	140377786916272 [label="mat2
 (48, 782, 32)" fillcolor=orange]
	140377602308800 -> 140377786916432 [dir=none]
	140377786916432 [label="self
 (48, 100, 782)" fillcolor=orange]
	140377602308800 [label="BmmBackward0
--------------------
mat2: [saved tensor]
self: [saved tensor]"]
	140377602308896 -> 140377602308800
	140377602308896 -> 140377602106928 [dir=none]
	140377602106928 [label="result
 (48, 100, 782)" fillcolor=orange]
	140377602308896 [label="SoftmaxBackward0
----------------------------
dim   : 18446744073709551615
result:       [saved tensor]"]
	140377602309040 -> 140377602308896
	140377602309040 [label="ViewBackward0
----------------------------
self_sizes: (6, 8, 100, 782)"]
	140377602309136 -> 140377602309040
	140377602309136 [label="AddBackward0
------------
alpha: 1"]
	140377602309232 -> 140377602309136
	140377602309232 [label="ViewBackward0
--------------------------
self_sizes: (48, 100, 782)"]
	140377602309328 -> 140377602309232
	140377602309328 -> 140377786915872 [dir=none]
	140377786915872 [label="mat2
 (48, 32, 782)" fillcolor=orange]
	140377602309328 -> 140377786916192 [dir=none]
	140377786916192 [label="self
 (48, 100, 32)" fillcolor=orange]
	140377602309328 [label="BmmBackward0
--------------------
mat2: [saved tensor]
self: [saved tensor]"]
	140377602309424 -> 140377602309328
	140377602309424 [label="ViewBackward0
---------------------------
self_sizes: (6, 8, 100, 32)"]
	140377602309568 -> 140377602309424
	140377602309568 [label=CloneBackward0]
	140377602309664 -> 140377602309568
	140377602309664 [label="TransposeBackward0
------------------
dim0: 1
dim1: 2"]
	140377602309760 -> 140377602309664
	140377602309760 [label="ViewBackward0
-------------------------
self_sizes: (6, 100, 256)"]
	140377602309856 -> 140377602309760
	140377602309856 -> 140377602107808 [dir=none]
	140377602107808 [label="other
 ()" fillcolor=orange]
	140377602309856 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140377602309952 -> 140377602309856
	140377602309952 [label="ViewBackward0
----------------------
self_sizes: (600, 256)"]
	140377602310048 -> 140377602309952
	140377602310048 -> 140377602107888 [dir=none]
	140377602107888 [label="mat1
 (600, 256)" fillcolor=orange]
	140377602310048 -> 140377602106288 [dir=none]
	140377602106288 [label="mat2
 (256, 256)" fillcolor=orange]
	140377602310048 [label="AddmmBackward0
----------------------------
alpha       :              1
beta        :              1
mat1        : [saved tensor]
mat1_sizes  :     (600, 256)
mat1_strides:       (256, 1)
mat2        : [saved tensor]
mat2_sizes  :     (256, 256)
mat2_strides:       (1, 256)"]
	140377602310096 -> 140377602310048
	140377785181744 [label="model.decoder.layers.4.encoder_attn.q_proj.bias
 (256)" fillcolor=lightblue]
	140377785181744 -> 140377602310096
	140377602310096 [label=AccumulateGrad]
	140377602309472 -> 140377602310048
	140377602309472 [label="ViewBackward0
-------------------------
self_sizes: (6, 100, 256)"]
	140377602085024 -> 140377602309472
	140377602085024 [label="AddBackward0
------------
alpha: 1"]
	140377787044912 -> 140377602085024
	140377796616496 -> 140377602085024
	140377602084928 -> 140377602310048
	140377602084928 [label=TBackward0]
	140377602085168 -> 140377602084928
	140377785181664 [label="model.decoder.layers.4.encoder_attn.q_proj.weight
 (256, 256)" fillcolor=lightblue]
	140377785181664 -> 140377602085168
	140377602085168 [label=AccumulateGrad]
	140377602309376 -> 140377602309328
	140377602309376 [label="TransposeBackward0
------------------
dim0: 1
dim1: 2"]
	140377602309712 -> 140377602309376
	140377602309712 [label="ViewBackward0
---------------------------
self_sizes: (6, 8, 782, 32)"]
	140377602309904 -> 140377602309712
	140377602309904 [label=CloneBackward0]
	140377602310000 -> 140377602309904
	140377602310000 [label="TransposeBackward0
------------------
dim0: 1
dim1: 2"]
	140377602085264 -> 140377602310000
	140377602085264 [label="ViewBackward0
-------------------------
self_sizes: (6, 782, 256)"]
	140377602085072 -> 140377602085264
	140377602085072 [label="ViewBackward0
-----------------------
self_sizes: (4692, 256)"]
	140377602085360 -> 140377602085072
	140377602085360 -> 140377602107408 [dir=none]
	140377602107408 [label="mat1
 (4692, 256)" fillcolor=orange]
	140377602085360 -> 140377602107728 [dir=none]
	140377602107728 [label="mat2
 (256, 256)" fillcolor=orange]
	140377602085360 [label="AddmmBackward0
----------------------------
alpha       :              1
beta        :              1
mat1        : [saved tensor]
mat1_sizes  :    (4692, 256)
mat1_strides:       (256, 1)
mat2        : [saved tensor]
mat2_sizes  :     (256, 256)
mat2_strides:       (1, 256)"]
	140377602085456 -> 140377602085360
	140377785181424 [label="model.decoder.layers.4.encoder_attn.k_proj.bias
 (256)" fillcolor=lightblue]
	140377785181424 -> 140377602085456
	140377602085456 [label=AccumulateGrad]
	140377602085408 -> 140377602085360
	140377602085408 [label="ViewBackward0
-------------------------
self_sizes: (6, 782, 256)"]
	140377602085552 -> 140377602085408
	140377602085552 [label="AddBackward0
------------
alpha: 1"]
	140377787485632 -> 140377602085552
	140377602084976 -> 140377602085360
	140377602084976 [label=TBackward0]
	140377602085696 -> 140377602084976
	140377785181344 [label="model.decoder.layers.4.encoder_attn.k_proj.weight
 (256, 256)" fillcolor=lightblue]
	140377785181344 -> 140377602085696
	140377602085696 [label=AccumulateGrad]
	140377602308656 -> 140377602308800
	140377602308656 [label="ViewBackward0
---------------------------
self_sizes: (6, 8, 782, 32)"]
	140377602309184 -> 140377602308656
	140377602309184 [label=CloneBackward0]
	140377602308944 -> 140377602309184
	140377602308944 [label="TransposeBackward0
------------------
dim0: 1
dim1: 2"]
	140377602309808 -> 140377602308944
	140377602309808 [label="ViewBackward0
-------------------------
self_sizes: (6, 782, 256)"]
	140377602309520 -> 140377602309808
	140377602309520 [label="ViewBackward0
-----------------------
self_sizes: (4692, 256)"]
	140377602085312 -> 140377602309520
	140377602085312 -> 140377602108048 [dir=none]
	140377602108048 [label="mat1
 (4692, 256)" fillcolor=orange]
	140377602085312 -> 140377602108128 [dir=none]
	140377602108128 [label="mat2
 (256, 256)" fillcolor=orange]
	140377602085312 [label="AddmmBackward0
----------------------------
alpha       :              1
beta        :              1
mat1        : [saved tensor]
mat1_sizes  :    (4692, 256)
mat1_strides:       (256, 1)
mat2        : [saved tensor]
mat2_sizes  :     (256, 256)
mat2_strides:       (1, 256)"]
	140377602085744 -> 140377602085312
	140377785181584 [label="model.decoder.layers.4.encoder_attn.v_proj.bias
 (256)" fillcolor=lightblue]
	140377785181584 -> 140377602085744
	140377602085744 [label=AccumulateGrad]
	140377602085504 -> 140377602085312
	140377602085504 [label="ViewBackward0
-------------------------
self_sizes: (6, 782, 256)"]
	140377787485632 -> 140377602085504
	140377602085216 -> 140377602085312
	140377602085216 [label=TBackward0]
	140377602085840 -> 140377602085216
	140377785181504 [label="model.decoder.layers.4.encoder_attn.v_proj.weight
 (256, 256)" fillcolor=lightblue]
	140377785181504 -> 140377602085840
	140377602085840 [label=AccumulateGrad]
	140377602306208 -> 140377787072368
	140377602306208 [label=TBackward0]
	140377602308128 -> 140377602306208
	140377785181824 [label="model.decoder.layers.4.encoder_attn.out_proj.weight
 (256, 256)" fillcolor=lightblue]
	140377785181824 -> 140377602308128
	140377602308128 [label=AccumulateGrad]
	140377787045488 -> 140377787044624
	140377785181984 [label="model.decoder.layers.4.encoder_attn_layer_norm.weight
 (256)" fillcolor=lightblue]
	140377785181984 -> 140377787045488
	140377787045488 [label=AccumulateGrad]
	140377787046832 -> 140377787044624
	140377785182064 [label="model.decoder.layers.4.encoder_attn_layer_norm.bias
 (256)" fillcolor=lightblue]
	140377785182064 -> 140377787046832
	140377787046832 [label=AccumulateGrad]
	140377787044768 -> 140377787044240
	140377787044768 -> 140377602108368 [dir=none]
	140377602108368 [label="other
 (6, 100, 256)" fillcolor=orange]
	140377787044768 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140377787045776 -> 140377787044768
	140377787045776 [label="ViewBackward0
----------------------
self_sizes: (600, 256)"]
	140377787046304 -> 140377787045776
	140377787046304 -> 140377602108448 [dir=none]
	140377602108448 [label="mat1
 (600, 2048)" fillcolor=orange]
	140377787046304 -> 140377602108208 [dir=none]
	140377602108208 [label="mat2
 (2048, 256)" fillcolor=orange]
	140377787046304 [label="AddmmBackward0
----------------------------
alpha       :              1
beta        :              1
mat1        : [saved tensor]
mat1_sizes  :    (600, 2048)
mat1_strides:      (2048, 1)
mat2        : [saved tensor]
mat2_sizes  :    (2048, 256)
mat2_strides:      (1, 2048)"]
	140377602308848 -> 140377787046304
	140377785182384 [label="model.decoder.layers.4.fc2.bias
 (256)" fillcolor=lightblue]
	140377785182384 -> 140377602308848
	140377602308848 [label=AccumulateGrad]
	140377602307408 -> 140377787046304
	140377602307408 [label="ViewBackward0
--------------------------
self_sizes: (6, 100, 2048)"]
	140377602306304 -> 140377602307408
	140377602306304 -> 140377602108608 [dir=none]
	140377602108608 [label="result
 (6, 100, 2048)" fillcolor=orange]
	140377602306304 [label="ReluBackward0
----------------------
result: [saved tensor]"]
	140377602309616 -> 140377602306304
	140377602309616 [label="ViewBackward0
-----------------------
self_sizes: (600, 2048)"]
	140377602308992 -> 140377602309616
	140377602308992 -> 140377602107968 [dir=none]
	140377602107968 [label="mat1
 (600, 256)" fillcolor=orange]
	140377602308992 -> 140377602108688 [dir=none]
	140377602108688 [label="mat2
 (256, 2048)" fillcolor=orange]
	140377602308992 [label="AddmmBackward0
----------------------------
alpha       :              1
beta        :              1
mat1        : [saved tensor]
mat1_sizes  :     (600, 256)
mat1_strides:       (256, 1)
mat2        : [saved tensor]
mat2_sizes  :    (256, 2048)
mat2_strides:       (1, 256)"]
	140377602085600 -> 140377602308992
	140377785182224 [label="model.decoder.layers.4.fc1.bias
 (2048)" fillcolor=lightblue]
	140377785182224 -> 140377602085600
	140377602085600 [label=AccumulateGrad]
	140377602085792 -> 140377602308992
	140377602085792 [label="ViewBackward0
-------------------------
self_sizes: (6, 100, 256)"]
	140377787044624 -> 140377602085792
	140377602085120 -> 140377602308992
	140377602085120 [label=TBackward0]
	140377602085984 -> 140377602085120
	140377785182144 [label="model.decoder.layers.4.fc1.weight
 (2048, 256)" fillcolor=lightblue]
	140377785182144 -> 140377602085984
	140377602085984 [label=AccumulateGrad]
	140377602306544 -> 140377787046304
	140377602306544 [label=TBackward0]
	140377602309088 -> 140377602306544
	140377785182304 [label="model.decoder.layers.4.fc2.weight
 (256, 2048)" fillcolor=lightblue]
	140377785182304 -> 140377602309088
	140377602309088 [label=AccumulateGrad]
	140377787044384 -> 140377787045968
	140377785182464 [label="model.decoder.layers.4.final_layer_norm.weight
 (256)" fillcolor=lightblue]
	140377785182464 -> 140377787044384
	140377787044384 [label=AccumulateGrad]
	140377787044576 -> 140377787045968
	140377785182544 [label="model.decoder.layers.4.final_layer_norm.bias
 (256)" fillcolor=lightblue]
	140377785182544 -> 140377787044576
	140377787044576 [label=AccumulateGrad]
	140377787046544 -> 140377787047456
	140377787046544 -> 140377602108768 [dir=none]
	140377602108768 [label="other
 (6, 100, 256)" fillcolor=orange]
	140377787046544 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140377787047168 -> 140377787046544
	140377787047168 [label="ViewBackward0
----------------------
self_sizes: (600, 256)"]
	140377787045440 -> 140377787047168
	140377787045440 -> 140377602108928 [dir=none]
	140377602108928 [label="mat1
 (600, 256)" fillcolor=orange]
	140377787045440 -> 140377602107648 [dir=none]
	140377602107648 [label="mat2
 (256, 256)" fillcolor=orange]
	140377787045440 [label="AddmmBackward0
----------------------------
alpha       :              1
beta        :              1
mat1        : [saved tensor]
mat1_sizes  :     (600, 256)
mat1_strides:       (256, 1)
mat2        : [saved tensor]
mat2_sizes  :     (256, 256)
mat2_strides:       (1, 256)"]
	140377602306832 -> 140377787045440
	140377785183184 [label="model.decoder.layers.5.self_attn.out_proj.bias
 (256)" fillcolor=lightblue]
	140377785183184 -> 140377602306832
	140377602306832 [label=AccumulateGrad]
	140377602309280 -> 140377787045440
	140377602309280 [label="ViewBackward0
-------------------------
self_sizes: (6, 100, 256)"]
	140377602085936 -> 140377602309280
	140377602085936 [label="UnsafeViewBackward0
---------------------------
self_sizes: (6, 100, 8, 32)"]
	140377602086128 -> 140377602085936
	140377602086128 [label=CloneBackward0]
	140377602086224 -> 140377602086128
	140377602086224 [label="TransposeBackward0
------------------
dim0: 1
dim1: 2"]
	140377602086320 -> 140377602086224
	140377602086320 [label="ViewBackward0
-------------------------
self_sizes: (48, 100, 32)"]
	140377602086416 -> 140377602086320
	140377602086416 -> 140377786896928 [dir=none]
	140377786896928 [label="mat2
 (48, 100, 32)" fillcolor=orange]
	140377602086416 -> 140377786897088 [dir=none]
	140377786897088 [label="self
 (48, 100, 100)" fillcolor=orange]
	140377602086416 [label="BmmBackward0
--------------------
mat2: [saved tensor]
self: [saved tensor]"]
	140377602086512 -> 140377602086416
	140377602086512 -> 140377602107088 [dir=none]
	140377602107088 [label="result
 (48, 100, 100)" fillcolor=orange]
	140377602086512 [label="SoftmaxBackward0
----------------------------
dim   : 18446744073709551615
result:       [saved tensor]"]
	140377602086656 -> 140377602086512
	140377602086656 -> 140377786896528 [dir=none]
	140377786896528 [label="mat2
 (48, 32, 100)" fillcolor=orange]
	140377602086656 -> 140377786896848 [dir=none]
	140377786896848 [label="self
 (48, 100, 32)" fillcolor=orange]
	140377602086656 [label="BmmBackward0
--------------------
mat2: [saved tensor]
self: [saved tensor]"]
	140377602086752 -> 140377602086656
	140377602086752 [label="ViewBackward0
---------------------------
self_sizes: (6, 8, 100, 32)"]
	140377602086896 -> 140377602086752
	140377602086896 [label=CloneBackward0]
	140377602086992 -> 140377602086896
	140377602086992 [label="TransposeBackward0
------------------
dim0: 1
dim1: 2"]
	140377602087088 -> 140377602086992
	140377602087088 [label="ViewBackward0
-------------------------
self_sizes: (6, 100, 256)"]
	140377602087184 -> 140377602087088
	140377602087184 -> 140377602109088 [dir=none]
	140377602109088 [label="other
 ()" fillcolor=orange]
	140377602087184 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140377602087280 -> 140377602087184
	140377602087280 [label="ViewBackward0
----------------------
self_sizes: (600, 256)"]
	140377602087376 -> 140377602087280
	140377602087376 -> 140377602109248 [dir=none]
	140377602109248 [label="mat1
 (600, 256)" fillcolor=orange]
	140377602087376 -> 140377602109008 [dir=none]
	140377602109008 [label="mat2
 (256, 256)" fillcolor=orange]
	140377602087376 [label="AddmmBackward0
----------------------------
alpha       :              1
beta        :              1
mat1        : [saved tensor]
mat1_sizes  :     (600, 256)
mat1_strides:       (256, 1)
mat2        : [saved tensor]
mat2_sizes  :     (256, 256)
mat2_strides:       (1, 256)"]
	140377602087472 -> 140377602087376
	140377785183024 [label="model.decoder.layers.5.self_attn.q_proj.bias
 (256)" fillcolor=lightblue]
	140377785183024 -> 140377602087472
	140377602087472 [label=AccumulateGrad]
	140377602087424 -> 140377602087376
	140377602087424 [label="ViewBackward0
-------------------------
self_sizes: (6, 100, 256)"]
	140377602087568 -> 140377602087424
	140377602087568 [label="AddBackward0
------------
alpha: 1"]
	140377787045968 -> 140377602087568
	140377796616496 -> 140377602087568
	140377602086800 -> 140377602087376
	140377602086800 [label=TBackward0]
	140377602087712 -> 140377602086800
	140377785182944 [label="model.decoder.layers.5.self_attn.q_proj.weight
 (256, 256)" fillcolor=lightblue]
	140377785182944 -> 140377602087712
	140377602087712 [label=AccumulateGrad]
	140377602086704 -> 140377602086656
	140377602086704 [label="TransposeBackward0
------------------
dim0: 1
dim1: 2"]
	140377602087040 -> 140377602086704
	140377602087040 [label="ViewBackward0
---------------------------
self_sizes: (6, 8, 100, 32)"]
	140377602087232 -> 140377602087040
	140377602087232 [label=CloneBackward0]
	140377602087520 -> 140377602087232
	140377602087520 [label="TransposeBackward0
------------------
dim0: 1
dim1: 2"]
	140377602087808 -> 140377602087520
	140377602087808 [label="ViewBackward0
-------------------------
self_sizes: (6, 100, 256)"]
	140377602087616 -> 140377602087808
	140377602087616 [label="ViewBackward0
----------------------
self_sizes: (600, 256)"]
	140377602087904 -> 140377602087616
	140377602087904 -> 140377602108288 [dir=none]
	140377602108288 [label="mat1
 (600, 256)" fillcolor=orange]
	140377602087904 -> 140377602108848 [dir=none]
	140377602108848 [label="mat2
 (256, 256)" fillcolor=orange]
	140377602087904 [label="AddmmBackward0
----------------------------
alpha       :              1
beta        :              1
mat1        : [saved tensor]
mat1_sizes  :     (600, 256)
mat1_strides:       (256, 1)
mat2        : [saved tensor]
mat2_sizes  :     (256, 256)
mat2_strides:       (1, 256)"]
	140377602088000 -> 140377602087904
	140377785182704 [label="model.decoder.layers.5.self_attn.k_proj.bias
 (256)" fillcolor=lightblue]
	140377785182704 -> 140377602088000
	140377602088000 [label=AccumulateGrad]
	140377602087952 -> 140377602087904
	140377602087952 [label="ViewBackward0
-------------------------
self_sizes: (6, 100, 256)"]
	140377602087568 -> 140377602087952
	140377602086848 -> 140377602087904
	140377602086848 [label=TBackward0]
	140377602088192 -> 140377602086848
	140377785182624 [label="model.decoder.layers.5.self_attn.k_proj.weight
 (256, 256)" fillcolor=lightblue]
	140377785182624 -> 140377602088192
	140377602088192 [label=AccumulateGrad]
	140377602086464 -> 140377602086416
	140377602086464 [label="ViewBackward0
---------------------------
self_sizes: (6, 8, 100, 32)"]
	140377602086944 -> 140377602086464
	140377602086944 [label=CloneBackward0]
	140377602087328 -> 140377602086944
	140377602087328 [label="TransposeBackward0
------------------
dim0: 1
dim1: 2"]
	140377602087664 -> 140377602087328
	140377602087664 [label="ViewBackward0
-------------------------
self_sizes: (6, 100, 256)"]
	140377602088048 -> 140377602087664
	140377602088048 [label="ViewBackward0
----------------------
self_sizes: (600, 256)"]
	140377602088288 -> 140377602088048
	140377602088288 -> 140377602109328 [dir=none]
	140377602109328 [label="mat1
 (600, 256)" fillcolor=orange]
	140377602088288 -> 140377602109168 [dir=none]
	140377602109168 [label="mat2
 (256, 256)" fillcolor=orange]
	140377602088288 [label="AddmmBackward0
----------------------------
alpha       :              1
beta        :              1
mat1        : [saved tensor]
mat1_sizes  :     (600, 256)
mat1_strides:       (256, 1)
mat2        : [saved tensor]
mat2_sizes  :     (256, 256)
mat2_strides:       (1, 256)"]
	140377602088096 -> 140377602088288
	140377785182864 [label="model.decoder.layers.5.self_attn.v_proj.bias
 (256)" fillcolor=lightblue]
	140377785182864 -> 140377602088096
	140377602088096 [label=AccumulateGrad]
	140377602088240 -> 140377602088288
	140377602088240 [label="ViewBackward0
-------------------------
self_sizes: (6, 100, 256)"]
	140377787045968 -> 140377602088240
	140377602086608 -> 140377602088288
	140377602086608 [label=TBackward0]
	140377602088480 -> 140377602086608
	140377785182784 [label="model.decoder.layers.5.self_attn.v_proj.weight
 (256, 256)" fillcolor=lightblue]
	140377785182784 -> 140377602088480
	140377602088480 [label=AccumulateGrad]
	140377602308608 -> 140377787045440
	140377602308608 [label=TBackward0]
	140377602086176 -> 140377602308608
	140377785183104 [label="model.decoder.layers.5.self_attn.out_proj.weight
 (256, 256)" fillcolor=lightblue]
	140377785183104 -> 140377602086176
	140377602086176 [label=AccumulateGrad]
	140377787044864 -> 140377787044192
	140377785183264 [label="model.decoder.layers.5.self_attn_layer_norm.weight
 (256)" fillcolor=lightblue]
	140377785183264 -> 140377787044864
	140377787044864 [label=AccumulateGrad]
	140377787044048 -> 140377787044192
	140377785183344 [label="model.decoder.layers.5.self_attn_layer_norm.bias
 (256)" fillcolor=lightblue]
	140377785183344 -> 140377787044048
	140377787044048 [label=AccumulateGrad]
	140377787043952 -> 140377787018736
	140377787043952 -> 140377602108528 [dir=none]
	140377602108528 [label="other
 (6, 100, 256)" fillcolor=orange]
	140377787043952 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140377787045248 -> 140377787043952
	140377787045248 [label="ViewBackward0
----------------------
self_sizes: (600, 256)"]
	140377787046784 -> 140377787045248
	140377787046784 -> 140377602253024 [dir=none]
	140377602253024 [label="mat1
 (600, 256)" fillcolor=orange]
	140377787046784 -> 140377602252864 [dir=none]
	140377602252864 [label="mat2
 (256, 256)" fillcolor=orange]
	140377787046784 [label="AddmmBackward0
----------------------------
alpha       :              1
beta        :              1
mat1        : [saved tensor]
mat1_sizes  :     (600, 256)
mat1_strides:       (256, 1)
mat2        : [saved tensor]
mat2_sizes  :     (256, 256)
mat2_strides:       (1, 256)"]
	140377602086368 -> 140377787046784
	140377785183984 [label="model.decoder.layers.5.encoder_attn.out_proj.bias
 (256)" fillcolor=lightblue]
	140377785183984 -> 140377602086368
	140377602086368 [label=AccumulateGrad]
	140377602085888 -> 140377787046784
	140377602085888 [label="ViewBackward0
-------------------------
self_sizes: (6, 100, 256)"]
	140377602086080 -> 140377602085888
	140377602086080 [label="UnsafeViewBackward0
---------------------------
self_sizes: (6, 100, 8, 32)"]
	140377602087760 -> 140377602086080
	140377602087760 [label=CloneBackward0]
	140377602088144 -> 140377602087760
	140377602088144 [label="TransposeBackward0
------------------
dim0: 1
dim1: 2"]
	140377602088432 -> 140377602088144
	140377602088432 [label="ViewBackward0
-------------------------
self_sizes: (48, 100, 32)"]
	140377602088528 -> 140377602088432
	140377602088528 -> 140377786897888 [dir=none]
	140377786897888 [label="mat2
 (48, 782, 32)" fillcolor=orange]
	140377602088528 -> 140377786898048 [dir=none]
	140377786898048 [label="self
 (48, 100, 782)" fillcolor=orange]
	140377602088528 [label="BmmBackward0
--------------------
mat2: [saved tensor]
self: [saved tensor]"]
	140377602088624 -> 140377602088528
	140377602088624 -> 140377602252944 [dir=none]
	140377602252944 [label="result
 (48, 100, 782)" fillcolor=orange]
	140377602088624 [label="SoftmaxBackward0
----------------------------
dim   : 18446744073709551615
result:       [saved tensor]"]
	140377602088768 -> 140377602088624
	140377602088768 [label="ViewBackward0
----------------------------
self_sizes: (6, 8, 100, 782)"]
	140377602088864 -> 140377602088768
	140377602088864 [label="AddBackward0
------------
alpha: 1"]
	140377602088912 -> 140377602088864
	140377602088912 [label="ViewBackward0
--------------------------
self_sizes: (48, 100, 782)"]
	140377602220192 -> 140377602088912
	140377602220192 -> 140377786897488 [dir=none]
	140377786897488 [label="mat2
 (48, 32, 782)" fillcolor=orange]
	140377602220192 -> 140377786897808 [dir=none]
	140377786897808 [label="self
 (48, 100, 32)" fillcolor=orange]
	140377602220192 [label="BmmBackward0
--------------------
mat2: [saved tensor]
self: [saved tensor]"]
	140377602220288 -> 140377602220192
	140377602220288 [label="ViewBackward0
---------------------------
self_sizes: (6, 8, 100, 32)"]
	140377602220432 -> 140377602220288
	140377602220432 [label=CloneBackward0]
	140377602220528 -> 140377602220432
	140377602220528 [label="TransposeBackward0
------------------
dim0: 1
dim1: 2"]
	140377602220624 -> 140377602220528
	140377602220624 [label="ViewBackward0
-------------------------
self_sizes: (6, 100, 256)"]
	140377602220720 -> 140377602220624
	140377602220720 -> 140377602253424 [dir=none]
	140377602253424 [label="other
 ()" fillcolor=orange]
	140377602220720 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140377602220816 -> 140377602220720
	140377602220816 [label="ViewBackward0
----------------------
self_sizes: (600, 256)"]
	140377602220912 -> 140377602220816
	140377602220912 -> 140377602253584 [dir=none]
	140377602253584 [label="mat1
 (600, 256)" fillcolor=orange]
	140377602220912 -> 140377602253264 [dir=none]
	140377602253264 [label="mat2
 (256, 256)" fillcolor=orange]
	140377602220912 [label="AddmmBackward0
----------------------------
alpha       :              1
beta        :              1
mat1        : [saved tensor]
mat1_sizes  :     (600, 256)
mat1_strides:       (256, 1)
mat2        : [saved tensor]
mat2_sizes  :     (256, 256)
mat2_strides:       (1, 256)"]
	140377602221008 -> 140377602220912
	140377785183824 [label="model.decoder.layers.5.encoder_attn.q_proj.bias
 (256)" fillcolor=lightblue]
	140377785183824 -> 140377602221008
	140377602221008 [label=AccumulateGrad]
	140377602220960 -> 140377602220912
	140377602220960 [label="ViewBackward0
-------------------------
self_sizes: (6, 100, 256)"]
	140377602221104 -> 140377602220960
	140377602221104 [label="AddBackward0
------------
alpha: 1"]
	140377787044192 -> 140377602221104
	140377796616496 -> 140377602221104
	140377602220336 -> 140377602220912
	140377602220336 [label=TBackward0]
	140377602221248 -> 140377602220336
	140377785183744 [label="model.decoder.layers.5.encoder_attn.q_proj.weight
 (256, 256)" fillcolor=lightblue]
	140377785183744 -> 140377602221248
	140377602221248 [label=AccumulateGrad]
	140377602220240 -> 140377602220192
	140377602220240 [label="TransposeBackward0
------------------
dim0: 1
dim1: 2"]
	140377602220576 -> 140377602220240
	140377602220576 [label="ViewBackward0
---------------------------
self_sizes: (6, 8, 782, 32)"]
	140377602220768 -> 140377602220576
	140377602220768 [label=CloneBackward0]
	140377602221056 -> 140377602220768
	140377602221056 [label="TransposeBackward0
------------------
dim0: 1
dim1: 2"]
	140377602221344 -> 140377602221056
	140377602221344 [label="ViewBackward0
-------------------------
self_sizes: (6, 782, 256)"]
	140377602221152 -> 140377602221344
	140377602221152 [label="ViewBackward0
-----------------------
self_sizes: (4692, 256)"]
	140377602221440 -> 140377602221152
	140377602221440 -> 140377602253504 [dir=none]
	140377602253504 [label="mat1
 (4692, 256)" fillcolor=orange]
	140377602221440 -> 140377602253344 [dir=none]
	140377602253344 [label="mat2
 (256, 256)" fillcolor=orange]
	140377602221440 [label="AddmmBackward0
----------------------------
alpha       :              1
beta        :              1
mat1        : [saved tensor]
mat1_sizes  :    (4692, 256)
mat1_strides:       (256, 1)
mat2        : [saved tensor]
mat2_sizes  :     (256, 256)
mat2_strides:       (1, 256)"]
	140377602221536 -> 140377602221440
	140377785183504 [label="model.decoder.layers.5.encoder_attn.k_proj.bias
 (256)" fillcolor=lightblue]
	140377785183504 -> 140377602221536
	140377602221536 [label=AccumulateGrad]
	140377602221488 -> 140377602221440
	140377602221488 [label="ViewBackward0
-------------------------
self_sizes: (6, 782, 256)"]
	140377602221632 -> 140377602221488
	140377602221632 [label="AddBackward0
------------
alpha: 1"]
	140377787485632 -> 140377602221632
	140377602220384 -> 140377602221440
	140377602220384 [label=TBackward0]
	140377602221776 -> 140377602220384
	140377785183424 [label="model.decoder.layers.5.encoder_attn.k_proj.weight
 (256, 256)" fillcolor=lightblue]
	140377785183424 -> 140377602221776
	140377602221776 [label=AccumulateGrad]
	140377602088384 -> 140377602088528
	140377602088384 [label="ViewBackward0
---------------------------
self_sizes: (6, 8, 782, 32)"]
	140377602088672 -> 140377602088384
	140377602088672 [label=CloneBackward0]
	140377602088720 -> 140377602088672
	140377602088720 [label="TransposeBackward0
------------------
dim0: 1
dim1: 2"]
	140377602220672 -> 140377602088720
	140377602220672 [label="ViewBackward0
-------------------------
self_sizes: (6, 782, 256)"]
	140377602221296 -> 140377602220672
	140377602221296 [label="ViewBackward0
-----------------------
self_sizes: (4692, 256)"]
	140377602221392 -> 140377602221296
	140377602221392 -> 140377602253744 [dir=none]
	140377602253744 [label="mat1
 (4692, 256)" fillcolor=orange]
	140377602221392 -> 140377602253824 [dir=none]
	140377602253824 [label="mat2
 (256, 256)" fillcolor=orange]
	140377602221392 [label="AddmmBackward0
----------------------------
alpha       :              1
beta        :              1
mat1        : [saved tensor]
mat1_sizes  :    (4692, 256)
mat1_strides:       (256, 1)
mat2        : [saved tensor]
mat2_sizes  :     (256, 256)
mat2_strides:       (1, 256)"]
	140377602221824 -> 140377602221392
	140377785183664 [label="model.decoder.layers.5.encoder_attn.v_proj.bias
 (256)" fillcolor=lightblue]
	140377785183664 -> 140377602221824
	140377602221824 [label=AccumulateGrad]
	140377602221584 -> 140377602221392
	140377602221584 [label="ViewBackward0
-------------------------
self_sizes: (6, 782, 256)"]
	140377787485632 -> 140377602221584
	140377602220144 -> 140377602221392
	140377602220144 [label=TBackward0]
	140377602221920 -> 140377602220144
	140377785183584 [label="model.decoder.layers.5.encoder_attn.v_proj.weight
 (256, 256)" fillcolor=lightblue]
	140377785183584 -> 140377602221920
	140377602221920 [label=AccumulateGrad]
	140377602085648 -> 140377787046784
	140377602085648 [label=TBackward0]
	140377602087856 -> 140377602085648
	140377785183904 [label="model.decoder.layers.5.encoder_attn.out_proj.weight
 (256, 256)" fillcolor=lightblue]
	140377785183904 -> 140377602087856
	140377602087856 [label=AccumulateGrad]
	140377787018880 -> 140377787018448
	140377785184064 [label="model.decoder.layers.5.encoder_attn_layer_norm.weight
 (256)" fillcolor=lightblue]
	140377785184064 -> 140377787018880
	140377787018880 [label=AccumulateGrad]
	140377787018688 -> 140377787018448
	140377785184144 [label="model.decoder.layers.5.encoder_attn_layer_norm.bias
 (256)" fillcolor=lightblue]
	140377785184144 -> 140377787018688
	140377787018688 [label=AccumulateGrad]
	140377787019072 -> 140377787018928
	140377787019072 -> 140377602254064 [dir=none]
	140377602254064 [label="other
 (6, 100, 256)" fillcolor=orange]
	140377787019072 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140377787045104 -> 140377787019072
	140377787045104 [label="ViewBackward0
----------------------
self_sizes: (600, 256)"]
	140377787045920 -> 140377787045104
	140377787045920 -> 140377602254144 [dir=none]
	140377602254144 [label="mat1
 (600, 2048)" fillcolor=orange]
	140377787045920 -> 140377602253904 [dir=none]
	140377602253904 [label="mat2
 (2048, 256)" fillcolor=orange]
	140377787045920 [label="AddmmBackward0
----------------------------
alpha       :              1
beta        :              1
mat1        : [saved tensor]
mat1_sizes  :    (600, 2048)
mat1_strides:      (2048, 1)
mat2        : [saved tensor]
mat2_sizes  :    (2048, 256)
mat2_strides:      (1, 2048)"]
	140377602088576 -> 140377787045920
	140377785299248 [label="model.decoder.layers.5.fc2.bias
 (256)" fillcolor=lightblue]
	140377785299248 -> 140377602088576
	140377602088576 [label=AccumulateGrad]
	140377602087136 -> 140377787045920
	140377602087136 [label="ViewBackward0
--------------------------
self_sizes: (6, 100, 2048)"]
	140377602086032 -> 140377602087136
	140377602086032 -> 140377602254304 [dir=none]
	140377602254304 [label="result
 (6, 100, 2048)" fillcolor=orange]
	140377602086032 [label="ReluBackward0
----------------------
result: [saved tensor]"]
	140377602088816 -> 140377602086032
	140377602088816 [label="ViewBackward0
-----------------------
self_sizes: (600, 2048)"]
	140377602221200 -> 140377602088816
	140377602221200 -> 140377602253664 [dir=none]
	140377602253664 [label="mat1
 (600, 256)" fillcolor=orange]
	140377602221200 -> 140377602254384 [dir=none]
	140377602254384 [label="mat2
 (256, 2048)" fillcolor=orange]
	140377602221200 [label="AddmmBackward0
----------------------------
alpha       :              1
beta        :              1
mat1        : [saved tensor]
mat1_sizes  :     (600, 256)
mat1_strides:       (256, 1)
mat2        : [saved tensor]
mat2_sizes  :    (256, 2048)
mat2_strides:       (1, 256)"]
	140377602221680 -> 140377602221200
	140377785299088 [label="model.decoder.layers.5.fc1.bias
 (2048)" fillcolor=lightblue]
	140377785299088 -> 140377602221680
	140377602221680 [label=AccumulateGrad]
	140377602221872 -> 140377602221200
	140377602221872 [label="ViewBackward0
-------------------------
self_sizes: (6, 100, 256)"]
	140377787018448 -> 140377602221872
	140377602220096 -> 140377602221200
	140377602220096 [label=TBackward0]
	140377602222064 -> 140377602220096
	140377785299008 [label="model.decoder.layers.5.fc1.weight
 (2048, 256)" fillcolor=lightblue]
	140377785299008 -> 140377602222064
	140377602222064 [label=AccumulateGrad]
	140377602086272 -> 140377787045920
	140377602086272 [label=TBackward0]
	140377602086560 -> 140377602086272
	140377785299168 [label="model.decoder.layers.5.fc2.weight
 (256, 2048)" fillcolor=lightblue]
	140377785299168 -> 140377602086560
	140377602086560 [label=AccumulateGrad]
	140377787017824 -> 140377787017344
	140377785299328 [label="model.decoder.layers.5.final_layer_norm.weight
 (256)" fillcolor=lightblue]
	140377785299328 -> 140377787017824
	140377787017824 [label=AccumulateGrad]
	140377787017584 -> 140377787017344
	140377785299408 [label="model.decoder.layers.5.final_layer_norm.bias
 (256)" fillcolor=lightblue]
	140377785299408 -> 140377787017584
	140377787017584 [label=AccumulateGrad]
	140377787017536 -> 140377787015280
	140377785299488 [label="model.decoder.layernorm.weight
 (256)" fillcolor=lightblue]
	140377785299488 -> 140377787017536
	140377787017536 [label=AccumulateGrad]
	140377787017200 -> 140377787015280
	140377785299568 [label="model.decoder.layernorm.bias
 (256)" fillcolor=lightblue]
	140377785299568 -> 140377787017200
	140377787017200 [label=AccumulateGrad]
	140377787015664 -> 140377787016288
	140377787015664 [label=TBackward0]
	140377787017872 -> 140377787015664
	140377785299648 [label="class_labels_classifier.weight
 (92, 256)" fillcolor=lightblue]
	140377785299648 -> 140377787017872
	140377787017872 [label=AccumulateGrad]
	140377787018016 -> 140377787203072
	140377602254464 [label="
 (600, 92)" fillcolor=darkolivegreen3]
	140377787016288 -> 140377602254464
	140377602254464 -> 140377787203072 [style=dotted]
}
